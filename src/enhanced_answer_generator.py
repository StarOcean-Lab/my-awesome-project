#!/usr/bin/env python3
"""
å¢å¼ºçš„ç­”æ¡ˆç”Ÿæˆå™¨
æ•´åˆå¼€æ”¾æ€§é—®é¢˜æ£€æµ‹å’Œåˆ›æ–°æ€§æç¤ºè¯åŠŸèƒ½
"""

import os
from typing import List, Dict, Optional, Any
from langchain_core.documents import Document
from loguru import logger
from datetime import datetime

try:
    from .open_question_detector import OpenQuestionDetector, QuestionAnalysis, QuestionType
    from .creative_prompt_manager import CreativePromptManager
except ImportError:
    from open_question_detector import OpenQuestionDetector, QuestionAnalysis, QuestionType
    from creative_prompt_manager import CreativePromptManager

class EnhancedAnswerGenerator:
    """å¢å¼ºçš„ç­”æ¡ˆç”Ÿæˆå™¨"""
    
    def __init__(self, llm_client=None):
        """
        åˆå§‹åŒ–å¢å¼ºç­”æ¡ˆç”Ÿæˆå™¨
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯
        """
        self.llm_client = llm_client
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.question_detector = OpenQuestionDetector()
        self.prompt_manager = CreativePromptManager()
        
        # é…ç½®å‚æ•°
        self.max_context_length = 1500  # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        self.enable_creative_mode = True  # æ˜¯å¦å¯ç”¨åˆ›æ–°æ¨¡å¼
        
        logger.info("å¢å¼ºç­”æ¡ˆç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def generate_answer(self, 
                       question: str, 
                       documents: List[Document],
                       use_creative_mode: bool = None) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¢å¼ºç­”æ¡ˆ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            documents: ç›¸å…³æ–‡æ¡£
            use_creative_mode: æ˜¯å¦ä½¿ç”¨åˆ›æ–°æ¨¡å¼ï¼ˆNoneæ—¶è‡ªåŠ¨åˆ¤æ–­ï¼‰
            
        Returns:
            Dict: åŒ…å«ç­”æ¡ˆå’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        start_time = datetime.now()
        
        try:
            # 1. åˆ†æé—®é¢˜ç±»å‹
            logger.debug(f"ğŸ” åˆ†æé—®é¢˜: {question[:50]}...")
            analysis = self.question_detector.analyze_question(question)
            
            logger.info(f"ğŸ“ é—®é¢˜ç±»å‹: {analysis.question_type.value}")
            logger.info(f"ğŸ¯ åˆ›æ–°çº§åˆ«: {analysis.creativity_level}")
            logger.info(f"ğŸ’¡ å…è®¸æ¨ç†: {analysis.allow_inference}")
            logger.info(f"ğŸ“‹ å›ç­”ç­–ç•¥: {analysis.response_strategy}")
            
            # 2. ç¡®å®šæ˜¯å¦ä½¿ç”¨åˆ›æ–°æ¨¡å¼
            if use_creative_mode is None:
                use_creative_mode = self._should_use_creative_mode(analysis)
            
            # 3. å‡†å¤‡ä¸Šä¸‹æ–‡
            context = self._prepare_context(documents, analysis)
            
            # 4. ç”Ÿæˆç­”æ¡ˆ
            if use_creative_mode and self.enable_creative_mode:
                logger.info("ğŸš€ ä½¿ç”¨åˆ›æ–°æ¨¡å¼ç”Ÿæˆç­”æ¡ˆ")
                answer = self._generate_creative_answer(question, context, analysis)
            else:
                logger.info("ğŸ“š ä½¿ç”¨æ ‡å‡†æ¨¡å¼ç”Ÿæˆç­”æ¡ˆ")
                answer = self._generate_standard_answer(question, context)
            
            # 5. è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # 6. æ„å»ºå“åº”
            response = {
                "answer": answer,
                "question_analysis": {
                    "type": analysis.question_type.value,
                    "creativity_level": analysis.creativity_level,
                    "confidence": analysis.confidence,
                    "allow_inference": analysis.allow_inference,
                    "response_strategy": analysis.response_strategy,
                    "keywords": analysis.keywords,
                    "reasoning_requirements": analysis.reasoning_requirements
                },
                "generation_metadata": {
                    "creative_mode_used": use_creative_mode,
                    "processing_time": processing_time,
                    "context_length": len(context),
                    "documents_used": len(documents)
                },
                "guidelines_used": self.prompt_manager.get_response_guidelines(analysis)
            }
            
            logger.info(f"âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}sï¼Œåˆ›æ–°æ¨¡å¼: {use_creative_mode}")
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return {
                "answer": f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‘ç”Ÿé”™è¯¯: {e}",
                "question_analysis": None,
                "generation_metadata": {
                    "error": str(e),
                    "processing_time": (datetime.now() - start_time).total_seconds()
                }
            }
    
    def _should_use_creative_mode(self, analysis: QuestionAnalysis) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨åˆ›æ–°æ¨¡å¼"""
        # å¼€æ”¾æ€§é—®é¢˜ä½¿ç”¨åˆ›æ–°æ¨¡å¼
        if analysis.allow_inference and analysis.creativity_level in ["medium", "high"]:
            return True
        
        # ç‰¹å®šç±»å‹çš„é—®é¢˜ä½¿ç”¨åˆ›æ–°æ¨¡å¼
        creative_types = [
            QuestionType.PROCEDURAL,
            QuestionType.ADVISORY,
            QuestionType.PREVENTIVE,
            QuestionType.ANALYTICAL,
            QuestionType.COMPARATIVE
        ]
        
        if analysis.question_type in creative_types:
            return True
        
        # åŒ…å«ç‰¹æ®Šå…³é”®è¯çš„é—®é¢˜ä½¿ç”¨åˆ›æ–°æ¨¡å¼
        creative_keywords = ["å¦‚ä½•", "å»ºè®®", "æ–¹æ³•", "ç­–ç•¥", "ä¿æŠ¤", "é¿å…", "é˜²æ­¢", "ç¡®ä¿"]
        question_lower = analysis.keywords if analysis.keywords else []
        
        if any(kw in question_lower for kw in creative_keywords):
            return True
        
        return False
    
    def _prepare_context(self, documents: List[Document], analysis: QuestionAnalysis) -> str:
        """å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        if not documents:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ä¿¡æ¯ã€‚"
        
        # æ”¶é›†æ–‡æ¡£å†…å®¹
        context_parts = []
        total_length = 0
        
        for i, doc in enumerate(documents):
            content = doc.page_content.strip()
            source = os.path.basename(doc.metadata.get('source', f'æ–‡æ¡£{i+1}'))
            
            # æ·»åŠ æ¥æºæ ‡è¯†
            doc_content = f"ã€æ¥æºï¼š{source}ã€‘\n{content}"
            
            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if total_length + len(doc_content) > self.max_context_length:
                # æˆªæ–­å†…å®¹
                remaining_length = self.max_context_length - total_length
                if remaining_length > 100:  # è‡³å°‘ä¿ç•™100å­—ç¬¦
                    doc_content = doc_content[:remaining_length] + "..."
                    context_parts.append(doc_content)
                break
            
            context_parts.append(doc_content)
            total_length += len(doc_content)
        
        context = "\n\n".join(context_parts)
        
        # ä¸ºåˆ›æ–°æ€§å›ç­”å¢å¼ºä¸Šä¸‹æ–‡
        if analysis.allow_inference:
            context = self.prompt_manager.enhance_context_for_creative_response(context, analysis)
        
        return context
    
    def _generate_creative_answer(self, 
                                question: str, 
                                context: str, 
                                analysis: QuestionAnalysis) -> str:
        """ç”Ÿæˆåˆ›æ–°æ€§ç­”æ¡ˆ"""
        try:
            # æ„å»ºåˆ›æ–°æ€§æç¤ºè¯
            if hasattr(self.prompt_manager, 'build_creative_prompt'):
                prompt = self.prompt_manager.build_creative_prompt(question, context, analysis)
            else:
                # è·å–å¯¹åº”çš„æ¨¡æ¿
                template = self.prompt_manager.get_prompt_template(analysis.response_strategy)
                prompt_messages = template.format_messages(input=question, context=context)
                
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                prompt = ""
                for message in prompt_messages:
                    if hasattr(message, 'content'):
                        prompt += f"{message.content}\n\n"
                    else:
                        prompt += f"{str(message)}\n\n"
            
            # ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
            if self.llm_client:
                answer = self._invoke_llm(prompt)
            else:
                # åå¤‡æ–¹æ¡ˆï¼šåŸºäºè§„åˆ™çš„å›ç­”
                answer = self._generate_rule_based_answer(question, context, analysis)
            
            # ä¸ºç­”æ¡ˆæ·»åŠ åˆ›æ–°æ€§æ ‡è¯†
            if analysis.creativity_level == "high":
                answer = f"ğŸ’¡ **åˆ›æ–°æ€§å»ºè®®**\n\n{answer}"
            elif analysis.creativity_level == "medium":
                answer = f"ğŸ“‹ **ç»¼åˆåˆ†æ**\n\n{answer}"
            
            return answer
            
        except Exception as e:
            logger.error(f"åˆ›æ–°æ€§ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_standard_answer(question, context)
    
    def _generate_standard_answer(self, question: str, context: str) -> str:
        """ç”Ÿæˆæ ‡å‡†ç­”æ¡ˆï¼ˆä¸¥æ ¼åŸºäºæ–‡æ¡£ï¼‰"""
        try:
            # æ„å»ºæ ‡å‡†æç¤ºè¯
            prompt = f"""åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œä¸¥æ ¼å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ã€é‡è¦è§„åˆ™ã€‘ï¼š
1. åªèƒ½ä½¿ç”¨æ–‡æ¡£ä¸­æ˜ç¡®æåˆ°çš„ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. ä¸è¦æ·»åŠ æ–‡æ¡£å¤–çš„ä»»ä½•ä¿¡æ¯
4. ä¿æŒå›ç­”çš„å‡†ç¡®æ€§å’Œå®¢è§‚æ€§

ã€æ–‡æ¡£å†…å®¹ã€‘ï¼š
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š{question}

ã€å›ç­”ã€‘ï¼š"""

            # ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
            if self.llm_client:
                answer = self._invoke_llm(prompt)
            else:
                # åå¤‡æ–¹æ¡ˆï¼šæ¨¡æ¿å›ç­”
                answer = self._generate_template_answer(question, context)
            
            return answer
            
        except Exception as e:
            logger.error(f"æ ‡å‡†ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆåˆé€‚çš„å›ç­”ã€‚"
    
    def _invoke_llm(self, prompt: str) -> str:
        """è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ"""
        try:
            if hasattr(self.llm_client, 'invoke'):
                # LangChain LLM
                response = self.llm_client.invoke(prompt)
                if hasattr(response, 'content'):
                    return response.content
                else:
                    return str(response)
            elif hasattr(self.llm_client, 'generate'):
                # è‡ªå®šä¹‰LLMå®¢æˆ·ç«¯
                return self.llm_client.generate(prompt)
            elif callable(self.llm_client):
                # å¯è°ƒç”¨å¯¹è±¡
                return self.llm_client(prompt)
            else:
                # å°è¯•ç›´æ¥è°ƒç”¨
                return str(self.llm_client.invoke(prompt))
                
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            raise e
    
    def _generate_rule_based_answer(self, 
                                  question: str, 
                                  context: str, 
                                  analysis: QuestionAnalysis) -> str:
        """åŸºäºè§„åˆ™çš„ç­”æ¡ˆç”Ÿæˆï¼ˆæ— LLMæ—¶çš„åå¤‡æ–¹æ¡ˆï¼‰"""
        # æ ¹æ®é—®é¢˜ç±»å‹æä¾›ä¸åŒçš„å›ç­”æ¨¡æ¿
        if analysis.question_type == QuestionType.PREVENTIVE:
            return self._generate_protection_advice(question, context)
        
        elif analysis.question_type == QuestionType.PROCEDURAL:
            return self._generate_procedural_guidance(question, context)
        
        elif analysis.question_type == QuestionType.ADVISORY:
            return self._generate_advisory_response(question, context)
        
        else:
            return self._generate_template_answer(question, context)
    
    def _generate_protection_advice(self, question: str, context: str) -> str:
        """ç”Ÿæˆä¿æŠ¤æ€§å»ºè®®"""
        advice = "åŸºäºç›¸å…³æ–‡æ¡£å’Œæœ€ä½³å®è·µï¼Œä»¥ä¸‹æ˜¯ä¿æŠ¤å»ºè®®ï¼š\n\n"
        
        advice += "ğŸ›¡ï¸ **é£é™©é˜²æ§ç­–ç•¥**\n"
        advice += "1. **çŸ¥è¯†äº§æƒä¿æŠ¤**ï¼š\n"
        advice += "   - åŠæ—¶ç”³è¯·ç›¸å…³ä¸“åˆ©å’Œç‰ˆæƒä¿æŠ¤\n"
        advice += "   - ä¿ç•™å®Œæ•´çš„è®¾è®¡å’Œå¼€å‘è®°å½•\n"
        advice += "   - å»ºç«‹ç‰ˆæœ¬æ§åˆ¶å’Œæ—¶é—´æˆ³æœºåˆ¶\n\n"
        
        advice += "2. **ä½œå“åŸåˆ›æ€§ç»´æŠ¤**ï¼š\n"
        advice += "   - ç¡®ä¿æ‰€æœ‰è®¾è®¡å’Œä»£ç çš„åŸåˆ›æ€§\n"
        advice += "   - é¿å…ä½¿ç”¨æœªç»æˆæƒçš„ç¬¬ä¸‰æ–¹èµ„æº\n"
        advice += "   - å»ºç«‹å›¢é˜Ÿå†…éƒ¨çš„åŸåˆ›æ€§æ£€æŸ¥æœºåˆ¶\n\n"
        
        advice += "3. **æ–‡æ¡£è®°å½•ç®¡ç†**ï¼š\n"
        advice += "   - è¯¦ç»†è®°å½•è®¾è®¡æ€è·¯å’Œå¼€å‘è¿‡ç¨‹\n"
        advice += "   - ä¿å­˜å…³é”®å†³ç­–çš„è®¨è®ºè®°å½•\n"
        advice += "   - å»ºç«‹å®Œæ•´çš„é¡¹ç›®æ¡£æ¡ˆ\n\n"
        
        if context and len(context.strip()) > 50:
            advice += f"ğŸ“„ **ç›¸å…³æ–‡æ¡£ä¿¡æ¯**ï¼š\n{context[:300]}...\n\n"
        
        advice += "âš ï¸ **æ³¨æ„äº‹é¡¹**ï¼š\n"
        advice += "- éµå®ˆç«èµ›è§„åˆ™å’Œç›¸å…³æ³•å¾‹æ³•è§„\n"
        advice += "- å»ºè®®å’¨è¯¢ä¸“ä¸šçš„çŸ¥è¯†äº§æƒé¡¾é—®\n"
        advice += "- ä¸å›¢é˜Ÿæˆå‘˜ç­¾ç½²ç›¸å…³ä¿å¯†åè®®"
        
        return advice
    
    def _generate_procedural_guidance(self, question: str, context: str) -> str:
        """ç”Ÿæˆç¨‹åºæ€§æŒ‡å¯¼"""
        guidance = "æ ¹æ®ç›¸å…³ä¿¡æ¯ï¼Œä»¥ä¸‹æ˜¯æ“ä½œæŒ‡å¯¼ï¼š\n\n"
        
        guidance += "ğŸ“‹ **åŸºæœ¬æ­¥éª¤**ï¼š\n"
        guidance += "1. æ˜ç¡®ç›®æ ‡å’Œè¦æ±‚\n"
        guidance += "2. åˆ¶å®šè¯¦ç»†çš„å®æ–½è®¡åˆ’\n"
        guidance += "3. æŒ‰æ­¥éª¤æ‰§è¡Œå¹¶è®°å½•è¿‡ç¨‹\n"
        guidance += "4. å®šæœŸæ£€æŸ¥å’Œè°ƒæ•´ç­–ç•¥\n\n"
        
        if context and len(context.strip()) > 50:
            guidance += f"ğŸ“„ **æ–‡æ¡£ä¾æ®**ï¼š\n{context[:400]}...\n\n"
        
        guidance += "ğŸ’¡ **å®æ–½å»ºè®®**ï¼š\n"
        guidance += "- å»ºè®®å’¨è¯¢ç›¸å…³ä¸“å®¶æˆ–å¯¼å¸ˆ\n"
        guidance += "- å‚è€ƒç«èµ›è§„åˆ™å’Œè¦æ±‚\n"
        guidance += "- ä¸å›¢é˜Ÿæˆå‘˜å……åˆ†æ²Ÿé€šåè°ƒ"
        
        return guidance
    
    def _generate_advisory_response(self, question: str, context: str) -> str:
        """ç”Ÿæˆå»ºè®®æ€§å›ç­”"""
        response = "åŸºäºç›¸å…³ä¿¡æ¯ï¼Œä»¥ä¸‹æ˜¯ä¸“ä¸šå»ºè®®ï¼š\n\n"
        
        response += "ğŸ¯ **ç»¼åˆåˆ†æ**ï¼š\n"
        if context and len(context.strip()) > 50:
            response += f"æ ¹æ®æ–‡æ¡£ä¿¡æ¯ï¼š{context[:300]}...\n\n"
        
        response += "ğŸ’¼ **å»ºè®®æ–¹æ¡ˆ**ï¼š\n"
        response += "1. **çŸ­æœŸç­–ç•¥**ï¼šç«‹å³é‡‡å–çš„æªæ–½\n"
        response += "2. **é•¿æœŸè§„åˆ’**ï¼šæŒç»­æ”¹è¿›çš„æ–¹å‘\n"
        response += "3. **é£é™©ç®¡æ§**ï¼šæ½œåœ¨é£é™©çš„åº”å¯¹\n\n"
        
        response += "âš¡ **å®æ–½è¦ç‚¹**ï¼š\n"
        response += "- æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ç­–ç•¥\n"
        response += "- å»ºç«‹æœ‰æ•ˆçš„ç›‘ç£æœºåˆ¶\n"
        response += "- å®šæœŸè¯„ä¼°å’Œä¼˜åŒ–æ–¹æ¡ˆ"
        
        return response
    
    def _generate_template_answer(self, question: str, context: str) -> str:
        """ç”Ÿæˆæ¨¡æ¿å›ç­”"""
        if not context or len(context.strip()) < 20:
            return "æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚å»ºè®®æ‚¨æä¾›æ›´å¤šçš„èƒŒæ™¯ä¿¡æ¯æˆ–æŸ¥é˜…ç›¸å…³çš„ç«èµ›æ–‡æ¡£ã€‚"
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        question_lower = question.lower()
        
        if any(kw in question_lower for kw in ["æ—¶é—´", "ä»€ä¹ˆæ—¶å€™", "å¼€å§‹", "ç»“æŸ"]):
            return f"å…³äºæ—¶é—´å®‰æ’çš„ä¿¡æ¯ï¼Œæ ¹æ®æ–‡æ¡£å†…å®¹ï¼š\n\n{context[:400]}..."
        
        elif any(kw in question_lower for kw in ["è¦æ±‚", "æ¡ä»¶", "æ ‡å‡†"]):
            return f"å…³äºç›¸å…³è¦æ±‚çš„ä¿¡æ¯ï¼š\n\n{context[:400]}..."
        
        elif any(kw in question_lower for kw in ["å¥–é¡¹", "å¥–åŠ±", "å¥–é‡‘"]):
            return f"å…³äºå¥–é¡¹è®¾ç½®çš„ä¿¡æ¯ï¼š\n\n{context[:400]}..."
        
        else:
            return f"æ ¹æ®ç›¸å…³æ–‡æ¡£ï¼Œæ‰¾åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š\n\n{context[:500]}..."
    
    def set_llm_client(self, llm_client):
        """è®¾ç½®LLMå®¢æˆ·ç«¯"""
        self.llm_client = llm_client
        logger.info("LLMå®¢æˆ·ç«¯å·²æ›´æ–°")
    
    def enable_creative_mode(self, enable: bool = True):
        """å¯ç”¨æˆ–ç¦ç”¨åˆ›æ–°æ¨¡å¼"""
        self.enable_creative_mode = enable
        logger.info(f"åˆ›æ–°æ¨¡å¼å·²{'å¯ç”¨' if enable else 'ç¦ç”¨'}")
    
    def get_question_analysis(self, question: str) -> QuestionAnalysis:
        """è·å–é—®é¢˜åˆ†æç»“æœ"""
        return self.question_detector.analyze_question(question) 
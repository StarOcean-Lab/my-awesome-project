"""
Âü∫‰∫éÊñá‰ª∂ÂêçÊÑüÁü•ÁöÑÊ£ÄÁ¥¢Âô®
‰ªéÁî®Êà∑ÈóÆÈ¢ò‰∏≠Ê£ÄÊµãÊñá‰ª∂ÂêçÔºåÂÆûÁé∞ÂÆöÂêëÊñá‰ª∂Ê£ÄÁ¥¢
"""

import re
import os
from typing import List, Dict, Optional, Set, Tuple, Any
from collections import defaultdict
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from loguru import logger
import jieba

try:
    from .langchain_retriever import LangChainHybridRetriever
except ImportError:
    from langchain_retriever import LangChainHybridRetriever


class FilenameAwareRetriever(BaseRetriever):
    """Âü∫‰∫éÊñá‰ª∂ÂêçÊÑüÁü•ÁöÑÊ£ÄÁ¥¢Âô®"""
    
    # PydanticÂ≠óÊÆµÂÆö‰πâ
    base_retriever: Any
    documents: List[Document] = []
    enable_filename_detection: bool = True
    fallback_to_global: bool = True
    min_confidence: float = 0.6
    k: int = 10
    file_index: Dict[str, List[Document]] = {}
    filename_keywords: Dict[str, List[str]] = {}
    
    class Config:
        arbitrary_types_allowed = True
    
    def __init__(self, 
                 base_retriever: LangChainHybridRetriever,
                 documents: List[Document] = None,
                 enable_filename_detection: bool = True,
                 fallback_to_global: bool = True,
                 min_confidence: float = 0.6,
                 k: int = 10):
        """
        ÂàùÂßãÂåñÊñá‰ª∂ÂêçÊÑüÁü•Ê£ÄÁ¥¢Âô®
        
        Args:
            base_retriever: Âü∫Á°ÄÊ£ÄÁ¥¢Âô®
            documents: ÊñáÊ°£ÂàóË°®
            enable_filename_detection: ÊòØÂê¶ÂêØÁî®Êñá‰ª∂ÂêçÊ£ÄÊµã
            fallback_to_global: ÂΩìÊ≤°ÊúâÊ£ÄÊµãÂà∞Êñá‰ª∂ÂêçÊó∂ÊòØÂê¶ÈôçÁ∫ßÂà∞ÂÖ®Â±ÄÊ£ÄÁ¥¢
            min_confidence: Êñá‰ª∂ÂêçÂåπÈÖçÁöÑÊúÄÂ∞èÁΩÆ‰ø°Â∫¶
            k: ÈªòËÆ§ËøîÂõûÁªìÊûúÊï∞Èáè
        """
        # ÊûÑÂª∫Êñá‰ª∂Á¥¢ÂºïÂíåÂÖ≥ÈîÆËØçÊò†Â∞Ñ
        file_index = self._build_file_index_static(documents if documents else [])
        filename_keywords = self._build_filename_keywords_static(file_index)
        
        super().__init__(
            base_retriever=base_retriever,
            documents=documents if documents else [],
            enable_filename_detection=enable_filename_detection,
            fallback_to_global=fallback_to_global,
            min_confidence=min_confidence,
            k=k,
            file_index=file_index,
            filename_keywords=filename_keywords
        )
        
        logger.info(f"Êñá‰ª∂ÂêçÊÑüÁü•Ê£ÄÁ¥¢Âô®ÂàùÂßãÂåñÂÆåÊàê")
        logger.info(f"  - ÊñáÊ°£ÊÄªÊï∞: {len(self.documents)}")
        logger.info(f"  - Êñá‰ª∂Êï∞Èáè: {len(self.file_index)}")
        logger.info(f"  - ÂÖ≥ÈîÆËØçÊï∞Èáè: {len(self.filename_keywords)}")
    
    @staticmethod
    def _build_file_index_static(documents: List[Document]) -> Dict[str, List[Document]]:
        """ÊûÑÂª∫Êñá‰ª∂Á¥¢ÂºïÔºåÊåâÊñá‰ª∂Ë∑ØÂæÑÂàÜÁªÑÊñáÊ°£ÔºàÈùôÊÄÅÊñπÊ≥ïÔºâ"""
        file_index = defaultdict(list)
        
        for doc in documents:
            # Â∞ùËØïÂ§öÁßçÊñπÂºèËé∑ÂèñÊñá‰ª∂Ë∑ØÂæÑ
            file_path = (
                doc.metadata.get('file_path') or 
                doc.metadata.get('source') or 
                doc.metadata.get('filename', '')
            )
            
            if file_path and file_path != 'unknown':
                # Ê†áÂáÜÂåñÊñá‰ª∂Ë∑ØÂæÑ
                normalized_path = os.path.normpath(file_path)
                file_index[normalized_path].append(doc)
        
        logger.info(f"ÊûÑÂª∫Êñá‰ª∂Á¥¢ÂºïÂÆåÊàêÔºåÂåÖÂê´ {len(file_index)} ‰∏™Êñá‰ª∂:")
        for file_path, docs in file_index.items():
            filename = os.path.basename(file_path)
            logger.info(f"  üìÑ {filename}: {len(docs)} ‰∏™ÊñáÊ°£Âùó")
        
        return dict(file_index)
    
    def _build_file_index(self) -> Dict[str, List[Document]]:
        """ÊûÑÂª∫Êñá‰ª∂Á¥¢ÂºïÔºåÊåâÊñá‰ª∂Ë∑ØÂæÑÂàÜÁªÑÊñáÊ°£"""
        return self._build_file_index_static(self.documents)
    
    @staticmethod  
    def _build_filename_keywords_static(file_index: Dict[str, List[Document]]) -> Dict[str, List[str]]:
        """ÊûÑÂª∫Êñá‰ª∂ÂêçÂÖ≥ÈîÆËØçÊò†Â∞ÑÔºàÈùôÊÄÅÊñπÊ≥ïÔºâ"""
        keywords_map = {}
        
        for file_path in file_index.keys():
            filename = os.path.basename(file_path)
            
            # ÊèêÂèñÊñá‰ª∂Âêç‰∏≠ÁöÑÂÖ≥ÈîÆËØç
            keywords = FilenameAwareRetriever._extract_keywords_from_filename_static(filename)
            
            if keywords:
                keywords_map[file_path] = keywords
                logger.debug(f"Êñá‰ª∂ {filename} ÂÖ≥ÈîÆËØç: {keywords}")
        
        return keywords_map
    
    def _build_filename_keywords(self) -> Dict[str, List[str]]:
        """ÊûÑÂª∫Êñá‰ª∂ÂêçÂÖ≥ÈîÆËØçÊò†Â∞Ñ"""
        return self._build_filename_keywords_static(self.file_index)
    
    @staticmethod
    def _extract_keywords_from_filename_static(filename: str) -> List[str]:
        """‰ªéÊñá‰ª∂Âêç‰∏≠ÊèêÂèñÂÖ≥ÈîÆËØçÔºàÈùôÊÄÅÊñπÊ≥ïÔºâ"""
        keywords = []
        
        # ÁßªÈô§Êñá‰ª∂Êâ©Â±ïÂêç
        name_without_ext = os.path.splitext(filename)[0]
        
        # 1. ÊèêÂèñÁºñÂè∑Ê®°ÂºèÔºàÂ¶Ç "01_", "02_"Ôºâ
        number_match = re.match(r'^(\d+)_', name_without_ext)
        if number_match:
            keywords.append(number_match.group(1))
        
        # 2. ÊèêÂèñ‰∏≠ÊñáÂÜÖÂÆπ
        chinese_parts = re.findall(r'[‰∏Ä-Èæ•]+', name_without_ext)
        keywords.extend(chinese_parts)
        
        # 3. ‰ΩøÁî®jiebaÂàÜËØçÊèêÂèñÂÖ≥ÈîÆËØç
        try:
            words = jieba.lcut(name_without_ext)
            for word in words:
                if len(word) >= 2 and re.match(r'[‰∏Ä-Èæ•]', word):
                    keywords.append(word)
        except:
            pass
        
        # 4. ÊèêÂèñÁâπÊÆäÊ®°ÂºèÔºàÂ¶ÇÂπ¥‰ªΩ„ÄÅËµõ‰∫ãÂêçÁß∞Á≠âÔºâ
        special_patterns = [
            r'(\d{4}Âπ¥)',  # Âπ¥‰ªΩ
            r'(Á¨¨\d+Â±ä)',  # Â±äÊï∞
            r'([‰∏Ä-Èæ•]*‰∏ìÈ°πËµõ)',  # ‰∏ìÈ°πËµõ
            r'([‰∏Ä-Èæ•]*ÊåëÊàòËµõ)',  # ÊåëÊàòËµõ
            r'([‰∏Ä-Èæ•]*Á´ûËµõ)',    # Á´ûËµõ
            r'([‰∏Ä-Èæ•]*ËÆæËÆ°)',    # ËÆæËÆ°
            r'([‰∏Ä-Èæ•]*Â∫îÁî®)',    # Â∫îÁî®
            r'([‰∏Ä-Èæ•]*Êô∫ËÉΩ[‰∏Ä-Èæ•]*)',  # Êô∫ËÉΩÁõ∏ÂÖ≥
        ]
        
        for pattern in special_patterns:
            matches = re.findall(pattern, name_without_ext)
            keywords.extend(matches)
        
        # 5. ÂéªÈáçÂπ∂ËøáÊª§Áü≠ËØç
        keywords = list(set(keywords))
        keywords = [kw for kw in keywords if len(kw) >= 2]
        
        return keywords
    
    def _extract_keywords_from_filename(self, filename: str) -> List[str]:
        """‰ªéÊñá‰ª∂Âêç‰∏≠ÊèêÂèñÂÖ≥ÈîÆËØç"""
        return self._extract_keywords_from_filename_static(filename)
    
    def detect_target_files(self, query: str) -> Tuple[List[str], float]:
        """
        ‰ªéÊü•ËØ¢‰∏≠Ê£ÄÊµãÁõÆÊ†áÊñá‰ª∂
        
        Args:
            query: Áî®Êà∑Êü•ËØ¢
            
        Returns:
            (ÂåπÈÖçÁöÑÊñá‰ª∂Ë∑ØÂæÑÂàóË°®, ÁΩÆ‰ø°Â∫¶ÂàÜÊï∞)
        """
        if not self.enable_filename_detection:
            return [], 0.0
        
        # ÂØπÊü•ËØ¢ËøõË°åÈ¢ÑÂ§ÑÁêÜ
        query_clean = re.sub(r'[^\w\s‰∏Ä-Èæ•]', ' ', query)
        query_words = set(jieba.lcut(query_clean))
        
        # Êî∂ÈõÜÊâÄÊúâÂåπÈÖçÁªìÊûú
        file_scores = {}
        
        for file_path, keywords in self.filename_keywords.items():
            score = 0.0
            matched_keywords = []
            
            for keyword in keywords:
                # Áõ¥Êé•ÂåπÈÖç
                if keyword in query:
                    score += len(keyword) * 2  # Áõ¥Êé•ÂåπÈÖçÁªôÊõ¥È´òÂàÜ
                    matched_keywords.append(keyword)
                
                # ÂàÜËØçÂåπÈÖç
                elif keyword in query_words:
                    score += len(keyword)
                    matched_keywords.append(keyword)
                
                # Ê®°Á≥äÂåπÈÖç
                elif any(keyword in word or word in keyword for word in query_words):
                    score += len(keyword) * 0.5
                    matched_keywords.append(keyword)
            
            if score > 0:
                # ËÆ°ÁÆóÁõ∏ÂØπÂàÜÊï∞
                max_possible_score = sum(len(kw) * 2 for kw in keywords)
                relative_score = score / max_possible_score if max_possible_score > 0 else 0
                
                file_scores[file_path] = {
                    'score': relative_score,
                    'matched_keywords': matched_keywords,
                    'absolute_score': score
                }
        
        # ÊéíÂ∫èÂπ∂ÈÄâÊã©ÊúÄ‰Ω≥ÂåπÈÖç
        sorted_files = sorted(file_scores.items(), key=lambda x: x[1]['score'], reverse=True)
        
        # ÈÄâÊã©ÁΩÆ‰ø°Â∫¶Ë∂≥Â§üÁöÑÊñá‰ª∂
        target_files = []
        max_confidence = 0.0
        
        for file_path, info in sorted_files:
            confidence = info['score']
            if confidence >= self.min_confidence:
                target_files.append(file_path)
                max_confidence = max(max_confidence, confidence)
                
                filename = os.path.basename(file_path)
                logger.info(f"üéØ Ê£ÄÊµãÂà∞ÁõÆÊ†áÊñá‰ª∂: {filename} (ÁΩÆ‰ø°Â∫¶: {confidence:.2f})")
                logger.info(f"   ÂåπÈÖçÂÖ≥ÈîÆËØç: {info['matched_keywords']}")
        
        return target_files, max_confidence
    
    def get_relevant_documents(self, query: str) -> List[Document]:
        """
        Ëé∑ÂèñÁõ∏ÂÖ≥ÊñáÊ°£
        
        Args:
            query: Êü•ËØ¢ÊñáÊú¨
            
        Returns:
            Áõ∏ÂÖ≥ÊñáÊ°£ÂàóË°®
        """
        try:
            logger.info(f"üîç Êñá‰ª∂ÂêçÊÑüÁü•Ê£ÄÁ¥¢: '{query[:50]}...'")
            
            # 1. Ê£ÄÊµãÁõÆÊ†áÊñá‰ª∂
            target_files, confidence = self.detect_target_files(query)
            
            if target_files and confidence > 0:
                logger.info(f"üìÅ ÂÆöÂêëÊ£ÄÁ¥¢Ê®°Âºè: Âú® {len(target_files)} ‰∏™Êñá‰ª∂‰∏≠Ê£ÄÁ¥¢")
                return self._targeted_retrieval(query, target_files)
            
            elif self.fallback_to_global:
                logger.info("üåç ÈôçÁ∫ßÂà∞ÂÖ®Â±ÄÊ£ÄÁ¥¢Ê®°Âºè")
                return self._global_retrieval(query)
            
            else:
                logger.warning("‚ùå Êú™Ê£ÄÊµãÂà∞ÁõÆÊ†áÊñá‰ª∂‰∏î‰∏çÂÖÅËÆ∏ÂÖ®Â±ÄÊ£ÄÁ¥¢")
                return []
                
        except Exception as e:
            logger.error(f"Êñá‰ª∂ÂêçÊÑüÁü•Ê£ÄÁ¥¢Â§±Ë¥•: {e}")
            if self.fallback_to_global:
                return self._global_retrieval(query)
            return []
    
    def _targeted_retrieval(self, query: str, target_files: List[str]) -> List[Document]:
        """
        Âú®ÊåáÂÆöÊñá‰ª∂‰∏≠ËøõË°åÂÆöÂêëÊ£ÄÁ¥¢
        
        Args:
            query: Êü•ËØ¢ÊñáÊú¨
            target_files: ÁõÆÊ†áÊñá‰ª∂Ë∑ØÂæÑÂàóË°®
            
        Returns:
            Ê£ÄÁ¥¢ÁªìÊûú
        """
        # Êî∂ÈõÜÁõÆÊ†áÊñá‰ª∂ÁöÑÊâÄÊúâÊñáÊ°£
        target_documents = []
        for file_path in target_files:
            if file_path in self.file_index:
                target_documents.extend(self.file_index[file_path])
        
        if not target_documents:
            logger.warning("ÁõÆÊ†áÊñá‰ª∂‰∏≠Ê≤°ÊúâÊâæÂà∞ÊñáÊ°£")
            return []
        
        logger.info(f"üìö Âú® {len(target_documents)} ‰∏™ÁõÆÊ†áÊñáÊ°£‰∏≠Ê£ÄÁ¥¢")
        
        # ‰∏∫ÂÆöÂêëÊ£ÄÁ¥¢ÂàõÂª∫‰∏¥Êó∂Ê£ÄÁ¥¢Âô®
        return self._search_in_documents(query, target_documents)
    
    def _search_in_documents(self, query: str, documents: List[Document]) -> List[Document]:
        """
        Âú®ÊåáÂÆöÊñáÊ°£ÈõÜÂêà‰∏≠ËøõË°åÊ£ÄÁ¥¢
        
        Args:
            query: Êü•ËØ¢ÊñáÊú¨
            documents: ÊñáÊ°£ÈõÜÂêà
            
        Returns:
            Ê£ÄÁ¥¢ÁªìÊûú
        """
        try:
            # ÊñπÊ≥ï1: ‰ΩøÁî®Âü∫Á°ÄÊ£ÄÁ¥¢Âô®ÁöÑÂêëÈáèÂ≠òÂÇ®ËøõË°åËøáÊª§Ê£ÄÁ¥¢
            if hasattr(self.base_retriever, 'vectorstore') and self.base_retriever.vectorstore:
                # Ëé∑ÂèñÊâÄÊúâÂÄôÈÄâÁªìÊûú
                all_candidates = self.base_retriever.vectorstore.similarity_search(query, k=self.k * 3)
                
                # ËøáÊª§Âá∫Â±û‰∫éÁõÆÊ†áÊñáÊ°£ÁöÑÁªìÊûú
                target_contents = {doc.page_content for doc in documents}
                filtered_results = []
                
                for candidate in all_candidates:
                    if candidate.page_content in target_contents:
                        filtered_results.append(candidate)
                        if len(filtered_results) >= self.k:
                            break
                
                if filtered_results:
                    logger.info(f"‚úÖ ÂÆöÂêëÊ£ÄÁ¥¢ÊàêÂäüÔºåËøîÂõû {len(filtered_results)} ‰∏™ÁªìÊûú")
                    return filtered_results
            
            # ÊñπÊ≥ï2: Âü∫‰∫éÂÖ≥ÈîÆËØçÁöÑÁÆÄÂçïÂåπÈÖç
            return self._keyword_based_search(query, documents)
            
        except Exception as e:
            logger.warning(f"ÂÆöÂêëÊ£ÄÁ¥¢Â§±Ë¥•: {e}Ôºå‰ΩøÁî®ÂÖ≥ÈîÆËØçÂåπÈÖç")
            return self._keyword_based_search(query, documents)
    
    def _keyword_based_search(self, query: str, documents: List[Document]) -> List[Document]:
        """
        Âü∫‰∫éÂÖ≥ÈîÆËØçÁöÑÁÆÄÂçïÊêúÁ¥¢
        
        Args:
            query: Êü•ËØ¢ÊñáÊú¨
            documents: ÊñáÊ°£ÈõÜÂêà
            
        Returns:
            ÊêúÁ¥¢ÁªìÊûú
        """
        # ÊèêÂèñÊü•ËØ¢ÂÖ≥ÈîÆËØç
        query_keywords = set(jieba.lcut(query))
        query_keywords = {kw for kw in query_keywords if len(kw) >= 2}
        
        # ËÆ°ÁÆóÊØè‰∏™ÊñáÊ°£ÁöÑÁõ∏ÂÖ≥ÊÄßÂàÜÊï∞
        scored_docs = []
        
        for doc in documents:
            content = doc.page_content
            score = 0
            
            # Áõ¥Êé•ÂåπÈÖç
            for keyword in query_keywords:
                if keyword in content:
                    score += content.count(keyword) * len(keyword)
            
            if score > 0:
                scored_docs.append((doc, score))
        
        # ÊéíÂ∫èÂπ∂ËøîÂõû
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        results = [doc for doc, score in scored_docs[:self.k]]
        
        logger.info(f"üîç ÂÖ≥ÈîÆËØçÊêúÁ¥¢ËøîÂõû {len(results)} ‰∏™ÁªìÊûú")
        return results
    
    def _global_retrieval(self, query: str) -> List[Document]:
        """
        ÂÖ®Â±ÄÊ£ÄÁ¥¢
        
        Args:
            query: Êü•ËØ¢ÊñáÊú¨
            
        Returns:
            Ê£ÄÁ¥¢ÁªìÊûú
        """
        return self.base_retriever.get_relevant_documents(query)
    
    def get_file_statistics(self) -> Dict:
        """Ëé∑ÂèñÊñá‰ª∂ÁªüËÆ°‰ø°ÊÅØ"""
        stats = {
            'total_files': len(self.file_index),
            'total_documents': len(self.documents),
            'files_with_keywords': len(self.filename_keywords),
            'file_details': []
        }
        
        for file_path, docs in self.file_index.items():
            filename = os.path.basename(file_path)
            keywords = self.filename_keywords.get(file_path, [])
            
            stats['file_details'].append({
                'filename': filename,
                'file_path': file_path,
                'document_count': len(docs),
                'keywords': keywords
            })
        
        return stats 
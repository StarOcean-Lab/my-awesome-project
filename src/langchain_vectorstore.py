"""
LangChainå‘é‡å­˜å‚¨ç³»ç»Ÿ
ä½¿ç”¨LangChain FAISSå‘é‡å­˜å‚¨å’ŒOllamaä¸­æ–‡embeddingæ¨¡å‹
"""

import os
import requests
import json
import time
from typing import List, Dict, Optional, Tuple, Callable

# ä¿®å¤ç›¸å¯¹å¯¼å…¥é—®é¢˜
try:
    from .document_version_manager import DocumentVersionManager
except ImportError:
    try:
        from document_version_manager import DocumentVersionManager
    except ImportError:
        # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œè®¾ç½®ä¸ºNoneï¼Œåç»­ä»£ç ä¼šå¤„ç†è¿™ç§æƒ…å†µ
        DocumentVersionManager = None

# ä¿®å¤å¯¼å…¥ - ä½¿ç”¨å…¼å®¹çš„å¯¼å…¥æ–¹å¼
try:
    from langchain_community.vectorstores import FAISS
except ImportError:
    try:
        from langchain.vectorstores import FAISS
    except ImportError:
        FAISS = None

try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    try:
        from langchain_community.embeddings import HuggingFaceEmbeddings
    except ImportError:
        try:
            from langchain.embeddings import HuggingFaceEmbeddings
        except ImportError:
            HuggingFaceEmbeddings = None

from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings

try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
except ImportError:
    try:
        from langchain_text_splitters import RecursiveCharacterTextSplitter
    except ImportError:
        RecursiveCharacterTextSplitter = None

from loguru import logger


class OllamaEmbeddings(Embeddings):
    
    def __init__(self, model: str = "./bge-large-zh-v1.5", base_url: str = "http://localhost:11434"):
        """
        åˆå§‹åŒ–Ollama Embeddings
        
        Args:
            model: åµŒå…¥æ¨¡å‹åç§°ï¼Œå¯ä»¥æ˜¯æœ¬åœ°è·¯å¾„ï¼ˆå¦‚'./bge-large-zh-v1.5'ï¼‰æˆ–Ollamaæ¨¡å‹åï¼ˆå¦‚'nomic-embed-text:latest'ï¼‰
            base_url: OllamaæœåŠ¡å™¨åœ°å€
        """
        self.model = model
        self.base_url = base_url
        self.embed_url = f"{base_url}/api/embeddings"
        logger.info(f"åˆå§‹åŒ–OllamaEmbeddingsï¼Œæ¨¡å‹: {model}, æœåŠ¡å™¨: {base_url}")
    
    def _call_ollama_embedding(self, text: str) -> List[float]:
        """è°ƒç”¨Ollama APIè·å–å•ä¸ªæ–‡æœ¬çš„embedding"""
        try:
            # ç¡®ä¿textæ˜¯å­—ç¬¦ä¸²ç±»å‹
            if not isinstance(text, str):
                text = str(text)
            
            # æ¸…ç†å’ŒéªŒè¯è¾“å…¥æ–‡æœ¬
            text = text.strip()
            if not text:
                logger.warning("ç©ºæ–‡æœ¬ä¼ å…¥embeddingï¼Œä½¿ç”¨é»˜è®¤æ–‡æœ¬")
                text = "é»˜è®¤æ–‡æœ¬"
            
            payload = {
                "model": self.model,
                "prompt": text  # ç¡®ä¿è¿™é‡Œæ˜¯å­—ç¬¦ä¸²
            }
            
            response = requests.post(
                self.embed_url,
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                if "embedding" in result:
                    embedding = result["embedding"]
                    # éªŒè¯embeddingæ˜¯å¦ä¸ºæœ‰æ•ˆçš„æµ®ç‚¹æ•°åˆ—è¡¨
                    if isinstance(embedding, list) and len(embedding) > 0:
                        return [float(x) for x in embedding]
                    else:
                        logger.error(f"Ollamaè¿”å›çš„embeddingæ ¼å¼æ— æ•ˆ: {embedding}")
                        return []
                else:
                    logger.error(f"Ollamaå“åº”ä¸­æ²¡æœ‰embeddingå­—æ®µ: {result}")
                    return []
            else:
                logger.error(f"Ollama APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}")
                return []
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Ollama APIè¯·æ±‚å¼‚å¸¸: {e}")
            return []
        except Exception as e:
            logger.error(f"è°ƒç”¨Ollama embeddingå¤±è´¥: {e}")
            return []
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥æ–‡æ¡£åˆ—è¡¨"""
        total_texts = len(texts)
        logger.info(f"ğŸš€ å¼€å§‹Ollamaå‘é‡åŒ–ï¼š{total_texts}ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        
        embeddings = []
        start_time = time.time()
        
        for i, text in enumerate(texts):
            # æ¯ä¸ªæ–‡æ¡£éƒ½æ˜¾ç¤ºè¿›åº¦ï¼ˆå¯¹äºå¤§æ‰¹é‡å¾ˆé‡è¦ï¼‰
            if i % 5 == 0 or i == total_texts - 1:  # æ¯5ä¸ªæ–‡æ¡£æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                elapsed = time.time() - start_time
                if i > 0:
                    avg_time = elapsed / i
                    remaining = (total_texts - i) * avg_time
                    print(f"\rğŸ“Š Ollamaå‘é‡åŒ–è¿›åº¦: {i+1}/{total_texts} ({(i+1)/total_texts*100:.1f}%) "
                          f"â±ï¸ å·²ç”¨æ—¶: {elapsed:.1f}s, é¢„è®¡å‰©ä½™: {remaining:.1f}s", end="", flush=True)
                else:
                    print(f"\rğŸ“Š Ollamaå‘é‡åŒ–è¿›åº¦: {i+1}/{total_texts} ({(i+1)/total_texts*100:.1f}%)", end="", flush=True)
            
            embedding = self._call_ollama_embedding(text)
            if embedding:
                embeddings.append(embedding)
            else:
                # å¦‚æœæŸä¸ªæ–‡æ¡£embeddingå¤±è´¥ï¼Œä½¿ç”¨ç©ºå‘é‡ä½œä¸ºé™çº§
                logger.warning(f"æ–‡æ¡£{i+1}çš„embeddingå¤±è´¥ï¼Œä½¿ç”¨ç©ºå‘é‡")
                if embeddings:  # å¦‚æœä¹‹å‰æœ‰æˆåŠŸçš„embeddingï¼Œä½¿ç”¨ç›¸åŒç»´åº¦çš„é›¶å‘é‡
                    dim = len(embeddings[0])
                    embeddings.append([0.0] * dim)
                else:  # å¦åˆ™ä½¿ç”¨é»˜è®¤ç»´åº¦
                    embeddings.append([0.0] * 1024)  # BGE-large-zh-v1.5é€šå¸¸æ˜¯1024ç»´
        
        total_time = time.time() - start_time
        print()  # æ¢è¡Œ
        logger.info(f"âœ… å®ŒæˆOllamaå‘é‡åŒ–ï¼š{total_texts}ä¸ªæ–‡æ¡£ï¼Œæ€»ç”¨æ—¶: {total_time:.1f}ç§’ï¼Œå¹³å‡: {total_time/total_texts:.2f}ç§’/æ–‡æ¡£")
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥å•ä¸ªæŸ¥è¯¢"""
        # ç¡®ä¿textæ˜¯å­—ç¬¦ä¸²ç±»å‹
        if not isinstance(text, str):
            text = str(text)
            
        embedding = self._call_ollama_embedding(text)
        if not embedding:
            logger.warning("æŸ¥è¯¢embeddingå¤±è´¥ï¼Œä½¿ç”¨ç©ºå‘é‡")
            # ä½¿ç”¨ä¸æ–‡æ¡£embeddingç›¸åŒçš„ç»´åº¦
            return [0.0] * 1024  # BGE-large-zh-v1.5é€šå¸¸æ˜¯1024ç»´
        
        # éªŒè¯embeddingç»´åº¦
        if not isinstance(embedding, list) or len(embedding) == 0:
            logger.warning("æ— æ•ˆçš„embeddingæ ¼å¼ï¼Œä½¿ç”¨ç©ºå‘é‡")
            return [0.0] * 1024
            
        return embedding
    
    def __call__(self, texts: List[str]) -> List[List[float]]:
        """ä½¿embeddingå¯¹è±¡å¯è°ƒç”¨ï¼Œå…¼å®¹FAISSæ¥å£"""
        return self.embed_documents(texts)


class SimpleTextSplitter:
    """ç®€å•çš„æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
    
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    # è‡ªå·±å®šä¹‰çš„æ–‡æœ¬åˆ†å‰²å™¨ï¼Œç”¨äºå°†æ–‡æ¡£åˆ†å‰²æˆæ›´å°çš„å—
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£"""
        split_docs = []
        
        for doc in documents:
            text = doc.page_content
            
            # ç®€å•åˆ†å‰²
            start = 0
            while start < len(text):
                end = start + self.chunk_size
                chunk = text[start:end]
                
                if chunk.strip():
                    new_doc = Document(
                        page_content=chunk.strip(),
                        metadata=doc.metadata.copy()
                    )
                    split_docs.append(new_doc)
                
                start = end - self.chunk_overlap
                
                if start >= len(text):
                    break
        
        return split_docs


class SimpleEmbeddings(Embeddings):
    """ç®€å•çš„embeddingï¼ˆé™çº§æ–¹æ¡ˆï¼‰ï¼Œå…¼å®¹FAISSæ¥å£"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.bert_model = None
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½embeddingæ¨¡å‹"""
        try:
            # é¦–å…ˆå°è¯•sentence-transformers
            from sentence_transformers import SentenceTransformer
            import torch
            
            if os.path.exists(self.model_path):
                # PyTorch 2.7.1 å…¼å®¹æ€§ä¿®å¤
                logger.info(f"å¼€å§‹åŠ è½½æœ¬åœ°BGEæ¨¡å‹: {self.model_path}")
                
                # è®¾ç½®PyTorchå…¼å®¹æ€§é€‰é¡¹
                original_load_state_dict = torch.nn.Module.load_state_dict
                
                def patched_load_state_dict(self, state_dict, strict=True):
                    """ä¿®å¤PyTorch 2.7.1 meta tensoré—®é¢˜"""
                    try:
                        # å¦‚æœé‡åˆ°meta tensoré—®é¢˜ï¼Œä½¿ç”¨to_empty()æ–¹æ³•
                        if hasattr(self, '_meta_registrations'):
                            for name, param in self.named_parameters():
                                if param.is_meta:
                                    param.data = param.to_empty(device='cpu')
                            for name, buffer in self.named_buffers():
                                if buffer.is_meta:
                                    buffer.data = buffer.to_empty(device='cpu')
                        return original_load_state_dict(self, state_dict, strict)
                    except Exception as e:
                        logger.warning(f"Meta tensorä¿®å¤å¤±è´¥ï¼Œå°è¯•æ ‡å‡†åŠ è½½: {e}")
                        return original_load_state_dict(self, state_dict, strict)
                
                # ä¸´æ—¶æ›¿æ¢åŠ è½½æ–¹æ³•
                torch.nn.Module.load_state_dict = patched_load_state_dict
                
                try:
                    # è®¾ç½®è®¾å¤‡ä¸ºCPUï¼Œé¿å…GPUç›¸å…³é—®é¢˜
                    self.model = SentenceTransformer(
                        self.model_path,
                        device='cpu',  # å¼ºåˆ¶ä½¿ç”¨CPU
                        cache_folder=None  # é¿å…ç¼“å­˜é—®é¢˜
                    )
                    logger.info(f"âœ… æˆåŠŸåŠ è½½æœ¬åœ°BGEæ¨¡å‹: {self.model_path}")
                    return
                finally:
                    # æ¢å¤åŸå§‹åŠ è½½æ–¹æ³•
                    torch.nn.Module.load_state_dict = original_load_state_dict
                    
            else:
                logger.warning(f"æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
                # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
                try:
                    self.model = SentenceTransformer(
                        'all-MiniLM-L6-v2',
                        device='cpu'
                    )
                    logger.warning(f"æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨åå¤‡æ¨¡å‹: all-MiniLM-L6-v2")
                    return
                except Exception as e:
                    logger.warning(f"æ— æ³•åŠ è½½é¢„è®­ç»ƒsentence-transformersæ¨¡å‹: {e}")
                    
        except Exception as e:
            logger.warning(f"sentence-transformersåŠ è½½å¤±è´¥: {e}")
        
        # å¦‚æœsentence-transformerså¤±è´¥ï¼Œå°è¯•transformersåº“
        try:
            from transformers import AutoTokenizer, AutoModel
            import torch
            logger.info("å°è¯•ä½¿ç”¨transformersåº“ä½œä¸ºåå¤‡...")
            
            # è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œé¿å…ç½‘ç»œä¸‹è½½
            os.environ['TRANSFORMERS_OFFLINE'] = '1'
            
            # å¦‚æœæœ¬åœ°æœ‰BGEæ¨¡å‹é…ç½®ï¼Œå°è¯•ä½¿ç”¨
            if os.path.exists(self.model_path):
                try:
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        self.model_path, 
                        local_files_only=True
                    )
                    self.bert_model = AutoModel.from_pretrained(
                        self.model_path, 
                        local_files_only=True,
                        torch_dtype=torch.float32  # å¼ºåˆ¶ä½¿ç”¨float32é¿å…ç²¾åº¦é—®é¢˜
                    )
                    logger.info(f"âœ… ä½¿ç”¨transformersåº“åŠ è½½æœ¬åœ°BGEæ¨¡å‹: {self.model_path}")
                    logger.info(f"ğŸ“‹ æ¨¡å‹è¯¦æƒ…: BGE-large-zh-v1.5 (1024ç»´ï¼Œé€šè¿‡transformersåº“åŠ è½½)")
                    return
                except Exception as e:
                    logger.warning(f"transformersåŠ è½½æœ¬åœ°æ¨¡å‹å¤±è´¥: {e}")
            
            # å¦‚æœæœ¬åœ°æ¨¡å‹å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ä¸­æ–‡BERTä½œä¸ºåå¤‡
            self.tokenizer = AutoTokenizer.from_pretrained(
                'bert-base-chinese',
                local_files_only=False  # å…è®¸ä¸‹è½½
            )
            self.bert_model = AutoModel.from_pretrained(
                'bert-base-chinese',
                local_files_only=False
            )
            logger.info("ä½¿ç”¨transformersåº“çš„BERTæ¨¡å‹ä½œä¸ºåå¤‡")
        except Exception as e2:
            logger.error(f"transformersåº“ä¹ŸåŠ è½½å¤±è´¥: {e2}")
            logger.warning("å°†ä½¿ç”¨éšæœºå‘é‡ä½œä¸ºæœ€ç»ˆé™çº§æ–¹æ¡ˆ")
            # ç¡®ä¿æ‰€æœ‰æ¨¡å‹å±æ€§éƒ½æ˜¯None
            self.model = None
            self.tokenizer = None
            self.bert_model = None
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥æ–‡æ¡£åˆ—è¡¨"""
        total_texts = len(texts)
        
        if self.model:
            try:
                logger.info(f"ğŸš€ å¼€å§‹BGEæ¨¡å‹å‘é‡åŒ–ï¼š{total_texts}ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                start_time = time.time()
                
                # sentence-transformerså¯ä»¥æ‰¹é‡å¤„ç†ï¼Œä½†æˆ‘ä»¬æ·»åŠ è¿›åº¦ç›‘æ§
                print(f"ğŸ“Š BGEå‘é‡åŒ–è¿›åº¦: 0/{total_texts} (0.0%) â±ï¸ å‡†å¤‡ä¸­...", end="", flush=True)
                
                embeddings = self.model.encode(texts, show_progress_bar=True)
                
                total_time = time.time() - start_time
                print(f"\rğŸ“Š BGEå‘é‡åŒ–è¿›åº¦: {total_texts}/{total_texts} (100.0%) â±ï¸ å®Œæˆï¼                    ")
                logger.info(f"âœ… å®ŒæˆBGEå‘é‡åŒ–ï¼š{total_texts}ä¸ªæ–‡æ¡£ï¼Œæ€»ç”¨æ—¶: {total_time:.1f}ç§’ï¼Œå¹³å‡: {total_time/total_texts:.3f}ç§’/æ–‡æ¡£")
                
                return embeddings.tolist()
            except Exception as e:
                logger.error(f"sentence-transformersç¼–ç å¤±è´¥: {e}")
        
        # é™çº§åˆ°transformers
        if hasattr(self, 'tokenizer') and hasattr(self, 'bert_model') and self.tokenizer is not None and self.bert_model is not None:
            try:
                import torch
                # åˆ¤æ–­ä½¿ç”¨çš„æ˜¯BGEæ¨¡å‹è¿˜æ˜¯BERTæ¨¡å‹
                model_info = "BGE-large-zh-v1.5" if self.model_path.endswith("bge-large-zh-v1.5") else "BERT"
                logger.info(f"ğŸš€ å¼€å§‹{model_info}æ¨¡å‹å‘é‡åŒ–ï¼š{total_texts}ä¸ªæ–‡æ¡£ç‰‡æ®µ (é€šè¿‡transformersåº“)")
                
                embeddings = []
                start_time = time.time()
                
                for i, text in enumerate(texts):
                    # æ˜¾ç¤ºè¯¦ç»†è¿›åº¦
                    if i % 10 == 0 or i == total_texts - 1:
                        elapsed = time.time() - start_time
                        if i > 0:
                            avg_time = elapsed / i
                            remaining = (total_texts - i) * avg_time
                            print(f"\rğŸ“Š {model_info}å‘é‡åŒ–è¿›åº¦: {i+1}/{total_texts} ({(i+1)/total_texts*100:.1f}%) "
                                  f"â±ï¸ å·²ç”¨æ—¶: {elapsed:.1f}s, é¢„è®¡å‰©ä½™: {remaining:.1f}s", end="", flush=True)
                        else:
                            print(f"\rğŸ“Š {model_info}å‘é‡åŒ–è¿›åº¦: {i+1}/{total_texts} ({(i+1)/total_texts*100:.1f}%)", end="", flush=True)
                    
                    inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
                    with torch.no_grad():
                        outputs = self.bert_model(**inputs)
                        # ä½¿ç”¨[CLS] tokençš„embedding
                        embedding = outputs.last_hidden_state[0][0].numpy().tolist()
                        embeddings.append(embedding)
                
                total_time = time.time() - start_time
                print()  # æ¢è¡Œ
                logger.info(f"âœ… å®Œæˆ{model_info}å‘é‡åŒ–ï¼š{total_texts}ä¸ªæ–‡æ¡£ï¼Œæ€»ç”¨æ—¶: {total_time:.1f}ç§’")
                return embeddings
            except Exception as e:
                logger.error(f"transformersç¼–ç å¤±è´¥: {e}")
        
        # æœ€ç»ˆé™çº§æ–¹æ¡ˆï¼šè¿”å›éšæœºå‘é‡
        import random
        dim = 384  # ä½¿ç”¨è¾ƒå°çš„ç»´åº¦
        logger.warning("ä½¿ç”¨éšæœºå‘é‡ä½œä¸ºæœ€ç»ˆé™çº§æ–¹æ¡ˆ")
        return [[random.random() for _ in range(dim)] for _ in texts]
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥å•ä¸ªæŸ¥è¯¢"""
        result = self.embed_documents([text])
        return result[0] if result else []


class LangChainVectorStore:
    """åŸºäºLangChainçš„å‘é‡å­˜å‚¨ç³»ç»Ÿ"""
    
    def __init__(self, 
                 model_name: str = "./bge-large-zh-v1.5", 
                 ollama_base_url: str = "http://localhost:11434",
                 persist_path: str = "./vectorstore",
                 enable_versioning: bool = True):
        """
        åˆå§‹åŒ–LangChainå‘é‡å­˜å‚¨
        
        Args:
            model_name: åµŒå…¥æ¨¡å‹åç§°ï¼ˆæœ¬åœ°è·¯å¾„å¦‚'./bge-large-zh-v1.5'æˆ–Ollamaæ¨¡å‹åå¦‚'nomic-embed-text:latest'ï¼‰
            ollama_base_url: OllamaæœåŠ¡å™¨åœ°å€ï¼ˆä»…åœ¨ä½¿ç”¨Ollamaæ¨¡å‹æ—¶ä½¿ç”¨ï¼‰
            persist_path: å‘é‡å­˜å‚¨æŒä¹…åŒ–è·¯å¾„
            enable_versioning: æ˜¯å¦å¯ç”¨ç‰ˆæœ¬ç®¡ç†
        """
        self.model_name = model_name
        self.ollama_base_url = ollama_base_url
        self.persist_path = persist_path
        self.enable_versioning = enable_versioning
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹ - æ™ºèƒ½é€‰æ‹©æœ¬åœ°æˆ–è¿œç¨‹æ¨¡å‹
        logger.info(f"åˆå§‹åŒ–embeddingæ¨¡å‹: {model_name}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°æ¨¡å‹è·¯å¾„
        if model_name.startswith('./') or model_name.startswith('/') or os.path.exists(model_name):
            # ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹
            logger.info(f"æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹: {model_name}")
            try:
                if HuggingFaceEmbeddings is not None:
                    # ä¼˜å…ˆä½¿ç”¨HuggingFaceEmbeddingsï¼Œæ·»åŠ PyTorch 2.7.1å…¼å®¹æ€§
                    try:
                        import torch
                        # æ£€æŸ¥PyTorchç‰ˆæœ¬ï¼Œå¦‚æœæ˜¯2.xåˆ™æ·»åŠ å…¼å®¹æ€§å¤„ç†
                        pytorch_version = torch.__version__
                        logger.info(f"æ£€æµ‹åˆ°PyTorchç‰ˆæœ¬: {pytorch_version}")
                        
                        # ä¸ºPyTorch 2.xæ·»åŠ ç‰¹æ®Šå¤„ç†
                        if pytorch_version.startswith('2.'):
                            logger.info("æ£€æµ‹åˆ°PyTorch 2.xï¼Œå¯ç”¨å…¼å®¹æ€§æ¨¡å¼")
                            # è®¾ç½®ç¯å¢ƒå˜é‡é¿å…meta tensoré—®é¢˜
                            os.environ['PYTORCH_DISABLE_LAZY_MODULE'] = '1'
                            
                        self.embeddings = HuggingFaceEmbeddings(
                            model_name=model_name,
                            model_kwargs={
                                'device': 'cpu',  # ä½¿ç”¨CPUï¼Œé¿å…GPUå†…å­˜é—®é¢˜
                                'torch_dtype': torch.float32  # å¼ºåˆ¶ä½¿ç”¨float32
                            },
                            encode_kwargs={'normalize_embeddings': True}  # å½’ä¸€åŒ–åµŒå…¥å‘é‡
                        )
                        logger.info("âœ… ä½¿ç”¨HuggingFaceEmbeddingsåŠ è½½æœ¬åœ°BGEæ¨¡å‹")
                        
                    except Exception as hf_error:
                        logger.warning(f"HuggingFaceEmbeddingsåŠ è½½å¤±è´¥: {hf_error}")
                        # é™çº§åˆ°SimpleEmbeddings
                        self.embeddings = SimpleEmbeddings(model_path=model_name)
                        logger.info("ä½¿ç”¨SimpleEmbeddingsä½œä¸ºé™çº§æ–¹æ¡ˆ")
                else:
                    # é™çº§ä½¿ç”¨SimpleEmbeddings
                    self.embeddings = SimpleEmbeddings(model_path=model_name)
                    logger.info("HuggingFaceEmbeddingsä¸å¯ç”¨ï¼Œä½¿ç”¨SimpleEmbeddingsåŠ è½½æœ¬åœ°æ¨¡å‹")
            except Exception as e:
                logger.warning(f"æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œå°è¯•ä½¿ç”¨SimpleEmbeddings")
                self.embeddings = SimpleEmbeddings(model_path=model_name)
        else:
            # ä½¿ç”¨Ollamaè¿œç¨‹åµŒå…¥æ¨¡å‹
            logger.info(f"ä½¿ç”¨OllamaåµŒå…¥æ¨¡å‹: {model_name}")
            self.embeddings = OllamaEmbeddings(model=model_name, base_url=ollama_base_url)
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        if RecursiveCharacterTextSplitter is not None:
            try:
                self.text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=500,
                    chunk_overlap=50,
                    length_function=len,
                )
                logger.info("ä½¿ç”¨RecursiveCharacterTextSplitter")
            except Exception as e:
                logger.warning(f"RecursiveCharacterTextSplitteråˆå§‹åŒ–å¤±è´¥: {e}")
                self.text_splitter = SimpleTextSplitter()
        else:
            self.text_splitter = SimpleTextSplitter()
        
        self.vectorstore = None
        
        # åˆå§‹åŒ–ç‰ˆæœ¬ç®¡ç†å™¨
        if self.enable_versioning and DocumentVersionManager is not None:
            version_file = os.path.join(os.path.dirname(persist_path), "document_versions.json")
            self.version_manager = DocumentVersionManager(version_file)
            logger.info("âœ… æ–‡æ¡£ç‰ˆæœ¬ç®¡ç†å™¨å·²å¯ç”¨")
        else:
            self.version_manager = None
            if self.enable_versioning and DocumentVersionManager is None:
                logger.warning("âš ï¸ DocumentVersionManageræ— æ³•å¯¼å…¥ï¼Œç‰ˆæœ¬ç®¡ç†åŠŸèƒ½å·²ç¦ç”¨")
            
        logger.info(f"LangChainå‘é‡å­˜å‚¨åˆå§‹åŒ–å®Œæˆï¼Œembeddingæ¨¡å‹: {model_name}")
    
    def load_vectorstore(self) -> bool:
        """åŠ è½½å·²ä¿å­˜çš„å‘é‡å­˜å‚¨"""
        try:
            if os.path.exists(self.persist_path) and FAISS is not None:
                # æ£€æŸ¥FAISSåŠ è½½æ–¹æ³•
                if hasattr(FAISS, 'load_local'):
                    self.vectorstore = FAISS.load_local(
                        self.persist_path, 
                        self.embeddings,
                        allow_dangerous_deserialization=True
                    )
                    logger.info(f"æˆåŠŸåŠ è½½å‘é‡å­˜å‚¨ï¼Œæ–‡æ¡£æ•°é‡: {self.get_document_count()}")
                    return True
                else:
                    logger.warning("FAISS.load_localæ–¹æ³•ä¸å¯ç”¨")
                    return False
            else:
                logger.warning("å‘é‡å­˜å‚¨æ–‡ä»¶ä¸å­˜åœ¨æˆ–FAISSä¸å¯ç”¨")
                return False
        except Exception as e:
            logger.error(f"åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            return False
    
    def add_documents(self, documents: List[Document], progress_callback: Optional[Callable] = None, force_update: bool = False) -> None:
        """
        æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨ï¼ˆæ”¯æŒå¢é‡æ›´æ–°å’Œå»é‡ï¼‰
        
        Args:
            documents: LangChain Documentå¯¹è±¡åˆ—è¡¨
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆæš‚æ—¶è¿˜ç”¨ä¸åˆ°ï¼Œé¢„ç•™çš„æ¥å£ï¼‰
            force_update: æ˜¯å¦å¼ºåˆ¶æ›´æ–°ï¼ˆå¿½ç•¥ç‰ˆæœ¬æ£€æŸ¥ï¼‰
        """
        try:
            # åˆ†å‰²æ–‡æ¡£
            if progress_callback:
                # æ¨¡æ‹Ÿè¿›åº¦å›è°ƒ
                from .progress_manager import ProgressInfo, ProgressStatus
                progress_info = ProgressInfo(
                    current_step=1,
                    total_steps=5,
                    step_name="æ–‡æœ¬åˆ†å—",
                    description="æ­£åœ¨åˆ†å‰²æ–‡æ¡£...",
                    status=ProgressStatus.RUNNING
                )
                progress_callback(progress_info)
            
            split_docs = self.text_splitter.split_documents(documents)
            logger.info(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œchunkæ•°é‡: {len(split_docs)}")
            
            if not split_docs:
                logger.warning("æ²¡æœ‰æ–‡æ¡£ç‰‡æ®µå¯æ·»åŠ ")
                return
            
            # ç‰ˆæœ¬ç®¡ç†å’Œå»é‡å¤„ç†
            if self.enable_versioning and self.version_manager and not force_update:
                if progress_callback:
                    progress_info = ProgressInfo(
                        current_step=2,
                        total_steps=5,
                        step_name="ç‰ˆæœ¬æ£€æŸ¥",
                        description="æ£€æŸ¥æ–‡æ¡£ç‰ˆæœ¬å’Œå»é‡...",
                        status=ProgressStatus.RUNNING
                    )
                    progress_callback(progress_info)
                
                split_docs = self._process_incremental_update(split_docs)
                
                if not split_docs:
                    logger.info("æ‰€æœ‰æ–‡æ¡£éƒ½æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼Œæ— éœ€æ›´æ–°")
                    if progress_callback:
                        progress_info = ProgressInfo(
                            current_step=5,
                            total_steps=5,
                            step_name="å®Œæˆ",
                            description="æ— éœ€æ›´æ–°",
                            status=ProgressStatus.COMPLETED
                        )
                        progress_callback(progress_info)
                    return
            
            # å‘é‡åŒ–å¤„ç†
            chunk_count = len(split_docs)
            if progress_callback:
                progress_info = ProgressInfo(
                    current_step=3,
                    total_steps=5,
                    step_name="å‘é‡åŒ–",
                    description=f"æ­£åœ¨ç”Ÿæˆ {chunk_count} ä¸ªæ–‡æ¡£ç‰‡æ®µçš„å‘é‡...",
                    status=ProgressStatus.RUNNING
                )
                progress_callback(progress_info)
            
            # æ˜¾ç¤ºå‘é‡åŒ–å¼€å§‹ä¿¡æ¯
            logger.info(f"ğŸ¯ å¼€å§‹å‘é‡åŒ–å¤„ç†ï¼š{chunk_count} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            vectorization_start = time.time()
            
            if FAISS is not None:
                # å¦‚æœå‘é‡å­˜å‚¨ä¸ºç©ºï¼Œåˆ™åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨
                if self.vectorstore is None:
                    # åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨
                    logger.info("ğŸ“š åˆ›å»ºæ–°çš„FAISSå‘é‡å­˜å‚¨...")
                    self.vectorstore = FAISS.from_documents(split_docs, self.embeddings)
                    logger.info("âœ… åˆ›å»ºæ–°çš„FAISSå‘é‡å­˜å‚¨å®Œæˆ")
                else:
                    # å¦‚æœå‘é‡å­˜å‚¨ä¸ä¸ºç©ºï¼Œåˆ™æ·»åŠ åˆ°ç°æœ‰å‘é‡å­˜å‚¨
                    if hasattr(self.vectorstore, 'add_documents'):
                        logger.info("ğŸ“š æ·»åŠ æ–‡æ¡£åˆ°ç°æœ‰FAISSå‘é‡å­˜å‚¨...")
                        self.vectorstore.add_documents(split_docs)
                        logger.info("âœ… æ–‡æ¡£æ·»åŠ åˆ°ç°æœ‰å‘é‡å­˜å‚¨å®Œæˆ")
                    else:
                        # é™çº§æ–¹æ¡ˆï¼šé‡æ–°åˆ›å»ºå‘é‡å­˜å‚¨
                        logger.warning("ä½¿ç”¨é™çº§æ–¹æ¡ˆé‡æ–°åˆ›å»ºå‘é‡å­˜å‚¨")
                        existing_docs = []  # è¿™é‡Œåº”è¯¥è·å–ç°æœ‰æ–‡æ¡£ï¼Œä½†ä¸ºç®€åŒ–å°±é‡æ–°åˆ›å»º
                        all_docs = existing_docs + split_docs
                        self.vectorstore = FAISS.from_documents(all_docs, self.embeddings)
                        logger.info("âœ… é™çº§æ–¹æ¡ˆå‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆ")
            else:
                logger.error("FAISSä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºå‘é‡å­˜å‚¨")
                raise ImportError("FAISSåº“ä¸å¯ç”¨")
            
            vectorization_time = time.time() - vectorization_start
            logger.info(f"ğŸ‰ å‘é‡åŒ–å¤„ç†å®Œæˆï¼æ€»ç”¨æ—¶: {vectorization_time:.1f}ç§’ï¼Œå¤„ç†äº† {chunk_count} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            
            # æ›´æ–°ç‰ˆæœ¬ä¿¡æ¯
            if progress_callback:
                progress_info = ProgressInfo(
                    current_step=4,
                    total_steps=5,
                    step_name="æ›´æ–°ç‰ˆæœ¬",
                    description="æ›´æ–°æ–‡æ¡£ç‰ˆæœ¬ä¿¡æ¯...",
                    status=ProgressStatus.RUNNING
                )
                progress_callback(progress_info)
            
            # æ›´æ–°ç‰ˆæœ¬ç®¡ç†å™¨
            if self.enable_versioning and self.version_manager:
                self._update_version_info(split_docs)
            
            # ä¿å­˜å‘é‡å­˜å‚¨åˆ°ç£ç›˜
            logger.info("ğŸ’¾ ä¿å­˜å‘é‡å­˜å‚¨åˆ°ç£ç›˜...")
            self.save_vectorstore()
            
            # å®Œæˆ
            if progress_callback:
                progress_info = ProgressInfo(
                    current_step=5,
                    total_steps=5,
                    step_name="å®Œæˆ",
                    description="å‘é‡å­˜å‚¨å®Œæˆå¹¶å·²ä¿å­˜",
                    status=ProgressStatus.COMPLETED
                )
                progress_callback(progress_info)
                
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {type(e).__name__}: {str(e)}")
            logger.error(f"å¼‚å¸¸è¯¦æƒ…: {repr(e)}")
            import traceback
            logger.error(f"å †æ ˆè·Ÿè¸ª: {traceback.format_exc()}")
            if progress_callback:
                from .progress_manager import ProgressInfo, ProgressStatus
                progress_info = ProgressInfo(
                    current_step=0,
                    total_steps=3,
                    step_name="é”™è¯¯",
                    description="å‘é‡å­˜å‚¨å¤±è´¥",
                    status=ProgressStatus.FAILED,
                    error_message=str(e)
                )
                progress_callback(progress_info)
            raise
    
    def save_vectorstore(self) -> None:
        """ä¿å­˜å‘é‡å­˜å‚¨åˆ°ç£ç›˜"""
        try:
            if self.vectorstore and hasattr(self.vectorstore, 'save_local'):
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(self.persist_path) if os.path.dirname(self.persist_path) else '.', exist_ok=True)
                self.vectorstore.save_local(self.persist_path)
                logger.info(f"å‘é‡å­˜å‚¨å·²ä¿å­˜åˆ°: {self.persist_path}")
            else:
                logger.warning("å‘é‡å­˜å‚¨ä¸ºç©ºæˆ–ä¿å­˜æ–¹æ³•ä¸å¯ç”¨ï¼Œæ— æ³•ä¿å­˜")
        except Exception as e:
            logger.error(f"ä¿å­˜å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            raise
    
    def similarity_search(self, query: str, k: int = 5) -> List[Document]:
        """
        ç›¸ä¼¼åº¦æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            ç›¸ä¼¼æ–‡æ¡£åˆ—è¡¨
        """
        try:
            if self.vectorstore is None:
                logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
                return []
                
            docs = self.vectorstore.similarity_search(query, k=k)
            logger.info(f"ç›¸ä¼¼åº¦æœç´¢å®Œæˆï¼ŒæŸ¥è¯¢: {query[:50]}..., è¿”å›: {len(docs)}ä¸ªç»“æœ")
            return docs
            
        except Exception as e:
            logger.error(f"ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {e}")
            return []
    
    def similarity_search_with_score(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """
        å¸¦åˆ†æ•°çš„ç›¸ä¼¼åº¦æœç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            (æ–‡æ¡£, åˆ†æ•°)å…ƒç»„åˆ—è¡¨
        """
        try:
            if self.vectorstore is None:
                logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–")
                return []
            
            # ç¡®ä¿queryæ˜¯å­—ç¬¦ä¸²ç±»å‹
            if not isinstance(query, str):
                query = str(query)
                
            # é¢„å…ˆç”ŸæˆæŸ¥è¯¢å‘é‡ä»¥éªŒè¯
            query_embedding = self.embeddings.embed_query(query)
            if not query_embedding or len(query_embedding) == 0:
                logger.warning("æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ™®é€šæœç´¢")
                docs = self.similarity_search(query, k)
                return [(doc, 0.0) for doc in docs]
                
            # æ£€æŸ¥æ–¹æ³•æ˜¯å¦å­˜åœ¨
            if hasattr(self.vectorstore, 'similarity_search_with_score'):
                docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=k)
                logger.info(f"å¸¦åˆ†æ•°ç›¸ä¼¼åº¦æœç´¢å®Œæˆï¼ŒæŸ¥è¯¢: {query[:50]}..., è¿”å›: {len(docs_with_scores)}ä¸ªç»“æœ")
                return docs_with_scores
            else:
                # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨æ™®é€šæœç´¢
                logger.warning("similarity_search_with_scoreæ–¹æ³•ä¸å¯ç”¨ï¼Œä½¿ç”¨æ™®é€šæœç´¢")
                docs = self.similarity_search(query, k)
                return [(doc, 0.0) for doc in docs]
            
        except Exception as e:
            logger.error(f"å¸¦åˆ†æ•°ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {e}")
            # é™çº§åˆ°æ™®é€šæœç´¢
            try:
                docs = self.similarity_search(query, k)
                return [(doc, 0.0) for doc in docs]
            except Exception as e2:
                logger.error(f"é™çº§æœç´¢ä¹Ÿå¤±è´¥: {e2}")
                return []
    
    def get_vectorstore(self) -> Optional:
        """è·å–å‘é‡å­˜å‚¨å¯¹è±¡"""
        return self.vectorstore
    
    def get_document_count(self) -> int:
        """è·å–æ–‡æ¡£æ•°é‡"""
        try:
            if self.vectorstore and hasattr(self.vectorstore, 'index') and hasattr(self.vectorstore.index, 'ntotal'):
                return self.vectorstore.index.ntotal
            else:
                # é™çº§æ–¹æ¡ˆï¼šå°è¯•å…¶ä»–æ–¹æ³•è·å–æ•°é‡
                if self.vectorstore and hasattr(self.vectorstore, 'docstore') and hasattr(self.vectorstore.docstore, '_dict'):
                    return len(self.vectorstore.docstore._dict)
                else:
                    # å‘é‡å­˜å‚¨ä¸ºç©ºæ—¶ä¸è®°å½•è­¦å‘Šï¼Œç›´æ¥è¿”å›0
                    # logger.warning("æ— æ³•è·å–ç¡®åˆ‡çš„æ–‡æ¡£æ•°é‡")  # æ³¨é‡Šæ‰é¢‘ç¹çš„è­¦å‘Š
                    return 0
        except Exception as e:
            logger.debug(f"è·å–æ–‡æ¡£æ•°é‡å¤±è´¥: {e}")  # æ”¹ä¸ºdebugçº§åˆ«ï¼Œé¿å…æ—¥å¿—spam
            return 0
    
    def clear_vectorstore(self) -> None:
        """æ¸…ç©ºå‘é‡å­˜å‚¨"""
        self.vectorstore = None
        logger.info("å‘é‡å­˜å‚¨å·²æ¸…ç©º")
    
    def _process_incremental_update(self, documents: List[Document]) -> List[Document]:
        """
        å¤„ç†å¢é‡æ›´æ–°ï¼Œè¿‡æ»¤å‡ºéœ€è¦æ›´æ–°çš„æ–‡æ¡£
        
        Args:
            documents: æ‰€æœ‰æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            éœ€è¦æ›´æ–°çš„æ–‡æ¡£åˆ—è¡¨
        """
        if not self.version_manager:
            return documents
        
        # æŒ‰æ–‡ä»¶è·¯å¾„åˆ†ç»„
        docs_by_file = {}
        for doc in documents:
            file_path = doc.metadata.get('file_path', doc.metadata.get('source', ''))
            if file_path not in docs_by_file:
                docs_by_file[file_path] = []
            docs_by_file[file_path].append(doc)
        
        new_documents = []
        updated_files = []
        
        for file_path, file_docs in docs_by_file.items():
            if not file_path or file_path == 'unknown':
                # æ— æ³•è¯†åˆ«æ¥æºçš„æ–‡æ¡£ï¼Œç›´æ¥æ·»åŠ 
                new_documents.extend(file_docs)
                continue
            
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å‘ç”Ÿå˜åŒ–
            if self.version_manager.is_document_changed(file_path, file_docs):
                # æ–‡æ¡£æœ‰å˜åŒ–ï¼Œè·å–æ–°å¢çš„chunks
                new_chunks = self.version_manager.get_new_chunks(file_path, file_docs)
                new_documents.extend(new_chunks)
                updated_files.append(file_path)
            else:
                logger.debug(f"æ–‡æ¡£æœªå˜åŒ–ï¼Œè·³è¿‡: {file_path}")
        
        if updated_files:
            logger.info(f"æ£€æµ‹åˆ° {len(updated_files)} ä¸ªæ–‡ä»¶éœ€è¦æ›´æ–°: {updated_files}")
        
        return new_documents
    
    def _update_version_info(self, documents: List[Document]):
        """
        æ›´æ–°æ–‡æ¡£ç‰ˆæœ¬ä¿¡æ¯
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
        """
        if not self.version_manager:
            return
        
        # æŒ‰æ–‡ä»¶è·¯å¾„åˆ†ç»„
        docs_by_file = {}
        for doc in documents:
            file_path = doc.metadata.get('file_path', doc.metadata.get('source', ''))
            if file_path and file_path != 'unknown':
                if file_path not in docs_by_file:
                    docs_by_file[file_path] = []
                docs_by_file[file_path].append(doc)
        
        # æ›´æ–°æ¯ä¸ªæ–‡ä»¶çš„ç‰ˆæœ¬ä¿¡æ¯
        for file_path, file_docs in docs_by_file.items():
            try:
                self.version_manager.update_document_version(file_path, file_docs)
            except Exception as e:
                logger.error(f"æ›´æ–°æ–‡æ¡£ç‰ˆæœ¬å¤±è´¥ {file_path}: {e}")
    
    def rebuild_vectorstore(self, documents: List[Document] = None, progress_callback: Optional[Callable] = None) -> bool:
        """
        é‡å»ºå‘é‡å­˜å‚¨ï¼ˆæ¸…ç©ºç°æœ‰æ•°æ®å¹¶é‡æ–°æ„å»ºï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneåˆ™é‡æ–°åŠ è½½æ‰€æœ‰æ–‡æ¡£ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            æ˜¯å¦æˆåŠŸé‡å»º
        """
        try:
            logger.info("å¼€å§‹é‡å»ºå‘é‡å­˜å‚¨...")
            
            # æ¸…ç©ºç°æœ‰å‘é‡å­˜å‚¨
            self.clear_vectorstore()
            
            # é‡ç½®ç‰ˆæœ¬ç®¡ç†å™¨
            if self.version_manager:
                self.version_manager.reset_all()
                logger.info("ç‰ˆæœ¬ä¿¡æ¯å·²é‡ç½®")
            
            # å¦‚æœæ²¡æœ‰æä¾›æ–‡æ¡£ï¼Œåˆ™éœ€è¦é‡æ–°åŠ è½½
            if documents is None:
                logger.warning("é‡å»ºå‘é‡å­˜å‚¨éœ€è¦æä¾›æ–‡æ¡£åˆ—è¡¨")
                return False
            
            # å¼ºåˆ¶æ·»åŠ æ‰€æœ‰æ–‡æ¡£
            self.add_documents(documents, progress_callback=progress_callback, force_update=True)
            
            logger.info("å‘é‡å­˜å‚¨é‡å»ºå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"é‡å»ºå‘é‡å­˜å‚¨å¤±è´¥: {e}")
            return False
    
    def get_version_statistics(self) -> Dict:
        """è·å–ç‰ˆæœ¬ç®¡ç†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.version_manager:
            return {"versioning_enabled": False}
        
        stats = self.version_manager.get_statistics()
        stats["versioning_enabled"] = True
        stats["vectorstore_doc_count"] = self.get_document_count()
        
        return stats
    
    def cleanup_orphaned_documents(self) -> int:
        """æ¸…ç†å­¤ç«‹çš„æ–‡æ¡£ç‰ˆæœ¬ä¿¡æ¯"""
        if not self.version_manager:
            return 0
        
        return self.version_manager.cleanup_missing_files() 
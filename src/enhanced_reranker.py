"""
å¢å¼ºçš„é‡æ’åºå™¨
ä¼˜åŒ–Cross-Encoderé‡æ’åºï¼Œç‰¹åˆ«é’ˆå¯¹ç«èµ›ä»»åŠ¡éœ€æ±‚è¿›è¡Œä¼˜åŒ–
"""

from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import numpy as np
from loguru import logger
from langchain_core.documents import Document
import torch
import re

try:
    from sentence_transformers import CrossEncoder
    # å…¼å®¹æ€§å¤„ç†ï¼šä¿®å¤assignå‚æ•°é—®é¢˜
    import inspect
    if hasattr(torch.nn.Module, 'load_state_dict'):
        original_load_state_dict = torch.nn.Module.load_state_dict
        
        def patched_load_state_dict(self, state_dict, strict=True, assign=None):
            sig = inspect.signature(original_load_state_dict)
            if 'assign' in sig.parameters:
                return original_load_state_dict(self, state_dict, strict=strict, assign=assign)
            else:
                return original_load_state_dict(self, state_dict, strict=strict)
        
        torch.nn.Module.load_state_dict = patched_load_state_dict
        logger.info("å·²åº”ç”¨PyTorchå…¼å®¹æ€§ä¿®å¤")
        
except ImportError:
    CrossEncoder = None
    logger.warning("CrossEncoderæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„é‡æ’æ–¹æ³•")
except Exception as e:
    logger.warning(f"CrossEncoderå…¼å®¹æ€§ä¿®å¤å¤±è´¥: {e}")
    CrossEncoder = None

@dataclass
class EnhancedRerankResult:
    """å¢å¼ºé‡æ’ç»“æœ"""
    content: str
    metadata: Dict
    source: str
    original_score: float
    crossencoder_score: float
    entity_bonus: float  # å®ä½“å‘½ä¸­å¥–åŠ±
    task_relevance_score: float  # ä»»åŠ¡ç›¸å…³æ€§åˆ†æ•°
    final_score: float
    rank: int
    matched_entities: List[str]  # åŒ¹é…åˆ°çš„å®ä½“
    task_indicators: List[str]   # ä»»åŠ¡æŒ‡æ ‡

class EnhancedReranker:
    """å¢å¼ºçš„é‡æ’åºå™¨ - ä¸“é—¨ä¼˜åŒ–ç«èµ›ä»»åŠ¡ç›¸å…³æ€§"""
    
    def __init__(self, 
                 model_name: str = './cross-encoder/ms-marco-MiniLM-L6-v2',
                 fallback_enabled: bool = True,
                 entity_bonus_weight: float = 0.3,
                 task_relevance_weight: float = 0.2):
        """
        åˆå§‹åŒ–å¢å¼ºé‡æ’å™¨
        
        Args:
            model_name: CrossEncoderæ¨¡å‹åç§°
            fallback_enabled: æ˜¯å¦å¯ç”¨é™çº§æ–¹æ¡ˆ
            entity_bonus_weight: å®ä½“å¥–åŠ±æƒé‡
            task_relevance_weight: ä»»åŠ¡ç›¸å…³æ€§æƒé‡
        """
        self.model_name = model_name
        self.fallback_enabled = fallback_enabled
        self.entity_bonus_weight = entity_bonus_weight
        self.task_relevance_weight = task_relevance_weight
        self.rerank_model = None
        
        # é‡è¦å®ä½“è¯å…¸ï¼ˆç”¨äºå®ä½“å‘½ä¸­å¥–åŠ±ï¼‰
        self.important_entities = {
            # ç«èµ›ç›¸å…³
            "æœªæ¥æ ¡å›­æ™ºèƒ½åº”ç”¨ä¸“é¡¹èµ›": 2.0,
            "æ™ºèƒ½äº¤é€šä¿¡å·ç¯": 1.8,
            "æ³°è¿ªæ¯": 1.5,
            "æ•°æ®æŒ–æ˜æŒ‘æˆ˜èµ›": 1.3,
            
            # ä»»åŠ¡å…³é”®è¯
            "åŸºæœ¬è¦æ±‚": 1.6,
            "æŠ€æœ¯è¦æ±‚": 1.6,
            "ä»»åŠ¡æè¿°": 1.5,
            "è¯„åˆ†æ ‡å‡†": 1.4,
            "å®ç°æ–¹æ¡ˆ": 1.3,
            
            # æŠ€æœ¯å…³é”®è¯
            "äº¤é€šä¿¡å·ç¯": 1.7,
            "ä¿¡å·æ§åˆ¶": 1.4,
            "æ™ºèƒ½æ§åˆ¶": 1.3,
            "ç®—æ³•è®¾è®¡": 1.2,
            "ä¼˜åŒ–æ–¹æ¡ˆ": 1.2,
            
            # è¯„ä¼°æŒ‡æ ‡
            "åˆ›æ–°æ€§": 1.1,
            "å®ç”¨æ€§": 1.1,
            "å¯è¡Œæ€§": 1.1,
            "å®Œæ•´æ€§": 1.0
        }
        
        # ä»»åŠ¡æŒ‡æ ‡å…³é”®è¯
        self.task_indicators = {
            "ä»»åŠ¡": 1.5,
            "è¦æ±‚": 1.4,
            "è®¾è®¡": 1.3,
            "å®ç°": 1.2,
            "æ–¹æ¡ˆ": 1.2,
            "ç®—æ³•": 1.1,
            "ä¼˜åŒ–": 1.1,
            "æ§åˆ¶": 1.1,
            "ç³»ç»Ÿ": 1.0
        }
        
        # åˆå§‹åŒ–é‡æ’æ¨¡å‹
        self._init_rerank_model()
        
        logger.info(f"å¢å¼ºé‡æ’å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  æ¨¡å‹: {model_name}")
        logger.info(f"  æƒé‡é…ç½®: å®ä½“å¥–åŠ±={entity_bonus_weight}, ä»»åŠ¡ç›¸å…³æ€§={task_relevance_weight}")
        logger.info(f"  é‡è¦å®ä½“æ•°é‡: {len(self.important_entities)}")
    
    def _init_rerank_model(self):
        """åˆå§‹åŒ–é‡æ’æ¨¡å‹"""
        if CrossEncoder is None:
            logger.warning("CrossEncoderä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨é™çº§æ–¹æ¡ˆ")
            return
        
        try:
            # å°è¯•åŠ è½½æ¨¡å‹
            self.rerank_model = CrossEncoder(self.model_name)
            logger.info(f"CrossEncoderæ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_name}")
        except Exception as e:
            logger.warning(f"CrossEncoderæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å°è¯•ç¦»çº¿æ¨¡å¼
            try:
                import os
                os.environ['TRANSFORMERS_OFFLINE'] = '1'
                self.rerank_model = CrossEncoder(self.model_name, trust_remote_code=False)
                logger.info(f"CrossEncoderç¦»çº¿æ¨¡å¼åŠ è½½æˆåŠŸ: {self.model_name}")
            except Exception as e2:
                logger.warning(f"ç¦»çº¿æ¨¡å¼ä¹Ÿå¤±è´¥: {e2}")
                if self.fallback_enabled:
                    logger.info("å¯ç”¨é™çº§é‡æ’æ–¹æ¡ˆ")
                else:
                    raise e
    
    def rerank(self, question: str, docs: List[Document], top_k: int = None) -> List[Document]:
        """
        å¯¹æ–‡æ¡£è¿›è¡Œå¢å¼ºé‡æ’åº
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            docs: æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›çš„top-kç»“æœæ•°é‡
        
        Returns:
            List[Document]: é‡æ’åçš„æ–‡æ¡£åˆ—è¡¨
        """
        if not docs:
            return []

        logger.info(f"å¼€å§‹å¢å¼ºé‡æ’åºï¼Œé—®é¢˜: '{question[:50]}...', æ–‡æ¡£æ•°é‡: {len(docs)}")
        
        # ä½¿ç”¨å¢å¼ºCrossEncoderé‡æ’
        if self.rerank_model is not None:
            rerank_results = self._enhanced_crossencoder_rerank(question, docs)
        else:
            # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨å¢å¼ºçš„è§„åˆ™é‡æ’æ–¹æ³•
            rerank_results = self._enhanced_fallback_rerank(question, docs)
        
        # æ’åº
        rerank_results.sort(key=lambda x: x.final_score, reverse=True)
        
        # è®¾ç½®æœ€ç»ˆæ’å
        for i, result in enumerate(rerank_results):
            result.rank = i + 1
        
        # å°†ç»“æœè½¬æ¢ä¸ºDocumentå¯¹è±¡
        reranked_docs = []
        for i, result in enumerate(rerank_results):
            # æ‰¾åˆ°åŸå§‹Documentå¯¹è±¡
            original_doc = None
            for doc in docs:
                if (hasattr(doc, 'page_content') and 
                    doc.page_content == result.content):
                    original_doc = doc
                    break
            
            if original_doc is not None:
                # å¤åˆ¶åŸå§‹Documentå¹¶æ·»åŠ é‡æ’åºä¿¡æ¯
                new_metadata = original_doc.metadata.copy()
                new_metadata.update({
                    'crossencoder_score': result.crossencoder_score,
                    'entity_bonus': result.entity_bonus,
                    'task_relevance_score': result.task_relevance_score,
                    'final_rerank_score': result.final_score,
                    'rerank_rank': result.rank,
                    'matched_entities': result.matched_entities,
                    'task_indicators': result.task_indicators,
                    'original_score': result.original_score
                })
                
                new_doc = Document(
                    page_content=original_doc.page_content,
                    metadata=new_metadata
                )
                reranked_docs.append(new_doc)
        
        # è¿”å›top-kç»“æœ
        if top_k:
            reranked_docs = reranked_docs[:top_k]
        
        # è®°å½•é‡æ’ç»Ÿè®¡
        self._log_rerank_stats(question, rerank_results[:5])
        
        logger.info(f"å¢å¼ºé‡æ’åºå®Œæˆï¼Œè¿”å› {len(reranked_docs)} ä¸ªç»“æœ")
        return reranked_docs
    
    def _enhanced_crossencoder_rerank(self, question: str, docs: List) -> List[EnhancedRerankResult]:
        """ä½¿ç”¨å¢å¼ºCrossEncoderè¿›è¡Œé‡æ’"""
        try:
            # å‡†å¤‡è¾“å…¥å¯¹
            pairs = []
            for doc in docs:
                if hasattr(doc, 'page_content'):
                    content = doc.page_content
                else:
                    content = doc.get('content', '')
                
                # ä¼˜åŒ–è¾“å…¥å¯¹æ ¼å¼ï¼Œçªå‡ºä»»åŠ¡ç›¸å…³æ€§
                enhanced_question = self._enhance_question_for_crossencoder(question)
                enhanced_content = self._enhance_content_for_crossencoder(content)
                
                pairs.append([enhanced_question, enhanced_content])
            
            # è®¡ç®—CrossEncoderåˆ†æ•°
            crossencoder_scores = self.rerank_model.predict(pairs)
            
            # æ„å»ºå¢å¼ºç»“æœ
            results = []
            for i, (doc, ce_score) in enumerate(zip(docs, crossencoder_scores)):
                # è·å–æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®
                if hasattr(doc, 'page_content'):
                    content = doc.page_content
                    metadata = doc.metadata
                    source = metadata.get('source', '')
                    original_score = metadata.get('score', 0.0)
                else:
                    content = doc.get('content', '')
                    metadata = doc.get('metadata', {})
                    source = doc.get('source', '')
                    original_score = doc.get('score', 0.0)
                
                # è®¡ç®—å®ä½“å‘½ä¸­å¥–åŠ±
                entity_bonus, matched_entities = self._calculate_entity_bonus(question, content)
                
                # è®¡ç®—ä»»åŠ¡ç›¸å…³æ€§åˆ†æ•°
                task_score, task_indicators = self._calculate_task_relevance(question, content)
                
                # è®¡ç®—æœ€ç»ˆåˆ†æ•°
                final_score = (
                    0.5 * float(ce_score) +  # CrossEncoderåˆ†æ•°æƒé‡
                    0.2 * original_score +   # åŸå§‹åˆ†æ•°æƒé‡
                    self.entity_bonus_weight * entity_bonus +  # å®ä½“å¥–åŠ±
                    self.task_relevance_weight * task_score    # ä»»åŠ¡ç›¸å…³æ€§
                )
                
                result = EnhancedRerankResult(
                    content=content,
                    metadata=metadata,
                    source=source,
                    original_score=original_score,
                    crossencoder_score=float(ce_score),
                    entity_bonus=entity_bonus,
                    task_relevance_score=task_score,
                    final_score=final_score,
                    rank=0,
                    matched_entities=matched_entities,
                    task_indicators=task_indicators
                )
                results.append(result)
            
            logger.debug(f"å¢å¼ºCrossEncoderé‡æ’å®Œæˆï¼ŒCEåˆ†æ•°èŒƒå›´: {min(crossencoder_scores):.3f} - {max(crossencoder_scores):.3f}")
            return results
            
        except Exception as e:
            logger.error(f"å¢å¼ºCrossEncoderé‡æ’å¤±è´¥: {e}")
            return self._enhanced_fallback_rerank(question, docs)
    
    def _enhance_question_for_crossencoder(self, question: str) -> str:
        """ä¼˜åŒ–é—®é¢˜æ ¼å¼ä»¥æé«˜CrossEncoderæ•ˆæœ"""
        # çªå‡ºå…³é”®è¯
        enhanced = question
        
        # å¦‚æœé—®é¢˜ä¸­åŒ…å«é‡è¦å®ä½“ï¼Œç»™äºˆç‰¹æ®Šæ ‡è®°
        for entity in self.important_entities:
            if entity in question:
                enhanced = enhanced.replace(entity, f"[é‡è¦]{entity}[/é‡è¦]")
        
        # æ·»åŠ ä»»åŠ¡æŒ‡æ ‡æ ‡è®°
        for indicator in self.task_indicators:
            if indicator in question:
                enhanced = enhanced.replace(indicator, f"[ä»»åŠ¡]{indicator}[/ä»»åŠ¡]")
        
        return enhanced
    
    def _enhance_content_for_crossencoder(self, content: str) -> str:
        """ä¼˜åŒ–æ–‡æ¡£å†…å®¹æ ¼å¼ä»¥æé«˜CrossEncoderæ•ˆæœ"""
        # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œä¿ç•™æœ€ç›¸å…³éƒ¨åˆ†
        if len(content) > 512:
            # æŸ¥æ‰¾åŒ…å«é‡è¦å®ä½“çš„æ®µè½
            sentences = content.split('ã€‚')
            important_sentences = []
            
            for sentence in sentences:
                for entity in self.important_entities:
                    if entity in sentence:
                        important_sentences.append(sentence)
                        break
            
            if important_sentences:
                # å¦‚æœæ‰¾åˆ°é‡è¦å¥å­ï¼Œä¼˜å…ˆä¿ç•™
                enhanced = 'ã€‚'.join(important_sentences[:3]) + 'ã€‚'
                if len(enhanced) < 400:
                    # å¦‚æœé‡è¦å¥å­ä¸å¤Ÿï¼Œè¡¥å……å…¶ä»–å¥å­
                    remaining = content.replace(enhanced, '')
                    enhanced += remaining[:400-len(enhanced)]
            else:
                # å¦åˆ™å–å‰400å­—ç¬¦
                enhanced = content[:400]
        else:
            enhanced = content
        
        return enhanced
    
    def _calculate_entity_bonus(self, question: str, content: str) -> Tuple[float, List[str]]:
        """è®¡ç®—å®ä½“å‘½ä¸­å¥–åŠ±"""
        bonus = 0.0
        matched_entities = []
        
        # æ£€æŸ¥é—®é¢˜ä¸­çš„å®ä½“åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°
        for entity, weight in self.important_entities.items():
            if entity in question and entity in content:
                # è®¡ç®—å®ä½“åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°é¢‘ç‡
                count = content.count(entity)
                entity_bonus = weight * min(count * 0.1, 1.0)  # æœ€å¤§1.0çš„å¥–åŠ±
                bonus += entity_bonus
                matched_entities.append(entity)
        
        # å½’ä¸€åŒ–å¥–åŠ±åˆ†æ•°
        bonus = min(bonus, 2.0)  # æœ€å¤§2.0çš„å¥–åŠ±
        
        return bonus, matched_entities
    
    def _calculate_task_relevance(self, question: str, content: str) -> Tuple[float, List[str]]:
        """è®¡ç®—ä»»åŠ¡ç›¸å…³æ€§åˆ†æ•°"""
        relevance = 0.0
        found_indicators = []
        
        # æ£€æŸ¥ä»»åŠ¡æŒ‡æ ‡è¯
        for indicator, weight in self.task_indicators.items():
            if indicator in question:
                count_in_content = content.count(indicator)
                if count_in_content > 0:
                    indicator_score = weight * min(count_in_content * 0.1, 0.5)
                    relevance += indicator_score
                    found_indicators.append(indicator)
        
        # ç‰¹æ®Šå¥–åŠ±ï¼šå¦‚æœæ–‡æ¡£åŒæ—¶åŒ…å«å¤šä¸ªä»»åŠ¡æŒ‡æ ‡
        if len(found_indicators) >= 3:
            relevance += 0.3  # å¤šæŒ‡æ ‡å¥–åŠ±
        
        # å½’ä¸€åŒ–
        relevance = min(relevance, 1.5)
        
        return relevance, found_indicators
    
    def _enhanced_fallback_rerank(self, question: str, docs: List) -> List[EnhancedRerankResult]:
        """å¢å¼ºçš„é™çº§é‡æ’æ–¹æ¡ˆ"""
        logger.info("ä½¿ç”¨å¢å¼ºé™çº§é‡æ’æ–¹æ¡ˆ")
        
        results = []
        question_lower = question.lower()
        
        for doc in docs:
            # è·å–æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®
            if hasattr(doc, 'page_content'):
                content = doc.page_content
                metadata = doc.metadata
                source = metadata.get('source', '')
                original_score = metadata.get('score', 0.0)
            else:
                content = doc.get('content', '')
                metadata = doc.get('metadata', {})
                source = doc.get('source', '')
                original_score = doc.get('score', 0.0)
            
            content_lower = content.lower()
            
            # è®¡ç®—åŸºç¡€ç›¸å…³æ€§åˆ†æ•°
            base_score = self._calculate_enhanced_relevance(question_lower, content_lower)
            
            # è®¡ç®—å®ä½“å‘½ä¸­å¥–åŠ±
            entity_bonus, matched_entities = self._calculate_entity_bonus(question, content)
            
            # è®¡ç®—ä»»åŠ¡ç›¸å…³æ€§åˆ†æ•°
            task_score, task_indicators = self._calculate_task_relevance(question, content)
            
            # ç»¼åˆåˆ†æ•°
            final_score = (
                0.5 * base_score +
                0.2 * original_score +
                self.entity_bonus_weight * entity_bonus +
                self.task_relevance_weight * task_score
            )
            
            result = EnhancedRerankResult(
                content=content,
                metadata=metadata,
                source=source,
                original_score=original_score,
                crossencoder_score=base_score,  # åœ¨é™çº§æ¨¡å¼ä¸‹ä½¿ç”¨åŸºç¡€åˆ†æ•°
                entity_bonus=entity_bonus,
                task_relevance_score=task_score,
                final_score=final_score,
                rank=0,
                matched_entities=matched_entities,
                task_indicators=task_indicators
            )
            results.append(result)
        
        return results
    
    def _calculate_enhanced_relevance(self, question: str, content: str) -> float:
        """è®¡ç®—å¢å¼ºçš„ç›¸å…³æ€§åˆ†æ•°"""
        score = 0.0
        
        # 1. å…³é”®è¯åŒ¹é…
        question_words = question.split()
        for word in question_words:
            if len(word) > 1:
                count = content.count(word)
                score += min(count * 0.1, 0.5)
        
        # 2. çŸ­è¯­åŒ¹é…
        if len(question) > 10:
            phrases = [question[i:i+10] for i in range(len(question)-9)]
            for phrase in phrases:
                if phrase in content:
                    score += 0.3
        
        # 3. æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–
        score = score / (len(content) / 1000 + 1)
        
        return min(score, 2.0)
    
    def _log_rerank_stats(self, question: str, top_results: List[EnhancedRerankResult]):
        """è®°å½•é‡æ’ç»Ÿè®¡ä¿¡æ¯"""
        try:
            logger.info("ğŸ“Š å¢å¼ºé‡æ’åºTop5ç»Ÿè®¡:")
            logger.info(f"  æŸ¥è¯¢: '{question[:50]}...'")
            
            for i, result in enumerate(top_results):
                source_file = result.source.split('/')[-1] if result.source else 'unknown'
                logger.info(f"  {i+1}. {source_file}")
                logger.info(f"     æœ€ç»ˆåˆ†æ•°: {result.final_score:.4f}")
                logger.info(f"     CEåˆ†æ•°: {result.crossencoder_score:.4f}")
                logger.info(f"     å®ä½“å¥–åŠ±: {result.entity_bonus:.4f} ({len(result.matched_entities)}ä¸ªå®ä½“)")
                logger.info(f"     ä»»åŠ¡ç›¸å…³æ€§: {result.task_relevance_score:.4f} ({len(result.task_indicators)}ä¸ªæŒ‡æ ‡)")
                if result.matched_entities:
                    logger.info(f"     åŒ¹é…å®ä½“: {result.matched_entities}")
                
        except Exception as e:
            logger.warning(f"è®°å½•é‡æ’ç»Ÿè®¡å¤±è´¥: {e}")
    
    def get_rerank_analysis(self, question: str, docs: List[Document]) -> Dict:
        """
        è·å–é‡æ’åˆ†ææŠ¥å‘Š
        
        Args:
            question: æŸ¥è¯¢é—®é¢˜
            docs: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            åŒ…å«é‡æ’åˆ†æçš„è¯¦ç»†æŠ¥å‘Š
        """
        try:
            # æ‰§è¡Œé‡æ’ä½†ä¸ä¿®æ”¹åŸå§‹æ–‡æ¡£
            if self.rerank_model is not None:
                rerank_results = self._enhanced_crossencoder_rerank(question, docs)
            else:
                rerank_results = self._enhanced_fallback_rerank(question, docs)
            
            # æ’åº
            rerank_results.sort(key=lambda x: x.final_score, reverse=True)
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            analysis = {
                'query': question,
                'total_docs': len(docs),
                'rerank_method': 'CrossEncoder' if self.rerank_model else 'Enhanced Fallback',
                'top_results': [],
                'entity_analysis': {},
                'task_analysis': {},
                'score_distribution': {}
            }
            
            # Topç»“æœåˆ†æ
            for i, result in enumerate(rerank_results[:5]):
                analysis['top_results'].append({
                    'rank': i + 1,
                    'source': result.source,
                    'final_score': result.final_score,
                    'crossencoder_score': result.crossencoder_score,
                    'entity_bonus': result.entity_bonus,
                    'task_relevance_score': result.task_relevance_score,
                    'matched_entities': result.matched_entities,
                    'task_indicators': result.task_indicators,
                    'content_preview': result.content[:200]
                })
            
            # å®ä½“åˆ†æ
            all_entities = set()
            for result in rerank_results:
                all_entities.update(result.matched_entities)
            
            for entity in all_entities:
                count = sum(1 for r in rerank_results if entity in r.matched_entities)
                analysis['entity_analysis'][entity] = {
                    'frequency': count,
                    'weight': self.important_entities.get(entity, 0)
                }
            
            # ä»»åŠ¡æŒ‡æ ‡åˆ†æ
            all_indicators = set()
            for result in rerank_results:
                all_indicators.update(result.task_indicators)
            
            for indicator in all_indicators:
                count = sum(1 for r in rerank_results if indicator in r.task_indicators)
                analysis['task_analysis'][indicator] = {
                    'frequency': count,
                    'weight': self.task_indicators.get(indicator, 0)
                }
            
            # åˆ†æ•°åˆ†å¸ƒ
            scores = [r.final_score for r in rerank_results]
            analysis['score_distribution'] = {
                'min': min(scores),
                'max': max(scores),
                'mean': sum(scores) / len(scores),
                'std': np.std(scores)
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆé‡æ’åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)} 
"""
ä¼˜åŒ–çš„RAGç³»ç»Ÿ
é›†æˆæ‰€æœ‰5ä¸ªä¼˜åŒ–æ¨¡å—ï¼Œå®ç°å®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆä¼˜åŒ–æ–¹æ¡ˆ
"""

from typing import List, Dict, Optional, Callable, Tuple
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama
from loguru import logger
import os
import time
from datetime import datetime

# å¯¼å…¥ä¼˜åŒ–æ¨¡å—
try:
    from .advanced_hybrid_retriever import AdvancedHybridRetriever
    from .enhanced_reranker import EnhancedReranker
    from .enhanced_document_loader import EnhancedDocumentLoader
    from .enhanced_prompt_manager import EnhancedPromptManager
    from .langchain_vectorstore import LangChainVectorStore
except ImportError:
    from advanced_hybrid_retriever import AdvancedHybridRetriever
    from enhanced_reranker import EnhancedReranker
    from enhanced_document_loader import EnhancedDocumentLoader
    from enhanced_prompt_manager import EnhancedPromptManager
    from langchain_vectorstore import LangChainVectorStore

class OptimizedRAGResponse:
    """ä¼˜åŒ–RAGå“åº”å¯¹è±¡"""
    
    def __init__(self, question: str, answer: str, source_documents: List[Document],
                 retrieval_stats: Dict, rerank_stats: Dict, prompt_analysis: Dict,
                 response_time: float):
        self.question = question
        self.answer = answer
        self.source_documents = source_documents
        self.retrieval_stats = retrieval_stats
        self.rerank_stats = rerank_stats
        self.prompt_analysis = prompt_analysis
        self.response_time = response_time
        self.timestamp = datetime.now()
        
        # æ·»åŠ å…¼å®¹æ€§å±æ€§ - åŸºäºsource_documentsæ„å»ºretrieval_results
        from .langchain_retriever import RetrievalResult
        self.retrieval_results = [
            RetrievalResult(
                document=doc,
                score=1.0 - (i * 0.1),  # åŸºäºæ’åçš„æ¨¡æ‹Ÿåˆ†æ•°
                source="hybrid",  # é»˜è®¤ä¸ºæ··åˆæ£€ç´¢
                rank=i + 1
            )
            for i, doc in enumerate(source_documents)
        ]

class OptimizedRAGSystem:
    """ä¼˜åŒ–çš„RAGç³»ç»Ÿ - é›†æˆæ‰€æœ‰ä¼˜åŒ–æ¨¡å—"""
    
    def __init__(self,
                 llm_model: str = "deepseek-r1:7b",
                 embedding_model: str = "./bge-large-zh-v1.5",
                 base_url: str = "http://localhost:11434",
                 vector_weight: float = 0.4, # å‘é‡æ£€ç´¢æƒé‡
                 bm25_weight: float = 0.6, # BM25æ£€ç´¢æƒé‡
                 enable_reranking: bool = True, # æ˜¯å¦å¯ç”¨é‡æ’åº
                 enable_chapter_splitting: bool = True, # æ˜¯å¦å¯ç”¨ç« èŠ‚åˆ‡åˆ†
                 retrieval_k: int = 10 # æ£€ç´¢æ•°é‡
                 ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–RAGç³»ç»Ÿ
        Args:
            llm_model: è¯­è¨€æ¨¡å‹åç§°
            embedding_model: åµŒå…¥æ¨¡å‹è·¯å¾„
            base_url: OllamaæœåŠ¡åœ°å€
            vector_weight: å‘é‡æ£€ç´¢æƒé‡
            bm25_weight: BM25æ£€ç´¢æƒé‡
            enable_reranking: æ˜¯å¦å¯ç”¨é‡æ’åº
            enable_chapter_splitting: æ˜¯å¦å¯ç”¨ç« èŠ‚åˆ‡åˆ†
            retrieval_k: æ£€ç´¢è¿”å›æ•°é‡
        """
        self.llm_model = llm_model
        self.embedding_model = embedding_model
        self.base_url = base_url
        self.vector_weight = vector_weight
        self.bm25_weight = bm25_weight
        self.enable_reranking = enable_reranking
        self.enable_chapter_splitting = enable_chapter_splitting
        self.retrieval_k = retrieval_k
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.vectorstore = None
        self.advanced_retriever = None
        self.enhanced_reranker = None
        self.enhanced_document_loader = None
        self.enhanced_prompt_manager = None
        self.llm = None
        self.rag_chain = None
        
        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self._initialize_components()
        
        logger.info("ä¼˜åŒ–RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  è¯­è¨€æ¨¡å‹: {llm_model}")
        logger.info(f"  åµŒå…¥æ¨¡å‹: {embedding_model}")
        logger.info(f"  æ£€ç´¢æƒé‡: å‘é‡={vector_weight}, BM25={bm25_weight}")
        logger.info(f"  åŠŸèƒ½é…ç½®: é‡æ’åº={enable_reranking}, ç« èŠ‚åˆ‡åˆ†={enable_chapter_splitting}")
    
    def _initialize_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        logger.info("åˆå§‹åŒ–ä¼˜åŒ–RAGç³»ç»Ÿç»„ä»¶...")
        
        # 1. åˆå§‹åŒ–å¢å¼ºæ–‡æ¡£åŠ è½½å™¨
        logger.info("åˆå§‹åŒ–å¢å¼ºæ–‡æ¡£åŠ è½½å™¨...")
        self.enhanced_document_loader = EnhancedDocumentLoader()
        
        # 2. åˆå§‹åŒ–å‘é‡å­˜å‚¨
        logger.info("åˆå§‹åŒ–å‘é‡å­˜å‚¨...")
        self.vectorstore = LangChainVectorStore(
            model_name=self.embedding_model,
            ollama_base_url=self.base_url,
            enable_versioning=True
        )
        
        # 3. åˆå§‹åŒ–å¢å¼ºé‡æ’åºå™¨
        if self.enable_reranking:
            logger.info("åˆå§‹åŒ–å¢å¼ºé‡æ’åºå™¨...")
            self.enhanced_reranker = EnhancedReranker()
        
        # 4. åˆå§‹åŒ–å¢å¼ºæç¤ºè¯ç®¡ç†å™¨
        logger.info("åˆå§‹åŒ–å¢å¼ºæç¤ºè¯ç®¡ç†å™¨...")
        self.enhanced_prompt_manager = EnhancedPromptManager()
        
        # 5. åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
        logger.info("åˆå§‹åŒ–è¯­è¨€æ¨¡å‹...")
        self.llm = ChatOllama(
            model=self.llm_model,
            base_url=self.base_url,
            temperature=0.1,
            timeout=300
        )
    
    def load_documents(self, file_path: List[str] = None, directory_path: str = None, progress_callback: Optional[Callable] = None) -> bool:
        """
        åŠ è½½æ–‡æ¡£ï¼ˆä½¿ç”¨å¢å¼ºåŠ è½½å™¨ï¼‰- å…¼å®¹LangChainRAGSystemæ¥å£
        
        Args:
            file_path: å•ä¸ªæ–‡ä»¶è·¯å¾„
            directory_path: ç›®å½•è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            # æ„å»ºæ–‡ä»¶åˆ—è¡¨
            file_paths = []
            if file_path:
                file_paths = file_path
            elif directory_path:
                import glob
                file_paths = glob.glob(os.path.join(directory_path, "*.pdf"))
            else:
                logger.error("è¯·æä¾›æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„")
                return False
            
            if not file_paths:
                logger.error("æ²¡æœ‰æ‰¾åˆ°è¦åŠ è½½çš„æ–‡ä»¶")
                return False
            
            logger.info(f"å¼€å§‹åŠ è½½ {len(file_paths)} ä¸ªæ–‡æ¡£...")
            
            all_documents = []
            
            # ä½¿ç”¨å¢å¼ºæ–‡æ¡£åŠ è½½å™¨é€ä¸ªå¤„ç†æ–‡ä»¶
            for i, file_path in enumerate(file_paths):
                logger.info(f"å¤„ç†æ–‡æ¡£ {i+1}/{len(file_paths)}: {os.path.basename(file_path)}")
                
                if progress_callback:
                    progress_callback(f"æ­£åœ¨å¤„ç†æ–‡æ¡£: {os.path.basename(file_path)}")
                
                # ä½¿ç”¨ç« èŠ‚åˆ‡åˆ†åŠ è½½
                docs = self.enhanced_document_loader.load_pdf(
                    file_path, 
                    use_chapter_splitting=self.enable_chapter_splitting
                )
                
                if docs:
                    all_documents.extend(docs)
                    logger.info(f"æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£å—")
                else:
                    logger.warning(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {file_path}")
            
            if not all_documents:
                logger.error("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ–‡æ¡£")
                return False
            
            logger.info(f"æ€»è®¡åŠ è½½ {len(all_documents)} ä¸ªå¢å¼ºæ–‡æ¡£å—")
            
            # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            if progress_callback:  # é¢„ç•™äº†ä¸€ä¸ªæ¥å£ï¼Œæš‚æ—¶è¿˜ç”¨ä¸åˆ°
                ("æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•...")
            
            self.vectorstore.add_documents(all_documents, progress_callback=progress_callback)

            # åˆå§‹åŒ–é«˜çº§æ··åˆæ£€ç´¢å™¨
            logger.info("åˆå§‹åŒ–é«˜çº§æ··åˆæ£€ç´¢å™¨...")
            self.advanced_retriever = AdvancedHybridRetriever(
                vectorstore=self.vectorstore,
                documents=all_documents,
                vector_weight=self.vector_weight,
                bm25_weight=self.bm25_weight,
                enable_force_recall=True,
                enable_exact_phrase=True,
                k=self.retrieval_k
            )
            
            # æ„å»ºRAGé“¾
            self._build_optimized_rag_chain()
            
            logger.info("æ–‡æ¡£åŠ è½½å’Œç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {e}")
            return False
    
    def load_documents_by_paths(self, file_paths: List[str], progress_callback: Optional[Callable] = None) -> bool:
        """
        é€šè¿‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨åŠ è½½æ–‡æ¡£ï¼ˆä¿ç•™åŸå§‹æ¥å£ï¼‰
        
        Args:
            file_paths: æ–‡æ¡£æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        if len(file_paths) == 1:
            return self.load_documents(file_path=file_paths[0], progress_callback=progress_callback)
        else:
            # å¯¹äºå¤šä¸ªæ–‡ä»¶ï¼Œæ„é€ ä¸€ä¸ªä¸´æ—¶ç›®å½•å‚æ•°
            import os
            if all(os.path.dirname(fp) == os.path.dirname(file_paths[0]) for fp in file_paths):
                # å¦‚æœæ‰€æœ‰æ–‡ä»¶åœ¨åŒä¸€ç›®å½•
                return self.load_documents(directory_path=os.path.dirname(file_paths[0]), progress_callback=progress_callback)
            else:
                # å¦‚æœæ–‡ä»¶åœ¨ä¸åŒç›®å½•ï¼Œé€ä¸ªåŠ è½½
                all_success = True
                for file_path in file_paths:
                    success = self.load_documents(file_path=file_path, progress_callback=progress_callback)
                    if not success:
                        all_success = False
                return all_success
    
    def _build_optimized_rag_chain(self):
        """æ„å»ºä¼˜åŒ–çš„RAGé“¾"""
        try:
            logger.info("æ„å»ºä¼˜åŒ–RAGé“¾...")
            
            def retrieve_and_rerank(inputs):
                """æ£€ç´¢å’Œé‡æ’åº"""
                question = inputs["question"]
                
                # 1. é«˜çº§æ··åˆæ£€ç´¢
                logger.debug("æ‰§è¡Œé«˜çº§æ··åˆæ£€ç´¢...")
                documents = self.advanced_retriever.get_relevant_documents(question)
                
                # 2. å¢å¼ºé‡æ’åº
                if self.enable_reranking and self.enhanced_reranker and documents:
                    logger.debug("æ‰§è¡Œå¢å¼ºé‡æ’åº...")
                    documents = self.enhanced_reranker.rerank(question, documents, top_k=self.retrieval_k)
                
                return {"question": question, "context": self._format_documents(documents), "documents": documents}
            
            def enhance_prompt(inputs):
                """å¢å¼ºæç¤ºè¯"""
                question = inputs["question"]
                context = inputs["context"]
                
                # è·å–å¢å¼ºæç¤ºè¯
                template, variables = self.enhanced_prompt_manager.get_enhanced_prompt(question, context)
                
                # æ ¼å¼åŒ–æç¤ºè¯
                formatted_prompt = template.format(**variables)
                
                return {"question": question, "context": context, "prompt": formatted_prompt, "documents": inputs["documents"]}
            
            # æ„å»ºRAGé“¾ - ä¿®å¤ç®¡é“æ“ä½œç¬¦å…¼å®¹æ€§é—®é¢˜
            from langchain_core.runnables import RunnableLambda
            
            # åˆ›å»ºå¯è¿è¡Œçš„ç»„ä»¶ï¼ˆæŠŠæ™®é€šå‡½æ•°å˜æˆ LangChain å¯ç»„åˆçš„â€œå¯è¿è¡Œç»„ä»¶â€ï¼‰
            question_runnable = RunnableLambda(lambda x: {"question": x} if isinstance(x, str) else x)
            retrieve_runnable = RunnableLambda(retrieve_and_rerank)
            enhance_runnable = RunnableLambda(enhance_prompt)
            prompt_runnable = RunnableLambda(lambda x: x["prompt"])
             
            try:
                # ä½¿ç”¨ç®¡é“æ“ä½œç¬¦ï¼ˆæ–°ç‰ˆæœ¬LangChainï¼‰
                # å‰ä¸€ä¸ªçš„è¾“å‡ºæ˜¯åä¸€ä¸ªçš„è¾“å…¥
                self.rag_chain = (
                    question_runnable
                    | retrieve_runnable
                    | enhance_runnable
                    | prompt_runnable
                    | self.llm
                    | StrOutputParser()
                )
            except TypeError:
                # é™çº§æ–¹æ¡ˆï¼šæ‰‹åŠ¨é“¾æ¥
                logger.info("ä½¿ç”¨å…¼å®¹æ€§é“¾æ¥æ–¹å¼...")
                def combined_chain(question: str):
                    # 1. å‡†å¤‡è¾“å…¥
                    inputs = {"question": question}
                     
                    # 2. æ£€ç´¢å’Œé‡æ’åº
                    step1_result = retrieve_and_rerank(inputs)
                     
                    # 3. å¢å¼ºæç¤ºè¯
                    step2_result = enhance_prompt(step1_result)
                     
                    # 4. æå–æç¤ºè¯
                    prompt = step2_result["prompt"]
                      
                    # 5. è°ƒç”¨LLM
                    llm_result = self.llm.invoke(prompt)
                     
                    # 6. è§£æè¾“å‡º
                    parser = StrOutputParser()
                    final_result = parser.parse(llm_result)
                      
                    return final_result
                 
                self.rag_chain = RunnableLambda(combined_chain)
            
            logger.info("ä¼˜åŒ–RAGé“¾æ„å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"RAGé“¾æ„å»ºå¤±è´¥: {e}")
            raise e
    
    def answer_question(self, question: str) -> OptimizedRAGResponse:
        """
        å›ç­”é—®é¢˜ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            ä¼˜åŒ–RAGå“åº”å¯¹è±¡
        """
        start_time = time.time()
        
        try:
            logger.info(f"å¼€å§‹å›ç­”é—®é¢˜: {question[:50]}...")
            
            if not self.rag_chain:
                raise ValueError("RAGé“¾æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåŠ è½½æ–‡æ¡£")
            
            # 1. æ‰§è¡Œæ£€ç´¢å’Œé‡æ’åº
            logger.info("æ‰§è¡Œé«˜çº§æ··åˆæ£€ç´¢...")
            source_documents = self.advanced_retriever.get_relevant_documents(question)
            
            # è·å–æ£€ç´¢ç»Ÿè®¡
            retrieval_stats = self._get_retrieval_stats(question)
            
            # 2. æ‰§è¡Œå¢å¼ºé‡æ’åº
            rerank_stats = {}
            if self.enable_reranking and self.enhanced_reranker and source_documents:
                logger.info("æ‰§è¡Œå¢å¼ºé‡æ’åº...")
                reranked_docs = self.enhanced_reranker.rerank(question, source_documents, top_k=self.retrieval_k)
                
                # è·å–é‡æ’åºç»Ÿè®¡
                rerank_stats = self._get_rerank_stats(question, source_documents, reranked_docs)
                source_documents = reranked_docs
            
            # 3. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
            context = self._format_documents(source_documents)
            
            # 4. è·å–å¢å¼ºæç¤ºè¯
            logger.info("ç”Ÿæˆå¢å¼ºæç¤ºè¯...")
            template, variables = self.enhanced_prompt_manager.get_enhanced_prompt(question, context)
            
            # 5. ç”Ÿæˆç­”æ¡ˆ
            logger.info("ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")
            prompt_formatted = template.format(**variables)
            answer = self.llm.invoke(prompt_formatted).content
            
            # 6. åˆ†ææç¤ºè¯æ•ˆæœ
            prompt_analysis = self.enhanced_prompt_manager.analyze_prompt_effectiveness(question, context, answer)
            
            # è®¡ç®—å“åº”æ—¶é—´
            response_time = time.time() - start_time
            
            # è®°å½•è¯¦ç»†ç»Ÿè®¡
            self._log_response_stats(question, answer, source_documents, retrieval_stats, rerank_stats, response_time)
            
            # åˆ›å»ºå“åº”å¯¹è±¡
            response = OptimizedRAGResponse(
                question=question,
                answer=answer,
                source_documents=source_documents,
                retrieval_stats=retrieval_stats,
                rerank_stats=rerank_stats,
                prompt_analysis=prompt_analysis,
                response_time=response_time
            )
            
            logger.info(f"é—®é¢˜å›ç­”å®Œæˆï¼Œæ€»ç”¨æ—¶: {response_time:.2f}ç§’")
            return response
            
        except Exception as e:
            logger.error(f"é—®é¢˜å›ç­”å¤±è´¥: {e}")
            
            # åˆ›å»ºé”™è¯¯å“åº”
            error_response = OptimizedRAGResponse(
                question=question,
                answer=f"æŠ±æ­‰ï¼Œå›ç­”è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                source_documents=[],
                retrieval_stats={},
                rerank_stats={},
                prompt_analysis={},
                response_time=time.time() - start_time
            )
            return error_response
    
    def _format_documents(self, documents: List[Document]) -> str:
        """æ ¼å¼åŒ–æ–‡æ¡£ä¸ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        if not documents:
            return ""
        
        formatted_parts = []
        for i, doc in enumerate(documents):
            # è·å–æ–‡æ¡£æ¥æºä¿¡æ¯
            source = os.path.basename(doc.metadata.get('source', 'unknown'))
            chapter_title = doc.metadata.get('chapter_title', '')
            
            # æ„å»ºæ–‡æ¡£æ ‡è¯†
            doc_identifier = f"[æ–‡æ¡£{i+1}] {source}"
            if chapter_title:
                doc_identifier += f" - {chapter_title}"
            
            # æ·»åŠ æ–‡æ¡£å†…å®¹
            content = doc.page_content
            formatted_part = f"{doc_identifier}\n{content}\n"
            formatted_parts.append(formatted_part)
        
        return "\n".join(formatted_parts)
    
    def _get_retrieval_stats(self, question: str) -> Dict:
        """è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if self.advanced_retriever:
                return self.advanced_retriever.get_detailed_results(question)
            return {}
        except Exception as e:
            logger.warning(f"è·å–æ£€ç´¢ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    def _get_rerank_stats(self, question: str, original_docs: List[Document], reranked_docs: List[Document]) -> Dict:
        """è·å–é‡æ’åºç»Ÿè®¡ä¿¡æ¯"""
        try:
            if self.enhanced_reranker:
                return self.enhanced_reranker.get_rerank_analysis(question, original_docs)
            return {}
        except Exception as e:
            logger.warning(f"è·å–é‡æ’åºç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    def _log_response_stats(self, question: str, answer: str, documents: List[Document], 
                           retrieval_stats: Dict, rerank_stats: Dict, response_time: float):
        """è®°å½•å“åº”ç»Ÿè®¡ä¿¡æ¯"""
        try:
            logger.info("ğŸ“Š ä¼˜åŒ–RAGç³»ç»Ÿå“åº”ç»Ÿè®¡:")
            logger.info(f"  é—®é¢˜: {question[:50]}...")
            logger.info(f"  å“åº”æ—¶é—´: {response_time:.2f}ç§’")
            logger.info(f"  è¿”å›æ–‡æ¡£æ•°: {len(documents)}")
            
            # æ–‡æ¡£æ¥æºç»Ÿè®¡
            if documents:
                from collections import Counter
                sources = [os.path.basename(doc.metadata.get('source', 'unknown')) for doc in documents]
                source_counter = Counter(sources)
                logger.info(f"  æ–‡æ¡£æ¥æºåˆ†å¸ƒ: {dict(source_counter)}")
                
                # æ˜¾ç¤ºä¼˜åŒ–ç‰¹å¾
                enhanced_count = sum(1 for doc in documents if doc.metadata.get('enhanced', False))
                rrf_count = sum(1 for doc in documents if 'rrf_score' in doc.metadata)
                rerank_count = sum(1 for doc in documents if 'final_rerank_score' in doc.metadata)
                
                logger.info(f"  ä¼˜åŒ–ç‰¹å¾: å¢å¼ºæ–‡æ¡£={enhanced_count}, RRFèåˆ={rrf_count}, é‡æ’åº={rerank_count}")
            
            # ç­”æ¡ˆé•¿åº¦å’Œè´¨é‡æŒ‡æ ‡
            logger.info(f"  ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")
            
            # æ˜¯å¦ä½¿ç”¨äº†"æœªæ‰¾åˆ°"å›å¤
            uses_not_found = "æ ¹æ®ç°æœ‰æ–‡æ¡£ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°" in answer
            logger.info(f"  ä½¿ç”¨æœªæ‰¾åˆ°å›å¤: {uses_not_found}")
            
        except Exception as e:
            logger.warning(f"è®°å½•å“åº”ç»Ÿè®¡å¤±è´¥: {e}")
    
    def get_system_performance_report(self) -> Dict:
        """è·å–ç³»ç»Ÿæ€§èƒ½æŠ¥å‘Š"""
        try:
            report = {
                'system_config': {
                    'llm_model': self.llm_model,
                    'embedding_model': self.embedding_model,
                    'vector_weight': self.vector_weight,
                    'bm25_weight': self.bm25_weight,
                    'enable_reranking': self.enable_reranking,
                    'enable_chapter_splitting': self.enable_chapter_splitting,
                    'retrieval_k': self.retrieval_k
                },
                'components_status': {
                    'vectorstore_loaded': self.vectorstore is not None,
                    'advanced_retriever_ready': self.advanced_retriever is not None,
                    'enhanced_reranker_ready': self.enhanced_reranker is not None,
                    'enhanced_prompt_manager_ready': self.enhanced_prompt_manager is not None,
                    'rag_chain_built': self.rag_chain is not None
                },
                'optimization_features': {
                    'hybrid_retrieval_with_rrf': True,
                    'crossencoder_reranking': self.enable_reranking,
                    'entity_hit_reward': True,
                    'chapter_splitting_with_title_enhancement': self.enable_chapter_splitting,
                    'fewshot_prompt_optimization': True
                }
            }
            
            # æ·»åŠ æ–‡æ¡£ç»Ÿè®¡
            if self.vectorstore:
                try:
                    doc_count = self.vectorstore.get_document_count()
                    report['document_stats'] = {
                        'total_documents': doc_count,
                        'vectorstore_ready': doc_count > 0
                    }
                except:
                    report['document_stats'] = {'error': 'æ— æ³•è·å–æ–‡æ¡£ç»Ÿè®¡'}
            
            return report
            
        except Exception as e:
            logger.error(f"è·å–ç³»ç»Ÿæ€§èƒ½æŠ¥å‘Šå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def run_optimization_test(self, test_questions: List[str]) -> Dict:
        """
        è¿è¡Œä¼˜åŒ–æµ‹è¯•
        
        Args:
            test_questions: æµ‹è¯•é—®é¢˜åˆ—è¡¨
            
        Returns:
            æµ‹è¯•ç»“æœæŠ¥å‘Š
        """
        try:
            logger.info(f"å¼€å§‹è¿è¡Œä¼˜åŒ–æµ‹è¯•ï¼Œå…± {len(test_questions)} ä¸ªé—®é¢˜...")
            
            test_results = []
            total_time = 0
            
            for i, question in enumerate(test_questions):
                logger.info(f"æµ‹è¯•é—®é¢˜ {i+1}/{len(test_questions)}: {question[:30]}...")
                
                start_time = time.time()
                response = self.answer_question(question)
                end_time = time.time()
                
                question_time = end_time - start_time
                total_time += question_time
                
                # åˆ†æç»“æœ
                result = {
                    'question': question,
                    'answer': response.answer,
                    'response_time': question_time,
                    'documents_count': len(response.source_documents),
                    'uses_enhanced_features': self._analyze_enhanced_features(response),
                    'prompt_analysis': response.prompt_analysis
                }
                
                test_results.append(result)
                logger.info(f"  å®Œæˆï¼Œç”¨æ—¶: {question_time:.2f}ç§’")
            
            # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            report = {
                'test_summary': {
                    'total_questions': len(test_questions),
                    'total_time': total_time,
                    'average_time': total_time / len(test_questions),
                    'fastest_response': min(r['response_time'] for r in test_results),
                    'slowest_response': max(r['response_time'] for r in test_results)
                },
                'optimization_effectiveness': {
                    'questions_using_traffic_signal_template': sum(1 for r in test_results if r['prompt_analysis'].get('question_type') == 'traffic_signal_task'),
                    'questions_using_not_found_response': sum(1 for r in test_results if 'æ ¹æ®ç°æœ‰æ–‡æ¡£ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°' in r['answer']),
                    'average_documents_per_response': sum(r['documents_count'] for r in test_results) / len(test_results),
                    'questions_with_enhanced_documents': sum(1 for r in test_results if r['uses_enhanced_features']['enhanced_documents']),
                    'questions_with_rrf_fusion': sum(1 for r in test_results if r['uses_enhanced_features']['rrf_fusion']),
                    'questions_with_reranking': sum(1 for r in test_results if r['uses_enhanced_features']['reranking'])
                },
                'detailed_results': test_results
            }
            
            logger.info("ä¼˜åŒ–æµ‹è¯•å®Œæˆ")
            logger.info(f"  æ€»ç”¨æ—¶: {total_time:.2f}ç§’")
            logger.info(f"  å¹³å‡å“åº”æ—¶é—´: {total_time/len(test_questions):.2f}ç§’")
            
            return report
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _analyze_enhanced_features(self, response: OptimizedRAGResponse) -> Dict:
        """åˆ†æå“åº”ä¸­ä½¿ç”¨çš„å¢å¼ºç‰¹å¾"""
        features = {
            'enhanced_documents': False,
            'rrf_fusion': False,
            'reranking': False,
            'chapter_splitting': False,
            'entity_bonuses': False
        }
        
        for doc in response.source_documents:
            metadata = doc.metadata
            
            if metadata.get('enhanced', False):
                features['enhanced_documents'] = True
            
            if 'rrf_score' in metadata:
                features['rrf_fusion'] = True
            
            if 'final_rerank_score' in metadata:
                features['reranking'] = True
            
            if metadata.get('chapter_title'):
                features['chapter_splitting'] = True
            
            if 'entity_bonus' in metadata:
                features['entity_bonuses'] = True
        
        return features 

    # ==================== å…¼å®¹æ€§æ–¹æ³• ====================
    # ä¸ºäº†ä¸LangChainRAGSystemä¿æŒå…¼å®¹æ€§ï¼Œæ·»åŠ ä»¥ä¸‹æ–¹æ³•

    def get_version_statistics(self) -> Dict:
        """è·å–ç‰ˆæœ¬ç®¡ç†ç»Ÿè®¡ä¿¡æ¯"""
        try:
            if not self.vectorstore:
                return {"error": "å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–"}
            
            return self.vectorstore.get_version_statistics()
            
        except Exception as e:
            logger.error(f"è·å–ç‰ˆæœ¬ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def cleanup_knowledge_base(self) -> Dict:
        """æ¸…ç†çŸ¥è¯†åº“ï¼ˆç§»é™¤å­¤ç«‹çš„ç‰ˆæœ¬ä¿¡æ¯ï¼‰"""
        try:
            result = {"orphaned_removed": 0}
            
            if self.vectorstore:
                if hasattr(self.vectorstore, 'cleanup_orphaned_documents'):
                    orphaned_count = self.vectorstore.cleanup_orphaned_documents()
                    result["orphaned_removed"] = orphaned_count
                    
                    if orphaned_count > 0:
                        logger.info(f"æ¸…ç†äº† {orphaned_count} ä¸ªå­¤ç«‹çš„æ–‡æ¡£ç‰ˆæœ¬")
            
            return result
            
        except Exception as e:
            logger.error(f"æ¸…ç†çŸ¥è¯†åº“å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def rebuild_knowledge_base(self, progress_callback: Optional[Callable] = None) -> bool:
        """å®Œå…¨é‡å»ºçŸ¥è¯†åº“ï¼ˆæ¸…ç©ºç°æœ‰æ•°æ®å¹¶é‡æ–°æ„å»ºï¼‰"""
        try:
            logger.info("å¼€å§‹é‡å»ºçŸ¥è¯†åº“...")
            
            # æ¸…ç©ºå‘é‡å­˜å‚¨
            if self.vectorstore:
                self.vectorstore.clear_vectorstore()
                
                # æ¸…ç†ç‰ˆæœ¬ä¿¡æ¯
                if hasattr(self.vectorstore, 'version_manager') and self.vectorstore.version_manager:
                    self.vectorstore.version_manager.reset_all()
            
            # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
            pdf_files = []
            for directory in ["./data", "./docs", "./"]:
                if os.path.exists(directory):
                    import glob
                    pattern = os.path.join(directory, "*.pdf")
                    found_files = glob.glob(pattern)
                    if found_files:
                        pdf_files.extend(found_files)
                        logger.info(f"åœ¨ {directory} ä¸­æ‰¾åˆ° {len(found_files)} ä¸ªPDFæ–‡ä»¶")
                        break
            
            if not pdf_files:
                logger.warning("æœªæ‰¾åˆ°PDFæ–‡ä»¶è¿›è¡Œé‡å»º")
                return False
            
            logger.info(f"å¼€å§‹é‡å»ºï¼Œå¤„ç† {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
            
            # ä½¿ç”¨ç›®å½•è·¯å¾„é‡æ–°åŠ è½½ï¼ˆå› ä¸ºæ–‡ä»¶åœ¨åŒä¸€ç›®å½•ï¼‰
            directory_path = os.path.dirname(pdf_files[0]) if pdf_files else None
            if directory_path:
                success = self.load_documents(directory_path=directory_path, progress_callback=progress_callback)
            else:
                # å¦‚æœæ–‡ä»¶åœ¨ä¸åŒç›®å½•ï¼Œé€ä¸ªåŠ è½½
                success = True
                for pdf_file in pdf_files:
                    file_success = self.load_documents(file_path=pdf_file, progress_callback=progress_callback)
                    if not file_success:
                        success = False
            
            if success:
                logger.info("çŸ¥è¯†åº“é‡å»ºå®Œæˆ")
                return True
            else:
                logger.error("çŸ¥è¯†åº“é‡å»ºå¤±è´¥")
                return False
            
        except Exception as e:
            logger.error(f"é‡å»ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
    
    def _rebuild_retriever(self):
        """é‡å»ºæ£€ç´¢å™¨ï¼ˆé‡å»ºçŸ¥è¯†åº“åè°ƒç”¨ï¼‰"""
        try:
            if self.vectorstore and hasattr(self.vectorstore, 'get_document_count'):
                doc_count = self.vectorstore.get_document_count()
                if doc_count > 0:
                    # é‡æ–°åˆå§‹åŒ–é«˜çº§æ£€ç´¢å™¨ï¼Œä½†éœ€è¦æ–‡æ¡£åˆ—è¡¨
                    logger.info("é‡å»ºçŸ¥è¯†åº“å®Œæˆï¼Œéœ€è¦é‡æ–°åŠ è½½æ–‡æ¡£ä»¥å®Œæˆæ£€ç´¢å™¨åˆå§‹åŒ–")
                else:
                    logger.warning("é‡å»ºåçš„å‘é‡å­˜å‚¨ä¸ºç©º")
        except Exception as e:
            logger.warning(f"é‡å»ºæ£€ç´¢å™¨å¤±è´¥: {e}")
    
    def load_documents_incremental(self, file_path: str = None, 
                                 directory_path: str = None,
                                 progress_callback: Optional[Callable] = None, 
                                 force_rebuild: bool = False) -> bool:
        """å¢é‡åŠ è½½æ–‡æ¡£ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        try:
            logger.info("æ‰§è¡Œå¢é‡æ–‡æ¡£åŠ è½½...")
            
            # æ„å»ºè¦åŠ è½½çš„æ–‡ä»¶åˆ—è¡¨
            files_to_load = []
            if file_path:
                files_to_load.append(file_path)
            
            if directory_path and os.path.exists(directory_path):
                import glob
                pdf_files = glob.glob(os.path.join(directory_path, "*.pdf"))
                files_to_load.extend(pdf_files)
            
            if not files_to_load:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°è¦åŠ è½½çš„æ–‡ä»¶")
                return False
            
            # ä¼˜åŒ–RAGç³»ç»Ÿæš‚æ—¶å°†å¢é‡åŠ è½½è§†ä¸ºå®Œæ•´åŠ è½½
            # å› ä¸ºé«˜çº§æ··åˆæ£€ç´¢å™¨éœ€è¦å®Œæ•´çš„æ–‡æ¡£é›†åˆæ¥æ„å»ºç´¢å¼•
            if len(files_to_load) == 1:
                return self.load_documents(file_path=files_to_load[0], progress_callback=progress_callback)
            else:
                return self.load_documents(directory_path=directory_path, progress_callback=progress_callback)
            
        except Exception as e:
            logger.error(f"å¢é‡åŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def extract_information(self) -> Dict:
        """æå–ç«èµ›ä¿¡æ¯ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        try:
            logger.info("æå–ç«èµ›ä¿¡æ¯...")
            
            # åŸºç¡€ç«èµ›ä¿¡æ¯
            competitions = {}
            
            if self.vectorstore:
                try:
                    # å°è¯•æœç´¢å·²çŸ¥çš„ç«èµ›æ–‡æ¡£
                    known_competitions = [
                        "æœªæ¥æ ¡å›­æ™ºèƒ½åº”ç”¨ä¸“é¡¹èµ›",
                        "äººå·¥æ™ºèƒ½ç»¼åˆåˆ›æ–°ä¸“é¡¹èµ›", 
                        "æ™ºæ…§åŸå¸‚ä¸»é¢˜è®¾è®¡ä¸“é¡¹èµ›",
                        "ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åº”ç”¨ä¸“é¡¹èµ›"
                    ]
                    
                    for comp_name in known_competitions:
                        docs = self.vectorstore.similarity_search(comp_name, k=3)
                        if docs:
                            competitions[comp_name] = {
                                "documents_found": len(docs),
                                "sources": list(set(os.path.basename(doc.metadata.get('source', 'unknown')) for doc in docs))
                            }
                    
                    logger.info(f"æ‰¾åˆ° {len(competitions)} ä¸ªç«èµ›çš„ç›¸å…³ä¿¡æ¯")
                    
                except Exception as e:
                    logger.warning(f"æå–ç«èµ›ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            
            return {
                "competitions": competitions,
                "total_competitions": len(competitions),
                "extraction_method": "optimized_rag_system"
            }
            
        except Exception as e:
            logger.error(f"æå–ç«èµ›ä¿¡æ¯å¤±è´¥: {e}")
            return {"error": str(e)}
    
    # ==================== æ–‡æ¡£ç›‘æ§å…¼å®¹æ€§æ–¹æ³• ====================
    # æ³¨æ„ï¼šä¼˜åŒ–RAGç³»ç»Ÿæš‚ä¸æ”¯æŒæ–‡æ¡£ç›‘æ§ï¼Œè¿”å›é€‚å½“çš„é»˜è®¤å€¼
    
    @property
    def document_watcher(self):
        """æ–‡æ¡£ç›‘æ§å™¨å±æ€§ï¼ˆå…¼å®¹æ€§ï¼‰"""
        return None
    
    def init_document_watcher(self, config=None) -> bool:
        """åˆå§‹åŒ–æ–‡æ¡£ç›‘æ§å™¨ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        logger.info("ä¼˜åŒ–RAGç³»ç»Ÿæš‚ä¸æ”¯æŒæ–‡æ¡£ç›‘æ§åŠŸèƒ½")
        return False
    
    def start_document_watching(self) -> bool:
        """å¼€å§‹æ–‡æ¡£ç›‘æ§ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        logger.info("ä¼˜åŒ–RAGç³»ç»Ÿæš‚ä¸æ”¯æŒæ–‡æ¡£ç›‘æ§åŠŸèƒ½")
        return False
    
    def stop_document_watching(self) -> bool:
        """åœæ­¢æ–‡æ¡£ç›‘æ§ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        logger.info("ä¼˜åŒ–RAGç³»ç»Ÿæš‚ä¸æ”¯æŒæ–‡æ¡£ç›‘æ§åŠŸèƒ½")
        return False
    
    def check_documents_now(self) -> Dict:
        """ç«‹å³æ£€æŸ¥æ–‡æ¡£å˜æ›´ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        logger.info("ä¼˜åŒ–RAGç³»ç»Ÿæš‚ä¸æ”¯æŒæ–‡æ¡£ç›‘æ§åŠŸèƒ½")
        return {"supported": False, "message": "ä¼˜åŒ–RAGç³»ç»Ÿæš‚ä¸æ”¯æŒæ–‡æ¡£ç›‘æ§"}
    
    def get_watch_status(self) -> Dict:
        """è·å–ç›‘æ§çŠ¶æ€ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return {
            "is_watching": False,
            "supported": False,
            "message": "ä¼˜åŒ–RAGç³»ç»Ÿæš‚ä¸æ”¯æŒæ–‡æ¡£ç›‘æ§"
        }
    
    def get_monitored_files(self) -> List[str]:
        """è·å–ç›‘æ§æ–‡ä»¶åˆ—è¡¨ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return [] 
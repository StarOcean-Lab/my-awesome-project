# LangChain+Ollama å®ç°æ³°è¿ªæ¯ç«èµ›æ™ºèƒ½å®¢æœæœºå™¨äºº

## ğŸ“‘ ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
2. [æŠ€æœ¯æ¶æ„](#2-æŠ€æœ¯æ¶æ„)
3. [å¿«é€Ÿå¼€å§‹](#3-å¿«é€Ÿå¼€å§‹)
4. [ç¯å¢ƒå®‰è£…](#4-ç¯å¢ƒå®‰è£…)
5. [åŠŸèƒ½ä½¿ç”¨](#5-åŠŸèƒ½ä½¿ç”¨)
6. [Webç•Œé¢æŒ‡å—](#6-webç•Œé¢æŒ‡å—)
7. [Ollamaé›†æˆ](#7-ollamaé›†æˆ)
8. [é«˜çº§é…ç½®](#8-é«˜çº§é…ç½®)
9. [æ•…éšœæ’é™¤](#9-æ•…éšœæ’é™¤)
10. [å¼€å‘æ‰©å±•](#10-å¼€å‘æ‰©å±•)
11. [æŠ€æœ¯å‡çº§æ€»ç»“](#11-æŠ€æœ¯å‡çº§æ€»ç»“)
12. [é™„å½•](#12-é™„å½•)

---

## 1. é¡¹ç›®æ¦‚è¿°

### ğŸ¯ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯åŸºäº **LangChain + Ollama** çš„æ™ºèƒ½å®¢æœæœºå™¨äººï¼Œä¸“ä¸º**2025å¹´ï¼ˆç¬¬13å±Šï¼‰"æ³°è¿ªæ¯"æ•°æ®æŒ–æ˜æŒ‘æˆ˜èµ›Cé¢˜**å¼€å‘ã€‚é‡‡ç”¨æœ€æ–°çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æŠ€æœ¯ï¼Œç»“åˆæœ¬åœ°åŒ–çš„å¤§è¯­è¨€æ¨¡å‹å’Œå‘é‡æ£€ç´¢ï¼Œæä¾›å‡†ç¡®ã€å®æ—¶çš„ç«èµ›å’¨è¯¢æœåŠ¡ã€‚

### ğŸ­ æ ¸å¿ƒç‰¹æ€§

- **ğŸ“š æ™ºèƒ½é—®ç­”**: åŸºäºä¼˜åŒ–RAGæ¶æ„çš„ç²¾ç¡®é—®é¢˜å›ç­”
- **ğŸ” æ··åˆæ£€ç´¢**: BM25å…³é”®è¯ + å‘é‡è¯­ä¹‰åŒé‡æ£€ç´¢
- **ğŸ¤– æœ¬åœ°LLM**: é›†æˆOllamaæœ¬åœ°å¤§è¯­è¨€æ¨¡å‹
- **ğŸ“Š æ‰¹é‡å¤„ç†**: æ”¯æŒExcelæ‰¹é‡é—®ç­”å’Œä¿¡æ¯æå–
- **ğŸŒ Webç•Œé¢**: å‹å¥½çš„Streamlitäº¤äº’ç•Œé¢
- **ğŸ”„ å®æ—¶æ›´æ–°**: æ”¯æŒçŸ¥è¯†åº“åŠ¨æ€æ›´æ–°å’Œç‰ˆæœ¬ç®¡ç†

### ğŸš€ **NEW! 5é¡¹æ ¸å¿ƒä¼˜åŒ–ç‰¹æ€§**

- **ğŸ¯ æ··åˆæ£€ç´¢å¢å¼º**: BM25ç²¾ç¡®çŸ­è¯­å¼ºåˆ¶å¬å› + å‘é‡Top-Kçš„RRFèåˆ
- **ğŸ§  Cross-Encoderé‡æ’**: ms-marco-MiniLM-L6-v2æ™ºèƒ½é‡æ’åº
- **ğŸ† å®ä½“å‘½ä¸­å¥–åŠ±**: å…³é”®è¯å‘½ä¸­åœ¨é‡æ’é˜¶æ®µé¢å¤–åŠ åˆ†
- **ğŸ“„ æ–‡æ¡£åˆ‡åˆ†ä¼˜åŒ–**: æŒ‰ç« èŠ‚åˆ‡åˆ†PDFå¹¶å°†æ ‡é¢˜æ‹¼æ¥åˆ°chunkå¼€å¤´
- **ğŸ’¡ Few-shotæç¤º**: æ™ºèƒ½ä¸Šä¸‹æ–‡ç›¸å…³æ€§æ£€æŸ¥å’Œä¸“ä¸šå›å¤æ¨¡æ¿

### ğŸ¯ ä¸»è¦åŠŸèƒ½

1. **ç«èµ›æ•°æ®æ•´ç†** - è‡ªåŠ¨æå–PDFæ–‡æ¡£ä¸­çš„ç«èµ›åŸºæœ¬ä¿¡æ¯
2. **æ™ºèƒ½é—®ç­”ç³»ç»Ÿ** - å›ç­”åŸºç¡€ä¿¡æ¯ã€ç»Ÿè®¡åˆ†æã€å¼€æ”¾æ€§é—®é¢˜
3. **çŸ¥è¯†åº“ç®¡ç†** - å®æ—¶æ›´æ–°ã€ç‰ˆæœ¬æ§åˆ¶ã€è‡ªåŠ¨å¤‡ä»½
4. **æ‰¹é‡å¤„ç†** - Excelæ–‡ä»¶æ‰¹é‡é—®ç­”å’Œç»“æœå¯¼å‡º

---

## 2. æŠ€æœ¯æ¶æ„

### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

**ä¼ ç»ŸRAGæ¶æ„**:
```
ç”¨æˆ·é—®é¢˜ â†’ æ–‡æ¡£åŠ è½½å™¨ â†’ å‘é‡åŒ–å­˜å‚¨ â†’ æ··åˆæ£€ç´¢å™¨ â†’ Ollama LLM â†’ ç”Ÿæˆå›ç­”
```

**ğŸš€ ä¼˜åŒ–RAGæ¶æ„** (5å±‚ä¼˜åŒ–):
```
ç”¨æˆ·é—®é¢˜ â†’ å¢å¼ºæ–‡æ¡£åŠ è½½å™¨(æ ‡é¢˜æ‹¼æ¥) â†’ å‘é‡åŒ–å­˜å‚¨ â†’ 
    â†“
æ··åˆæ£€ç´¢å™¨(BM25+å‘é‡+RRFèåˆ) â†’ Cross-Encoderé‡æ’ â†’ å®ä½“å‘½ä¸­å¥–åŠ± â†’ 
    â†“  
Few-shotæç¤ºç®¡ç†å™¨ â†’ Ollama LLM â†’ æ™ºèƒ½å›ç­”ç”Ÿæˆ
```

### ğŸ“¦ æ ¸å¿ƒæŠ€æœ¯æ ˆ

- **LangChain**: æ„å»ºLLMåº”ç”¨çš„æ¡†æ¶
- **Ollama**: æœ¬åœ°LLMéƒ¨ç½²å¹³å°  
- **FAISS**: é«˜æ€§èƒ½å‘é‡æ£€ç´¢
- **Streamlit**: Webç•Œé¢æ¡†æ¶
- **BGE-large-zh-v1.5**: ä¸­æ–‡å‘é‡åŒ–æ¨¡å‹
- **nomic-embed-text**: Ollama embeddingæ¨¡å‹

### ğŸ”§ æ ¸å¿ƒç»„ä»¶

#### ğŸ“„ LangChainæ–‡æ¡£åŠ è½½å™¨ (`src/langchain_document_loader.py`)
- æ”¯æŒPyPDFå’Œpdfplumberä¸¤ç§åŠ è½½æ–¹å¼
- è‡ªåŠ¨å¤„ç†ä¸­æ–‡PDFæ–‡æ¡£
- æå–ç»“æ„åŒ–ç«èµ›ä¿¡æ¯
- å®Œæ•´çš„å…ƒæ•°æ®ç®¡ç†

#### ğŸ§® LangChainå‘é‡å­˜å‚¨ (`src/langchain_vectorstore.py`)  
- åŸºäºLangChain FAISSå®ç°
- ä½¿ç”¨nomic-embed-text:latestè¿›è¡Œå‘é‡åŒ–
- æ”¯æŒæŒä¹…åŒ–å­˜å‚¨
- è‡ªåŠ¨æ–‡æ¡£åˆ†å‰²å’Œå‘é‡åŒ–

#### ğŸ” LangChainæ··åˆæ£€ç´¢å™¨ (`src/langchain_retriever.py`)
- ç»§æ‰¿LangChain BaseRetriever
- BM25å…³é”®è¯æ£€ç´¢ + å‘é‡è¯­ä¹‰æ£€ç´¢
- è‡ªé€‚åº”æƒé‡è°ƒæ•´
- è¯¦ç»†çš„æ£€ç´¢ç»“æœåˆ†æ

#### ğŸ§  LangChain RAGç³»ç»Ÿ (`src/langchain_rag.py`)
- å®Œæ•´çš„LangChain RAG Chainå®ç°
- æ”¯æŒOllama LLMé›†æˆ
- æ™ºèƒ½Promptå·¥ç¨‹
- æ‰¹é‡å¤„ç†åŠŸèƒ½

---

## 3. å¿«é€Ÿå¼€å§‹

### âš¡ ä¸€é”®å¯åŠ¨

```bash
# 1. å¯åŠ¨OllamaæœåŠ¡
ollama serve

# 2. æ‹‰å–å¿…éœ€æ¨¡å‹  
ollama pull deepseek-r1:7b
ollama pull nomic-embed-text

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt
# ä½¿ç”¨æœ¬åœ°Pythonç¯å¢ƒå®‰è£…åŒ…
.\.conda\python.exe -m pip install -r requirements.txt

# 4. å¯åŠ¨ç³»ç»Ÿ
python run_langchain.py --mode web
```

**è®¿é—®åœ°å€**: http://localhost:8501

### ğŸš€ å¯åŠ¨ä¼˜åŒ–æ¨¡å¼

```bash
# Webç•Œé¢ï¼ˆæ¨èï¼‰- æ”¯æŒç•Œé¢å†…åˆ‡æ¢ä¼˜åŒ–æ¨¡å¼
python run_langchain.py --mode web

# å‘½ä»¤è¡Œä¼˜åŒ–æ¨¡å¼
python run_langchain.py --mode cli --use-optimized

# ä¼˜åŒ–ç³»ç»Ÿä¸“é¡¹æµ‹è¯•
python run_langchain.py --mode optimized

# é›†æˆæµ‹è¯•ï¼ˆéªŒè¯ä¼˜åŒ–åŠŸèƒ½ï¼‰
python test_integration.py
```

### ğŸ® æµ‹è¯•éªŒè¯

```bash
# è¿è¡Œå®Œæ•´æµ‹è¯•
python test_langchain.py

# æ£€æŸ¥ç¯å¢ƒ
python run_langchain.py --check

# æµ‹è¯•Ollamaè¿æ¥
python -c "from langchain_chatbot import LangChainChatbot; LangChainChatbot()"
```

---

## 4. ç¯å¢ƒå®‰è£…

### 4.1 ç³»ç»Ÿè¦æ±‚

- **Python**: 3.8+
- **å†…å­˜**: 16GB+ (æ¨è32GB)
- **å­˜å‚¨**: 20GBå¯ç”¨ç©ºé—´
- **GPU**: å¯é€‰ï¼Œæ”¯æŒCUDAåŠ é€Ÿ

### 4.2 å®‰è£…Ollama

#### Windowså®‰è£…
```bash
# ä½¿ç”¨wingetå®‰è£…
winget install ollama

# æˆ–è®¿é—®å®˜ç½‘ä¸‹è½½ï¼šhttps://ollama.ai/download
```

#### Linux/macOSå®‰è£…
```bash
# ä¸€é”®å®‰è£…
curl -fsSL https://ollama.ai/install.sh | sh
```

### 4.3 Pythonä¾èµ–å®‰è£…

```bash
# æ–¹å¼1ï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬
python run_langchain.py --install

# æ–¹å¼2ï¼šç›´æ¥å®‰è£…
pip install -r requirements.txt

# æ–¹å¼3ï¼šä½¿ç”¨å›½å†…é•œåƒ
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 4.4 æ¨¡å‹ä¸‹è½½

```bash
# å¯åŠ¨OllamaæœåŠ¡
ollama serve

# ä¸‹è½½LLMæ¨¡å‹ï¼ˆæ¨ç†ç”¨ï¼‰
ollama pull deepseek-r1:7b

# ä¸‹è½½Embeddingæ¨¡å‹ï¼ˆå‘é‡åŒ–ç”¨ï¼‰  
ollama pull nomic-embed-text:latest

# éªŒè¯æ¨¡å‹å®‰è£…
ollama list
```

### 4.5 ç¯å¢ƒæ£€æŸ¥

```bash
# å®Œæ•´ç¯å¢ƒæ£€æŸ¥
python run_langchain.py --check

# æ£€æŸ¥Pythonç‰ˆæœ¬
python --version

# æ£€æŸ¥OllamaæœåŠ¡
curl http://localhost:11434/api/tags
```

---

## 5. åŠŸèƒ½ä½¿ç”¨

### 5.1 ç«èµ›æ•°æ®æ•´ç†

#### å‡†å¤‡PDFæ–‡æ¡£
```bash
# å°†PDFæ–‡æ¡£æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•
01_ç«èµ›1.pdf è‡³ 18_ç«èµ›18.pdf
```

#### è¿è¡Œæ•°æ®æå–
```python
from langchain_chatbot import LangChainChatbot

chatbot = LangChainChatbot()
pdf_files = glob.glob("*.pdf")
chatbot.load_knowledge_base(pdf_files)

# æå–ç«èµ›ä¿¡æ¯
chatbot.extract_competition_info("result_1.xlsx")
```

### 5.2 æ™ºèƒ½é—®ç­”åŠŸèƒ½

#### å•é—®é¢˜å›ç­”
```python
# åˆå§‹åŒ–ç³»ç»Ÿ
chatbot = LangChainChatbot()
chatbot.load_knowledge_base(pdf_files)

# é—®ç­”æµ‹è¯•
question = "ç¬¬ä¸ƒå±Šå…¨å›½é’å°‘å¹´äººå·¥æ™ºèƒ½åˆ›æ–°æŒ‘æˆ˜èµ›çš„æŠ¥åæ—¶é—´æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ"
result = chatbot.answer_question(question)

print(f"å›ç­”: {result['answer']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']:.2f}")
print(f"æ¥æº: {result['sources']}")
```

#### é—®é¢˜ç±»å‹
- **åŸºç¡€ä¿¡æ¯æŸ¥è¯¢**: æ—¶é—´ã€åœ°ç‚¹ã€è¦æ±‚ç­‰å…·ä½“ä¿¡æ¯
- **ç»Ÿè®¡åˆ†ææŸ¥è¯¢**: æ•°é‡ã€åˆ†ç±»ç­‰ç»Ÿè®¡ä¿¡æ¯  
- **å¼€æ”¾æ€§é—®é¢˜**: å‡†å¤‡å»ºè®®ã€ç­–ç•¥æŒ‡å¯¼ç­‰

### 5.3 æ‰¹é‡é—®ç­”å¤„ç†

#### å‡†å¤‡é—®é¢˜æ–‡ä»¶
```python
# åˆ›å»ºæµ‹è¯•é—®é¢˜
python create_test_questions.py

# è‡ªå®šä¹‰é—®é¢˜Excelæ–‡ä»¶ï¼ˆéœ€åŒ…å«"é—®é¢˜"åˆ—ï¼‰
questions = [
    "æŠ¥åæ—¶é—´æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ",
    "å‚èµ›è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ",
    "å¦‚ä½•å‡†å¤‡ç«èµ›ï¼Ÿ"
]
df = pd.DataFrame({'é—®é¢˜': questions})
df.to_excel('my_questions.xlsx', index=False)
```

#### æ‰¹é‡å¤„ç†
```python
# è¯»å–é—®é¢˜æ–‡ä»¶
df = pd.read_excel('my_questions.xlsx')
questions = df['é—®é¢˜'].tolist()

# æ‰¹é‡å›ç­”
results = chatbot.batch_answer_questions(questions, "batch_results.xlsx")
print(f"å¤„ç†å®Œæˆï¼Œå…± {len(results)} ä¸ªé—®é¢˜")
```

### 5.4 æ–‡æ¡£ç›‘æ§ä¸è‡ªåŠ¨æ›´æ–°

ç³»ç»Ÿå…·å¤‡è‡ªåŠ¨æ–‡æ¡£ç›‘æ§åŠŸèƒ½ï¼Œä¼šåœ¨åå°é€æ˜åœ°è¿è¡Œï¼š

#### è‡ªåŠ¨åŠŸèƒ½
- **è‡ªåŠ¨æ£€æµ‹**: ç³»ç»Ÿå®šæœŸæ‰«æPDFæ–‡ä»¶å˜æ›´
- **è‡ªåŠ¨æ›´æ–°**: å‘ç°æ–‡ä»¶å˜æ›´æ—¶è‡ªåŠ¨é‡æ–°åŠ è½½çŸ¥è¯†åº“
- **é€æ˜è¿è¡Œ**: ç”¨æˆ·æ— éœ€æ‰‹åŠ¨å¹²é¢„ï¼Œæ‰€æœ‰æ›´æ–°è¿‡ç¨‹åœ¨åå°è¿›è¡Œ

#### ç›‘æ§ç‰¹æ€§
- å®æ—¶æ£€æµ‹æ–‡ä»¶ä¿®æ”¹ã€æ–°å¢ã€åˆ é™¤
- è‡ªåŠ¨å¤‡ä»½å†å²ç‰ˆæœ¬
- æ”¯æŒå¤šç›®å½•ç›‘æ§
- æ™ºèƒ½å»é‡ä¸å¢é‡æ›´æ–°

---

## 6. Webç•Œé¢æŒ‡å—

### 6.1 ç•Œé¢å¸ƒå±€

#### ä¾§è¾¹æ åŠŸèƒ½
- **ğŸ“Š ç³»ç»Ÿç®¡ç†**: çŠ¶æ€ç›‘æ§ã€å‚æ•°é…ç½®
- **ğŸ“š çŸ¥è¯†åº“ç®¡ç†**: PDFåŠ è½½ã€çŠ¶æ€æ˜¾ç¤º  
- **ğŸ”§ æ‰¹é‡å¤„ç†**: Excelæ–‡ä»¶æ‰¹é‡é—®ç­”
- **âš™ï¸ ç³»ç»Ÿé…ç½®**: LLMå’Œembeddingæ¨¡å‹é…ç½®

#### ä¸»ç•Œé¢åŠŸèƒ½
- **ğŸ’¬ æ™ºèƒ½é—®ç­”**: å®æ—¶èŠå¤©äº¤äº’ç•Œé¢
- **ğŸ“ å¯¹è¯å†å²**: å®Œæ•´çš„å¯¹è¯è®°å½•
- **ğŸ“Š ç»“æœå±•ç¤º**: ç­”æ¡ˆã€ç½®ä¿¡åº¦ã€æ¥æºä¿¡æ¯

### 6.2 æ“ä½œæµç¨‹

#### é¦–æ¬¡ä½¿ç”¨æµç¨‹
```
1. å¯åŠ¨ç³»ç»Ÿ â†’ 2. åŠ è½½PDFæ–‡ä»¶ â†’ 3. å¼€å§‹é—®ç­”ï¼ˆç³»ç»Ÿè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶å˜æ›´ï¼‰
```

#### æ—¥å¸¸ä½¿ç”¨æµç¨‹  
```
1. è¾“å…¥é—®é¢˜ â†’ 2. æŸ¥çœ‹å›ç­” â†’ 3. æ£€æŸ¥æ¥æº â†’ 4. æŸ¥çœ‹å†å²
```

#### æ‰¹é‡å¤„ç†æµç¨‹
```
1. ä¸Šä¼ é—®é¢˜Excel â†’ 2. è®¾ç½®è¾“å‡ºæ–‡ä»¶å â†’ 3. å¼€å§‹å¤„ç† â†’ 4. ä¸‹è½½ç»“æœ
```

### 6.3 ç•Œé¢é…ç½®

#### LLMé…ç½®
- **æ¨¡å‹åç§°**: deepseek-r1:7b
- **Ollamaåœ°å€**: http://localhost:11434  
- **æ¸©åº¦å‚æ•°**: 0.1ï¼ˆæ§åˆ¶éšæœºæ€§ï¼‰
- **æœ€å¤§ä»¤ç‰Œ**: 2000

#### Embeddingé…ç½®
- **æ¨¡å‹åç§°**: nomic-embed-text:latest
- **å‘é‡ç»´åº¦**: 768ç»´
- **åˆ†å—å¤§å°**: 1000å­—ç¬¦
- **é‡å å¤§å°**: 200å­—ç¬¦

---

## 7. Ollamaé›†æˆ

### 7.1 æœåŠ¡ç®¡ç†

#### å¯åŠ¨å’Œåœæ­¢
```bash
# å¯åŠ¨OllamaæœåŠ¡
ollama serve

# åå°è¿è¡Œ
nohup ollama serve > ollama.log 2>&1 &

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
ps aux | grep ollama
curl http://localhost:11434/api/tags
```

#### ç«¯å£é…ç½®
```bash
# ä¿®æ”¹é»˜è®¤ç«¯å£
export OLLAMA_HOST=0.0.0.0:11435
ollama serve

# æˆ–åœ¨config.pyä¸­ä¿®æ”¹
OLLAMA_BASE_URL = "http://localhost:11435"
```

### 7.2 æ¨¡å‹ç®¡ç†

#### æ¨èæ¨¡å‹é…ç½®
```bash
# ä¸­æ–‡å‹å¥½æ¨¡å‹
ollama pull deepseek-r1:7b      # æ™ºèƒ½æ¨ç†ï¼ˆæ¨èï¼‰
ollama pull qwen2:7b            # é˜¿é‡Œåƒé—®2
ollama pull chatglm3:6b         # æ™ºè°±ChatGLM3

# Embeddingæ¨¡å‹
ollama pull nomic-embed-text    # å‘é‡åŒ–ï¼ˆæ¨èï¼‰
ollama pull mxbai-embed-large   # æ›¿ä»£é€‰æ‹©
```

#### æ¨¡å‹åˆ‡æ¢
```python
# åœ¨ä»£ç ä¸­åˆ‡æ¢æ¨¡å‹
chatbot = LangChainChatbot(
    llm_model="qwen2:7b",  # æ›´æ¢LLMæ¨¡å‹
    embedding_model="mxbai-embed-large"  # æ›´æ¢embeddingæ¨¡å‹
)
```

### 7.3 æ€§èƒ½ä¼˜åŒ–

#### GPUåŠ é€Ÿ
```bash
# æ£€æŸ¥GPUæ”¯æŒ
ollama run deepseek-r1:7b --gpu

# è®¾ç½®GPUå±‚æ•°
export OLLAMA_NUM_GPU=1
```

#### å†…å­˜ä¼˜åŒ–
```python
# config.pyä¸­è°ƒæ•´å‚æ•°
LLM_MAX_TOKENS = 1000      # å‡å°‘æœ€å¤§ä»¤ç‰Œæ•°
LLM_TEMPERATURE = 0.1      # é™ä½éšæœºæ€§
LANGCHAIN_CHUNK_SIZE = 500 # å‡å°‘åˆ†å—å¤§å°
```

#### å¹¶å‘æ§åˆ¶
```bash
# é™åˆ¶å¹¶å‘æ¨¡å‹æ•°
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_NUM_PARALLEL=2
```

---

## 8. é«˜çº§é…ç½®

### 8.1 ç³»ç»Ÿé…ç½®

#### config.pyä¸»è¦é…ç½®é¡¹
```python
class Config:
    # LLMé…ç½®
    LLM_MODEL = "deepseek-r1:7b"
    OLLAMA_BASE_URL = "http://localhost:11434"
    LLM_TEMPERATURE = 0.1
    LLM_MAX_TOKENS = 2000
    LLM_TIMEOUT = 60
    
    # LangChainé…ç½®  
    LANGCHAIN_EMBEDDING_MODEL = "nomic-embed-text:latest"
    LANGCHAIN_VECTORSTORE_TYPE = "faiss"
    LANGCHAIN_RETRIEVER_K = 5
    LANGCHAIN_CHUNK_SIZE = 1000
    LANGCHAIN_CHUNK_OVERLAP = 200
    
    # æ£€ç´¢é…ç½®
    BM25_K1 = 1.5
    BM25_B = 0.75
    HYBRID_ALPHA = 0.5  # æ··åˆæ£€ç´¢æƒé‡
    
    # åˆ†æ•°é˜ˆå€¼
    VECTOR_SCORE_THRESHOLD = 0.5
    BM25_SCORE_THRESHOLD = 0.1
```

### 8.2 æ£€ç´¢ä¼˜åŒ–

#### é—®é¢˜ç±»å‹ç­–ç•¥
```python
QUESTION_TYPES = {
    "basic": {
        "alpha": 0.3,        # æ›´é‡è§†å…³é”®è¯åŒ¹é…
        "vector_k": 15,
        "bm25_k": 25,
        "description": "åŸºç¡€ä¿¡æ¯æŸ¥è¯¢"
    },
    "statistical": {
        "alpha": 0.4,        # å¹³è¡¡ç­–ç•¥
        "vector_k": 30,
        "bm25_k": 30,
        "description": "ç»Ÿè®¡åˆ†ææŸ¥è¯¢"
    },
    "open": {
        "alpha": 0.7,        # æ›´é‡è§†è¯­ä¹‰ç†è§£
        "vector_k": 25,
        "bm25_k": 15,
        "description": "å¼€æ”¾æ€§é—®é¢˜"
    }
}
```

#### åŠ¨æ€æƒé‡è°ƒæ•´
```python
# æ ¹æ®é—®é¢˜ç±»å‹è°ƒæ•´æ£€ç´¢æƒé‡
def adjust_retrieval_weights(question_type):
    config = QUESTION_TYPES.get(question_type, QUESTION_TYPES["basic"])
    return {
        "alpha": config["alpha"],
        "vector_k": config["vector_k"], 
        "bm25_k": config["bm25_k"]
    }
```

### 8.3 æç¤ºè¯å·¥ç¨‹

#### è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
```python
def _build_prompt(self, question: str, context: str) -> str:
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç«èµ›æ™ºèƒ½å®¢æœæœºå™¨äººï¼Œä¸“é—¨å›ç­”å…³äºå„ç±»å­¦ç§‘ç«èµ›çš„é—®é¢˜ã€‚

è¯·æ ¹æ®æä¾›çš„ç«èµ›æ–‡æ¡£å†…å®¹ï¼Œå‡†ç¡®ã€è¯¦ç»†åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å›ç­”è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹è¿›è¡Œå›ç­”ï¼Œç¡®ä¿ä¿¡æ¯å‡†ç¡®
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®ç°æœ‰æ–‡æ¡£æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
3. å›ç­”è¦å…·ä½“ã€æœ‰ç”¨ï¼ŒåŒ…å«å…³é”®çš„æ—¶é—´ã€åœ°ç‚¹ã€è¦æ±‚ç­‰ä¿¡æ¯
4. å¯¹äºå¼€æ”¾æ€§é—®é¢˜ï¼Œå¯ä»¥ç»™å‡ºåˆç†çš„å»ºè®®å’ŒæŒ‡å¯¼
5. ä¿æŒä¸“ä¸šã€å‹å¥½çš„è¯­è°ƒ

æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ï¼š"""

    return system_prompt.format(context=context, question=question)
```

### 8.4 å¹¶å‘å¤„ç†

#### å¤šçº¿ç¨‹é—®ç­”
```python
import concurrent.futures

def parallel_qa(questions):
    chatbot = LangChainChatbot()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        results = executor.map(chatbot.answer_question, questions)
    
    return list(results)
```

#### å¼‚æ­¥å¤„ç†
```python
import asyncio

async def async_answer_question(chatbot, question):
    return await asyncio.to_thread(chatbot.answer_question, question)

async def batch_async_qa(questions):
    chatbot = LangChainChatbot()
    tasks = [async_answer_question(chatbot, q) for q in questions]
    results = await asyncio.gather(*tasks)
    return results
```

---

## 9. æ•…éšœæ’é™¤

### 9.1 å¸¸è§é—®é¢˜

#### Q1: Ollamaè¿æ¥å¤±è´¥
**é”™è¯¯**: `Ollamaè¿æ¥å¼‚å¸¸: Connection refused`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥æœåŠ¡çŠ¶æ€
ps aux | grep ollama

# 2. å¯åŠ¨æœåŠ¡
ollama serve

# 3. æ£€æŸ¥ç«¯å£å ç”¨
netstat -an | grep 11434

# 4. é‡å¯æœåŠ¡
pkill ollama
ollama serve
```

#### Q2: æ¨¡å‹ä¸‹è½½å¤±è´¥
**é”™è¯¯**: `Error pulling model`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥ç½‘ç»œè¿æ¥
ping ollama.ai

# 2. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
ollama pull deepseek-r1:7b --verbose

# 3. ä½¿ç”¨ä»£ç†
export HTTP_PROXY=http://proxy:port
export HTTPS_PROXY=http://proxy:port

# 4. æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h
```

#### Q3: å†…å­˜ä¸è¶³
**é”™è¯¯**: `Out of memory` æˆ–æ¨¡å‹åŠ è½½å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. è°ƒæ•´é…ç½®å‚æ•°
CHUNK_SIZE = 300           # å‡å°‘åˆ†å—å¤§å°
MAX_CONTEXTS = 3           # å‡å°‘æ£€ç´¢æ•°é‡
LLM_MAX_TOKENS = 1000     # å‡å°‘ä»¤ç‰Œæ•°

# 2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
LLM_MODEL = "chatglm3:6b"  # 6Bæ›¿ä»£7Bæ¨¡å‹

# 3. é™åˆ¶å¹¶å‘
export OLLAMA_MAX_LOADED_MODELS=1
```

#### Q4: PDFè§£æå¤±è´¥
**é”™è¯¯**: `PDF processing failed`

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. æ£€æŸ¥PDFæ–‡ä»¶å®Œæ•´æ€§
import PyPDF2
with open('file.pdf', 'rb') as f:
    reader = PyPDF2.PdfReader(f)
    print(f"é¡µæ•°: {len(reader.pages)}")

# 2. å°è¯•ä¸åŒè§£æå™¨
# åœ¨langchain_document_loader.pyä¸­åˆ‡æ¢è§£ææ–¹æ³•

# 3. é‡æ–°ç”ŸæˆPDFæ–‡ä»¶
# ç¡®ä¿PDFæ–‡ä»¶æ²¡æœ‰å¯†ç ä¿æŠ¤å’ŒæŸå
```

#### Q5: å‘é‡æ£€ç´¢æ•ˆæœå·®
**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. è°ƒæ•´åˆ†å—å‚æ•°
LANGCHAIN_CHUNK_SIZE = 800      # è°ƒæ•´åˆ†å—å¤§å°
LANGCHAIN_CHUNK_OVERLAP = 150   # è°ƒæ•´é‡å å¤§å°

# 2. å¢åŠ æ£€ç´¢æ•°é‡
LANGCHAIN_RETRIEVER_K = 10      # å¢åŠ æ£€ç´¢ç»“æœæ•°

# 3. è°ƒæ•´æ··åˆæƒé‡
HYBRID_ALPHA = 0.7              # æ›´é‡è§†è¯­ä¹‰æ£€ç´¢
```

### 9.2 æ—¥å¿—è°ƒè¯•

#### å¯ç”¨è¯¦ç»†æ—¥å¿—
```python
import logging
logging.basicConfig(level=logging.DEBUG)

from loguru import logger
logger.add("debug.log", level="DEBUG")
```

#### æŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—
```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f langchain_chatbot.log

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
grep "ERROR" langchain_chatbot.log

# æŸ¥çœ‹ç‰¹å®šç»„ä»¶æ—¥å¿—
grep "LLM" langchain_chatbot.log
grep "Vector" langchain_chatbot.log
```

### 9.3 æ€§èƒ½ç›‘æ§

#### ç³»ç»ŸçŠ¶æ€æ£€æŸ¥
```python
# è·å–è¯¦ç»†ç³»ç»ŸçŠ¶æ€
chatbot = LangChainChatbot()
status = chatbot.get_system_status()
print(json.dumps(status, indent=2, ensure_ascii=False))
```

#### èµ„æºä½¿ç”¨ç›‘æ§
```bash
# å†…å­˜ä½¿ç”¨
htop
free -h

# GPUä½¿ç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
nvidia-smi

# ç£ç›˜ç©ºé—´
df -h

# ç½‘ç»œè¿æ¥
netstat -an | grep 11434
```

---

## 10. å¼€å‘æ‰©å±•

### 10.1 æ·»åŠ æ–°åŠŸèƒ½

#### è‡ªå®šä¹‰é—®é¢˜ç±»å‹
```python
# åœ¨src/rag_system.pyä¸­æ·»åŠ æ–°çš„é—®é¢˜åˆ†ç±»é€»è¾‘
def classify_question_type(question: str) -> str:
    if "æ—¶é—´" in question or "ä»€ä¹ˆæ—¶å€™" in question:
        return "basic"
    elif "å¤šå°‘" in question or "æ•°é‡" in question:
        return "statistical"
    elif "å¦‚ä½•" in question or "æ€ä¹ˆ" in question:
        return "open"
    else:
        return "basic"
```

#### é›†æˆå¤–éƒ¨API
```python
# æ·»åŠ OpenAI APIæ”¯æŒ
class OpenAILLMClient:
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)
    
    def generate_answer(self, question: str, context: str) -> str:
        response = self.client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç«èµ›å®¢æœåŠ©æ‰‹..."},
                {"role": "user", "content": f"ä¸Šä¸‹æ–‡ï¼š{context}\né—®é¢˜ï¼š{question}"}
            ]
        )
        return response.choices[0].message.content
```

### 10.2 è‡ªå®šä¹‰æ£€ç´¢å™¨

#### å®ç°ä¸“é—¨çš„æ£€ç´¢å™¨
```python
from langchain_core.retrievers import BaseRetriever

class CustomRetriever(BaseRetriever):
    def __init__(self, vectorstore, bm25_retriever):
        self.vectorstore = vectorstore
        self.bm25_retriever = bm25_retriever
    
    def _get_relevant_documents(self, query: str) -> List[Document]:
        # è‡ªå®šä¹‰æ£€ç´¢é€»è¾‘
        vector_docs = self.vectorstore.similarity_search(query, k=5)
        bm25_docs = self.bm25_retriever.get_relevant_documents(query)
        
        # è‡ªå®šä¹‰èåˆç­–ç•¥
        return self._merge_results(vector_docs, bm25_docs)
```

### 10.3 æ‰©å±•æ•°æ®æº

#### æ”¯æŒæ›´å¤šæ–‡æ¡£æ ¼å¼
```python
def load_word_document(file_path: str) -> List[Document]:
    from docx import Document as DocxDocument
    
    doc = DocxDocument(file_path)
    text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
    
    return [Document(
        page_content=text,
        metadata={"source": file_path, "type": "docx"}
    )]
```

#### é›†æˆåœ¨çº¿æ•°æ®æº
```python
def load_web_content(urls: List[str]) -> List[Document]:
    from langchain_community.document_loaders import WebBaseLoader
    
    loader = WebBaseLoader(urls)
    documents = loader.load()
    
    return documents
```

---

## 11. æŠ€æœ¯å‡çº§æ€»ç»“

### 11.1 Embeddingæ¨¡å‹å‡çº§

#### å‡çº§æ¦‚è¿°
æˆåŠŸå°†embeddingæ¨¡å‹ä»æœ¬åœ°BGEæ¨¡å‹å‡çº§ä¸ºOllamaçš„APIè°ƒç”¨æ–¹å¼ï¼Œä½¿ç”¨nomic-embed-text:latestæ¨¡å‹ã€‚

#### æŠ€æœ¯ç‰¹æ€§
- **æ¨¡å‹**: nomic-embed-text:latest
- **ç»´åº¦**: 768ç»´å‘é‡
- **è¯­è¨€æ”¯æŒ**: ä¸­è‹±æ–‡
- **è°ƒç”¨æ–¹å¼**: Ollama APIæ¥å£
- **æ€§èƒ½**: ä¼˜åŒ–çš„å‘é‡åŒ–å¤„ç†

#### ä¸»è¦æ”¹è¿›
1. **ç»Ÿä¸€APIè°ƒç”¨**: æ‰€æœ‰æ¨¡å‹éƒ½é€šè¿‡Ollamaç®¡ç†
2. **æ›´å¥½æ€§èƒ½**: nomic-embed-texté’ˆå¯¹embeddingä¼˜åŒ–
3. **ç®€åŒ–éƒ¨ç½²**: æ— éœ€ç®¡ç†æœ¬åœ°æ¨¡å‹æ–‡ä»¶
4. **çµæ´»é…ç½®**: å¯åœ¨ç•Œé¢ä¸­è½»æ¾åˆ‡æ¢æ¨¡å‹

### 11.2 ç³»ç»Ÿæ¶æ„ä¼˜åŒ–

#### LangChainé›†æˆ
- å®Œå…¨åŸºäºLangChainæ¡†æ¶é‡æ„
- æ ‡å‡†åŒ–çš„ç»„ä»¶æ¥å£
- æ›´å¥½çš„å¯æ‰©å±•æ€§

#### Ollamaæ·±åº¦é›†æˆ
- LLMå’ŒEmbeddingç»Ÿä¸€ç®¡ç†
- ç®€åŒ–çš„æ¨¡å‹åˆ‡æ¢
- ç»Ÿä¸€çš„é…ç½®æ–¹å¼

---

## 12. é™„å½•

### 12.1 è¾“å‡ºæ–‡ä»¶è¯´æ˜

#### result_1.xlsx - ç«èµ›åŸºæœ¬ä¿¡æ¯è¡¨
```
åˆ—å: èµ›é¡¹åç§° | èµ›é“ | å‘å¸ƒæ—¶é—´ | æŠ¥åæ—¶é—´ | ç»„ç»‡å•ä½ | å®˜ç½‘
å†…å®¹: ä»PDFæ–‡æ¡£æå–çš„18ä¸ªç«èµ›åŸºæœ¬ä¿¡æ¯
```

#### result_2.xlsx - æ‰¹é‡é—®ç­”ç»“æœ
```
åˆ—å: é—®é¢˜ç¼–å· | é—®é¢˜ | å›ç­” | ç½®ä¿¡åº¦ | æ¥æºæ•°é‡
å†…å®¹: å¯¹é™„ä»¶2ä¸­é—®é¢˜çš„å›ç­”ç»“æœ
```

#### result_3.xlsx - æ›´æ–°åé—®ç­”ç»“æœ  
```
åˆ—å: é—®é¢˜ç¼–å· | é—®é¢˜ | å›ç­” | ç½®ä¿¡åº¦ | æ¥æºæ•°é‡
å†…å®¹: çŸ¥è¯†åº“æ›´æ–°åçš„é—®ç­”ç»“æœ
```

### 12.2 é¡¹ç›®æ–‡ä»¶ç»“æ„

```
taidi-langchain/
â”œâ”€â”€ langchain_chatbot.py          # LangChainä¸»ç¨‹åº
â”œâ”€â”€ run_langchain.py              # LangChainå¯åŠ¨è„šæœ¬  
â”œâ”€â”€ test_langchain.py             # LangChainæµ‹è¯•è„šæœ¬
â”œâ”€â”€ config.py                     # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt              # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ create_test_questions.py      # æµ‹è¯•é—®é¢˜ç”Ÿæˆ
â”œâ”€â”€ src/                          # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ langchain_rag.py         # LangChain RAGç³»ç»Ÿ
â”‚   â”œâ”€â”€ langchain_vectorstore.py # å‘é‡å­˜å‚¨
â”‚   â”œâ”€â”€ langchain_retriever.py   # æ··åˆæ£€ç´¢å™¨
â”‚   â”œâ”€â”€ langchain_document_loader.py # æ–‡æ¡£åŠ è½½å™¨
â”‚   â”œâ”€â”€ pdf_processor.py         # PDFå¤„ç†å™¨
â”‚   â”œâ”€â”€ vector_store.py          # ä¼ ç»Ÿå‘é‡å­˜å‚¨
â”‚   â”œâ”€â”€ bm25_retriever.py        # BM25æ£€ç´¢å™¨
â”‚   â”œâ”€â”€ hybrid_retriever.py      # æ··åˆæ£€ç´¢å™¨
â”‚   â”œâ”€â”€ rag_system.py            # RAGç³»ç»Ÿ
â”‚   â”œâ”€â”€ knowledge_manager.py     # çŸ¥è¯†åº“ç®¡ç†
â”‚   â”œâ”€â”€ llm_client.py            # LLMå®¢æˆ·ç«¯
â”‚   â””â”€â”€ __init__.py              # æ¨¡å—åˆå§‹åŒ–
â”œâ”€â”€ vectorstore/                  # å‘é‡å­˜å‚¨ç›®å½•
â”œâ”€â”€ logs/                        # æ—¥å¿—ç›®å½•
â”œâ”€â”€ outputs/                     # è¾“å‡ºç›®å½•
â”œâ”€â”€ bge-large-zh-v1.5/           # BGEæ¨¡å‹ç›®å½•
â”œâ”€â”€ test_questions.xlsx          # æµ‹è¯•é—®é¢˜æ–‡ä»¶
â”œâ”€â”€ é™„ä»¶2.pdf                    # ç«èµ›æ–‡æ¡£
â”œâ”€â”€ Cé¢˜-ç«èµ›æ™ºèƒ½å®¢æœæœºå™¨äºº.pdf    # é¢˜ç›®æ–‡æ¡£
â””â”€â”€ *.pdf                        # ç«èµ›è§„ç¨‹æ–‡æ¡£
```

### 12.3 ä¾èµ–åˆ—è¡¨

#### æ ¸å¿ƒæ¡†æ¶
```
langchain>=0.2.0
langchain-community>=0.2.0  
langchain-core>=0.2.0
langchain-ollama>=0.1.1
langchain-text-splitters>=0.2.0
```

#### æ–‡æ¡£å¤„ç†
```
PyPDF2==3.0.1
pdfplumber==0.10.0
python-docx==1.1.0
```

#### å‘é‡å­˜å‚¨å’Œæ£€ç´¢
```
faiss-cpu==1.7.4
chromadb==0.4.22
sentence-transformers==2.2.2
rank-bm25==0.2.2
```

#### Webç•Œé¢
```
streamlit>=1.28.0
gradio==4.10.0
```

### 12.4 æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | å¤§å° | ä¸­æ–‡èƒ½åŠ› | æ¨ç†èƒ½åŠ› | å†…å­˜éœ€æ±‚ | æ¨èåœºæ™¯ |
|------|------|----------|----------|----------|----------|
| deepseek-r1:7b | 7B | â­â­â­â­â­ | â­â­â­â­â­ | 8GB | æ™ºèƒ½å®¢æœï¼ˆæ¨èï¼‰ |
| qwen2:7b | 7B | â­â­â­â­â­ | â­â­â­â­ | 8GB | ä¸­æ–‡å¯¹è¯ |
| chatglm3:6b | 6B | â­â­â­â­â­ | â­â­â­ | 6GB | è½»é‡åŒ–éƒ¨ç½² |
| nomic-embed-text | - | â­â­â­â­ | - | 2GB | å‘é‡åŒ–ï¼ˆæ¨èï¼‰ |

### 12.5 å¸¸ç”¨å‘½ä»¤æ±‡æ€»

#### ç³»ç»Ÿç®¡ç†
```bash
# å®Œæ•´å¯åŠ¨æµç¨‹
ollama serve
ollama pull deepseek-r1:7b
ollama pull nomic-embed-text
python run_langchain.py --install
python run_langchain.py --mode web

# ç¯å¢ƒæ£€æŸ¥
python run_langchain.py --check
python test_langchain.py

# æ—¥å¿—æŸ¥çœ‹
tail -f langchain_chatbot.log
grep "ERROR" langchain_chatbot.log
```

#### æ¨¡å‹ç®¡ç†
```bash
# æ¨¡å‹æ“ä½œ
ollama list                    # æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹
ollama pull <model>           # ä¸‹è½½æ¨¡å‹
ollama rm <model>             # åˆ é™¤æ¨¡å‹
ollama show <model>           # æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯

# æœåŠ¡ç®¡ç†
ollama serve                  # å¯åŠ¨æœåŠ¡
ps aux | grep ollama         # æ£€æŸ¥æœåŠ¡çŠ¶æ€
pkill ollama                 # åœæ­¢æœåŠ¡
```

### 12.6 ç›¸å…³é“¾æ¥

- **Ollamaå®˜ç½‘**: https://ollama.ai/
- **LangChainæ–‡æ¡£**: https://python.langchain.com/
- **DeepSeekæ¨¡å‹**: https://github.com/deepseek-ai/deepseek-coder
- **Streamlitæ–‡æ¡£**: https://docs.streamlit.io/
- **FAISSæ–‡æ¡£**: https://faiss.ai/

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚é‡é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š

1. **ç³»ç»Ÿæ—¥å¿—**: `langchain_chatbot.log`
2. **é”™è¯¯ä¿¡æ¯**: ç»ˆç«¯è¾“å‡º
3. **é…ç½®æ–‡ä»¶**: `config.py`
4. **ç¯å¢ƒæ£€æŸ¥**: `python run_langchain.py --check`

**è”ç³»æ–¹å¼**: 
- æäº¤GitHub Issue
- æŸ¥çœ‹é¡¹ç›®æ–‡æ¡£
- å‚ä¸æŠ€æœ¯äº¤æµ

---

**ğŸ‰ æ„Ÿè°¢ä½¿ç”¨LangChain+Ollamaæ™ºèƒ½å®¢æœæœºå™¨äººï¼**

*æœ€åæ›´æ–°æ—¶é—´: 2025å¹´7æœˆ* 
#!/usr/bin/env python3
"""
ä¼˜åŒ–RAGç³»ç»Ÿçš„æµ‹è¯•è„šæœ¬ï¼ŒéªŒè¯æ‰€æœ‰5ä¸ªä¼˜åŒ–åŠŸèƒ½çš„ååŒå·¥ä½œæ•ˆæœ
"""

import os
import sys
import time
import json
from typing import List, Dict
from loguru import logger

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from optimized_rag_system import OptimizedRAGSystem

class OptimizedRAGTester:
    """ä¼˜åŒ–RAGç³»ç»Ÿæµ‹è¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.test_questions = [
            # äº¤é€šä¿¡å·ç¯ä¸“é¡¹é—®é¢˜
            "æ™ºèƒ½äº¤é€šä¿¡å·ç¯çš„åŸºæœ¬è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ",
            "äº¤é€šä¿¡å·ç¯ç³»ç»Ÿéœ€è¦å®ç°å“ªäº›æŠ€æœ¯è¦æ±‚ï¼Ÿ", 
            "æ™ºèƒ½äº¤é€šä¿¡å·ç¯çš„ä¼˜åŒ–ç®—æ³•å¦‚ä½•è®¾è®¡ï¼Ÿ",
            
            # ç«èµ›ä»»åŠ¡é—®é¢˜
            "æœªæ¥æ ¡å›­æ™ºèƒ½åº”ç”¨ä¸“é¡¹èµ›çš„è¯„åˆ†æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ",
            "ç«èµ›ä»»åŠ¡æè¿°ä¸­åŒ…å«å“ªäº›å…³é”®è¦æ±‚ï¼Ÿ",
            "æ³°è¿ªæ¯æ•°æ®æŒ–æ˜æŒ‘æˆ˜èµ›çš„åŸºæœ¬è¦æ±‚æœ‰å“ªäº›ï¼Ÿ",
            
            # æŠ€æœ¯è¦æ±‚é—®é¢˜
            "ç³»ç»Ÿæ¶æ„è®¾è®¡æœ‰ä»€ä¹ˆæŠ€æœ¯è¦æ±‚ï¼Ÿ",
            "ç®—æ³•å®ç°éœ€è¦æ»¡è¶³å“ªäº›æ€§èƒ½æŒ‡æ ‡ï¼Ÿ",
            
            # æµ‹è¯•"æœªæ‰¾åˆ°"æƒ…å†µ
            "ç«æ˜Ÿæ¢æµ‹å™¨çš„æŠ€æœ¯è§„æ ¼æ˜¯ä»€ä¹ˆï¼Ÿ",
            "äººå·¥æ™ºèƒ½èŠ¯ç‰‡çš„åˆ¶é€ å·¥è‰ºè¦æ±‚ï¼Ÿ"
        ]
        
        logger.info("ä¼˜åŒ–RAGç³»ç»Ÿæµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"å‡†å¤‡æµ‹è¯• {len(self.test_questions)} ä¸ªé—®é¢˜")
    
    def run_full_optimization_test(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„ä¼˜åŒ–æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´ä¼˜åŒ–æµ‹è¯•...")
        
        try:
            # 1. åˆå§‹åŒ–ä¼˜åŒ–RAGç³»ç»Ÿ
            logger.info("åˆå§‹åŒ–ä¼˜åŒ–RAGç³»ç»Ÿ...")
            rag_system = OptimizedRAGSystem(
                llm_model="deepseek-r1:7b",
                embedding_model="./bge-large-zh-v1.5",
                vector_weight=0.4,   # é™ä½å‘é‡æƒé‡ï¼Œæé«˜BM25æƒé‡
                bm25_weight=0.6,     # ç¬¦åˆä¼˜åŒ–è¦æ±‚
                enable_reranking=True, # ä½¿ç”¨é‡æ’åº
                enable_chapter_splitting=True, # ä½¿ç”¨ç« èŠ‚åˆ‡åˆ†
                retrieval_k=10 # æ£€ç´¢æ•°é‡
            )
            
            # 2. åŠ è½½æµ‹è¯•æ–‡æ¡£
            logger.info("åŠ è½½æµ‹è¯•æ–‡æ¡£...")
            pdf_files = self._get_test_pdf_files()
            if not pdf_files:
                logger.error("æœªæ‰¾åˆ°PDFæµ‹è¯•æ–‡ä»¶")
                return {"error": "æœªæ‰¾åˆ°PDFæµ‹è¯•æ–‡ä»¶"}
            
            success = rag_system.load_documents(pdf_files)
            if not success:
                logger.error("æ–‡æ¡£åŠ è½½å¤±è´¥")  
                return {"error": "æ–‡æ¡£åŠ è½½å¤±è´¥"}
            
            # 3. è·å–ç³»ç»Ÿæ€§èƒ½æŠ¥å‘Š
            logger.info("è·å–ç³»ç»Ÿæ€§èƒ½æŠ¥å‘Š...")
            performance_report = rag_system.get_system_performance_report()
            
            # 4. è¿è¡Œä¼˜åŒ–æµ‹è¯•
            logger.info("è¿è¡Œä¼˜åŒ–åŠŸèƒ½æµ‹è¯•...")
            test_report = rag_system.run_optimization_test(self.test_questions)
            
            # 5. éªŒè¯ä¼˜åŒ–åŠŸèƒ½
            logger.info("éªŒè¯å„é¡¹ä¼˜åŒ–åŠŸèƒ½...")
            optimization_validation = self._validate_optimizations(rag_system, test_report)
            
            # 6. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            comprehensive_report = {
                "test_metadata": {
                    "test_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "test_questions_count": len(self.test_questions),
                    "pdf_files_count": len(pdf_files),
                    "pdf_files": [os.path.basename(f) for f in pdf_files]
                },
                "system_performance": performance_report,
                "optimization_test_results": test_report,
                "optimization_validation": optimization_validation,
                "overall_assessment": self._generate_overall_assessment(performance_report, test_report, optimization_validation)
            }
            
            # 7. ä¿å­˜æµ‹è¯•æŠ¥å‘Š
            self._save_test_report(comprehensive_report)
            
            logger.info("âœ… å®Œæ•´ä¼˜åŒ–æµ‹è¯•å®Œæˆ")
            return comprehensive_report
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _get_test_pdf_files(self) -> List[str]:
        """è·å–æµ‹è¯•PDFæ–‡ä»¶"""
        pdf_files = []
        
        # æŸ¥æ‰¾dataç›®å½•ä¸­çš„PDFæ–‡ä»¶
        search_dirs = ["./data"]
        
        for search_dir in search_dirs:
            if os.path.exists(search_dir):
                for file in os.listdir(search_dir):
                    if file.lower().endswith('.pdf'):
                        full_path = os.path.join(search_dir, file)
                        pdf_files.append(full_path)
        
        logger.info(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶: {[os.path.basename(f) for f in pdf_files]}")
        
        return pdf_files
    
    def _validate_optimizations(self, rag_system: OptimizedRAGSystem, test_report: Dict) -> Dict:
        """éªŒè¯å„é¡¹ä¼˜åŒ–åŠŸèƒ½"""
        validation = {
            "optimization_1_hybrid_retrieval_rrf": self._validate_hybrid_retrieval_rrf(test_report),
            "optimization_2_crossencoder_reranking": self._validate_crossencoder_reranking(test_report),
            "optimization_3_entity_hit_reward": self._validate_entity_hit_reward(test_report),
            "optimization_4_document_chunking_title_enhancement": self._validate_document_chunking_enhancement(test_report),
            "optimization_5_fewshot_prompt_optimization": self._validate_fewshot_prompt_optimization(test_report)
        }
        
        # è®¡ç®—æ€»ä½“éªŒè¯åˆ†æ•°
        validation_scores = []
        for opt_name, opt_result in validation.items():
            if 'validation_score' in opt_result:
                validation_scores.append(opt_result['validation_score'])
        
        validation["overall_validation_score"] = sum(validation_scores) / len(validation_scores) if validation_scores else 0.0
        
        return validation
    
    def _validate_hybrid_retrieval_rrf(self, test_report: Dict) -> Dict:
        """éªŒè¯ä¼˜åŒ–1ï¼šå…³é”®è¯+å‘é‡æ··åˆæ£€ç´¢ï¼ˆBM25å¼ºåˆ¶å¬å›+RRFèåˆï¼‰"""
        try:
            detailed_results = test_report.get('detailed_results', [])
            
            # æ£€æŸ¥RRFèåˆä½¿ç”¨æƒ…å†µ
            rrf_usage_count = 0
            force_recall_count = 0
            
            for result in detailed_results:
                if result.get('uses_enhanced_features', {}).get('rrf_fusion', False):
                    rrf_usage_count += 1
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¼ºåˆ¶å¬å›çš„è¯æ®ï¼ˆé€šè¿‡åˆ†ææ£€ç´¢ç»Ÿè®¡ï¼‰
                retrieval_stats = result.get('prompt_analysis', {})
                if 'force_recalled' in str(retrieval_stats):
                    force_recall_count += 1
            
            total_questions = len(detailed_results)
            rrf_usage_rate = rrf_usage_count / total_questions if total_questions > 0 else 0
            
            validation_score = min(rrf_usage_rate * 2, 1.0)  # æœ€é«˜1.0åˆ†
            
            return {
                "optimization_name": "å…³é”®è¯+å‘é‡æ··åˆæ£€ç´¢ï¼ˆBM25å¼ºåˆ¶å¬å›+RRFèåˆï¼‰",
                "rrf_fusion_usage_count": rrf_usage_count,
                "rrf_fusion_usage_rate": rrf_usage_rate,
                "force_recall_evidence_count": force_recall_count,
                "validation_score": validation_score,
                "status": "ä¼˜ç§€" if validation_score >= 0.8 else "è‰¯å¥½" if validation_score >= 0.5 else "éœ€æ”¹è¿›",
                "details": f"RRFèåˆä½¿ç”¨ç‡: {rrf_usage_rate:.1%}, æ£€æµ‹åˆ°å¼ºåˆ¶å¬å›è¯æ®: {force_recall_count}æ¬¡"
            }
            
        except Exception as e:
            return {"optimization_name": "å…³é”®è¯+å‘é‡æ··åˆæ£€ç´¢", "error": str(e), "validation_score": 0.0}
    
    def _validate_crossencoder_reranking(self, test_report: Dict) -> Dict:
        """éªŒè¯ä¼˜åŒ–2ï¼šCross-Encoderé‡æ’åº"""
        try:
            detailed_results = test_report.get('detailed_results', [])
            
            reranking_usage_count = 0
            crossencoder_score_count = 0
            
            for result in detailed_results:
                if result.get('uses_enhanced_features', {}).get('reranking', False):
                    reranking_usage_count += 1
                
                # æ£€æŸ¥æ˜¯å¦æœ‰CrossEncoderåˆ†æ•°
                if 'crossencoder_score' in str(result):
                    crossencoder_score_count += 1
            
            total_questions = len(detailed_results)
            reranking_usage_rate = reranking_usage_count / total_questions if total_questions > 0 else 0
            
            validation_score = reranking_usage_rate
            
            return {
                "optimization_name": "Cross-Encoderé‡æ’åºä¼˜åŒ–",
                "reranking_usage_count": reranking_usage_count,
                "reranking_usage_rate": reranking_usage_rate,
                "crossencoder_evidence_count": crossencoder_score_count,
                "validation_score": validation_score,
                "status": "ä¼˜ç§€" if validation_score >= 0.8 else "è‰¯å¥½" if validation_score >= 0.5 else "éœ€æ”¹è¿›",
                "details": f"é‡æ’åºä½¿ç”¨ç‡: {reranking_usage_rate:.1%}, CrossEncoderè¯æ®: {crossencoder_score_count}æ¬¡"
            }
            
        except Exception as e:
            return {"optimization_name": "Cross-Encoderé‡æ’åº", "error": str(e), "validation_score": 0.0}
    
    def _validate_entity_hit_reward(self, test_report: Dict) -> Dict:
        """éªŒè¯ä¼˜åŒ–3ï¼šå®ä½“å‘½ä¸­å¥–åŠ±"""
        try:
            detailed_results = test_report.get('detailed_results', [])
            
            entity_bonus_count = 0
            traffic_signal_questions = 0
            
            for result in detailed_results:
                question = result.get('question', '')
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å®ä½“å¥–åŠ±è¯æ®
                if result.get('uses_enhanced_features', {}).get('entity_bonuses', False):
                    entity_bonus_count += 1
                
                # ç‰¹åˆ«æ£€æŸ¥äº¤é€šä¿¡å·ç¯é—®é¢˜
                if any(keyword in question for keyword in ['äº¤é€šä¿¡å·ç¯', 'æ™ºèƒ½äº¤é€š']):
                    traffic_signal_questions += 1
            
            total_questions = len(detailed_results)
            entity_bonus_rate = entity_bonus_count / total_questions if total_questions > 0 else 0
            
            # å¦‚æœæœ‰äº¤é€šä¿¡å·ç¯é—®é¢˜ï¼Œåº”è¯¥èƒ½è§¦å‘å®ä½“å¥–åŠ±
            expected_entity_bonus = traffic_signal_questions > 0
            validation_score = entity_bonus_rate if expected_entity_bonus else min(entity_bonus_rate + 0.5, 1.0)
            
            return {
                "optimization_name": "å®ä½“å‘½ä¸­å¥–åŠ±æœºåˆ¶",
                "entity_bonus_usage_count": entity_bonus_count,
                "entity_bonus_usage_rate": entity_bonus_rate,
                "traffic_signal_questions": traffic_signal_questions,
                "validation_score": validation_score,
                "status": "ä¼˜ç§€" if validation_score >= 0.8 else "è‰¯å¥½" if validation_score >= 0.5 else "éœ€æ”¹è¿›",
                "details": f"å®ä½“å¥–åŠ±ä½¿ç”¨ç‡: {entity_bonus_rate:.1%}, äº¤é€šä¿¡å·ç¯é—®é¢˜: {traffic_signal_questions}ä¸ª"
            }
            
        except Exception as e:
            return {"optimization_name": "å®ä½“å‘½ä¸­å¥–åŠ±", "error": str(e), "validation_score": 0.0}
    
    def _validate_document_chunking_enhancement(self, test_report: Dict) -> Dict:
        """éªŒè¯ä¼˜åŒ–4ï¼šæ–‡æ¡£åˆ‡åˆ†+æ ‡é¢˜å¢å¼º"""
        try:
            detailed_results = test_report.get('detailed_results', [])
            
            enhanced_docs_count = 0
            chapter_splitting_count = 0
            title_enhancement_evidence = 0
            
            for result in detailed_results:
                if result.get('uses_enhanced_features', {}).get('enhanced_documents', False):
                    enhanced_docs_count += 1
                
                if result.get('uses_enhanced_features', {}).get('chapter_splitting', False):
                    chapter_splitting_count += 1
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡é¢˜å¢å¼ºè¯æ®ï¼ˆé€šè¿‡æ£€æŸ¥ç­”æ¡ˆä¸­æ˜¯å¦åŒ…å«ç« èŠ‚ä¿¡æ¯ï¼‰
                answer = result.get('answer', '')
                if any(marker in answer for marker in ['ã€ä¸»è¦ç« èŠ‚ã€‘', 'ã€å­ç« èŠ‚ã€‘', '#å…³é”®:']):
                    title_enhancement_evidence += 1
            
            total_questions = len(detailed_results)
            enhancement_rate = enhanced_docs_count / total_questions if total_questions > 0 else 0
            chapter_rate = chapter_splitting_count / total_questions if total_questions > 0 else 0
            
            validation_score = (enhancement_rate + chapter_rate) / 2
            
            return {
                "optimization_name": "æ–‡æ¡£åˆ‡åˆ†+æ ‡é¢˜å¢å¼º",
                "enhanced_documents_usage_count": enhanced_docs_count,
                "chapter_splitting_usage_count": chapter_splitting_count,
                "title_enhancement_evidence": title_enhancement_evidence,
                "enhancement_usage_rate": enhancement_rate,
                "validation_score": validation_score,
                "status": "ä¼˜ç§€" if validation_score >= 0.8 else "è‰¯å¥½" if validation_score >= 0.5 else "éœ€æ”¹è¿›",
                "details": f"å¢å¼ºæ–‡æ¡£ç‡: {enhancement_rate:.1%}, ç« èŠ‚åˆ‡åˆ†ç‡: {chapter_rate:.1%}"
            }
            
        except Exception as e:
            return {"optimization_name": "æ–‡æ¡£åˆ‡åˆ†+æ ‡é¢˜å¢å¼º", "error": str(e), "validation_score": 0.0}
    
    def _validate_fewshot_prompt_optimization(self, test_report: Dict) -> Dict:
        """éªŒè¯ä¼˜åŒ–5ï¼šFew-shoté‡æç¤ºä¼˜åŒ–"""
        try:
            optimization_effectiveness = test_report.get('optimization_effectiveness', {})
            detailed_results = test_report.get('detailed_results', [])
            
            # æ£€æŸ¥äº¤é€šä¿¡å·ç¯æ¨¡æ¿ä½¿ç”¨
            traffic_signal_template_count = optimization_effectiveness.get('questions_using_traffic_signal_template', 0)
            
            # æ£€æŸ¥"æœªæ‰¾åˆ°"å›å¤ä½¿ç”¨
            not_found_response_count = optimization_effectiveness.get('questions_using_not_found_response', 0)
            
            # åˆ†ææç¤ºè¯æ•ˆæœ
            effective_prompt_count = 0
            for result in detailed_results:
                prompt_analysis = result.get('prompt_analysis', {})
                if prompt_analysis.get('prompt_effectiveness', {}).get('overall_score', 0) > 0.5:
                    effective_prompt_count += 1
            
            total_questions = len(detailed_results)
            
            # è®¡ç®—Few-shotæ•ˆæœåˆ†æ•°
            template_usage_rate = traffic_signal_template_count / total_questions if total_questions > 0 else 0
            effective_prompt_rate = effective_prompt_count / total_questions if total_questions > 0 else 0
            
            validation_score = (template_usage_rate + effective_prompt_rate) / 2
            
            return {
                "optimization_name": "Few-shoté‡æç¤ºä¼˜åŒ–",
                "traffic_signal_template_usage": traffic_signal_template_count,
                "not_found_response_usage": not_found_response_count,
                "effective_prompt_count": effective_prompt_count,
                "template_usage_rate": template_usage_rate,
                "validation_score": validation_score,
                "status": "ä¼˜ç§€" if validation_score >= 0.8 else "è‰¯å¥½" if validation_score >= 0.5 else "éœ€æ”¹è¿›",
                "details": f"ç‰¹å®šæ¨¡æ¿ä½¿ç”¨ç‡: {template_usage_rate:.1%}, æœ‰æ•ˆæç¤ºç‡: {effective_prompt_rate:.1%}"
            }
            
        except Exception as e:
            return {"optimization_name": "Few-shoté‡æç¤ºä¼˜åŒ–", "error": str(e), "validation_score": 0.0}
    
    def _generate_overall_assessment(self, performance_report: Dict, test_report: Dict, optimization_validation: Dict) -> Dict:
        """ç”Ÿæˆæ€»ä½“è¯„ä¼°"""
        try:
            # è®¡ç®—å„é¡¹æŒ‡æ ‡
            overall_score = optimization_validation.get('overall_validation_score', 0.0)
            
            # ç³»ç»Ÿç¨³å®šæ€§è¯„ä¼°
            system_ready = all([
                performance_report.get('components_status', {}).get('vectorstore_loaded', False),
                performance_report.get('components_status', {}).get('advanced_retriever_ready', False),
                performance_report.get('components_status', {}).get('enhanced_reranker_ready', False),
                performance_report.get('components_status', {}).get('rag_chain_built', False)
            ])
            
            # æ€§èƒ½è¯„ä¼°
            test_summary = test_report.get('test_summary', {})
            avg_response_time = test_summary.get('average_time', 0)
            
            # åŠŸèƒ½å®Œæ•´æ€§è¯„ä¼°
            optimization_features = performance_report.get('optimization_features', {})
            feature_completeness = sum(optimization_features.values()) / len(optimization_features) if optimization_features else 0
            
            # ç”Ÿæˆè¯„çº§
            if overall_score >= 0.9 and system_ready and avg_response_time < 10:
                grade = "A+ (ä¼˜ç§€)"
            elif overall_score >= 0.8 and system_ready:
                grade = "A (è‰¯å¥½)"
            elif overall_score >= 0.6:
                grade = "B (åŠæ ¼)"
            else:
                grade = "C (éœ€æ”¹è¿›)"
            
            return {
                "overall_score": overall_score,
                "grade": grade,
                "system_stability": "ç¨³å®š" if system_ready else "ä¸ç¨³å®š",
                "average_response_time": avg_response_time,
                "feature_completeness": feature_completeness,
                "optimization_summary": {
                    "å®ç°çš„ä¼˜åŒ–åŠŸèƒ½": [
                        "âœ… å…³é”®è¯+å‘é‡æ··åˆæ£€ç´¢ï¼ˆBM25å¼ºåˆ¶å¬å›+RRFèåˆï¼‰",
                        "âœ… Cross-Encoderé‡æ’åºä¼˜åŒ–",
                        "âœ… å®ä½“å‘½ä¸­å¥–åŠ±æœºåˆ¶",
                        "âœ… æ–‡æ¡£åˆ‡åˆ†+æ ‡é¢˜å¢å¼º",
                        "âœ… Few-shoté‡æç¤ºä¼˜åŒ–"
                    ],
                    "ä¼˜åŒ–æ•ˆæœ": f"æ€»ä½“éªŒè¯åˆ†æ•°: {overall_score:.2f}/1.00",
                    "ç³»ç»ŸçŠ¶æ€": "å°±ç»ª" if system_ready else "éœ€æ£€æŸ¥"
                },
                "recommendations": self._generate_recommendations(overall_score, avg_response_time, optimization_validation)
            }
            
        except Exception as e:
            return {"error": f"æ€»ä½“è¯„ä¼°å¤±è´¥: {e}"}
    
    def _generate_recommendations(self, overall_score: float, avg_response_time: float, optimization_validation: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if overall_score < 0.8:
            recommendations.append("å»ºè®®æ£€æŸ¥å„é¡¹ä¼˜åŒ–åŠŸèƒ½çš„å®ç°æ•ˆæœï¼Œç¡®ä¿æ­£ç¡®é…ç½®")
        
        if avg_response_time > 10:
            recommendations.append("å“åº”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–æ¨¡å‹æ¨ç†é€Ÿåº¦æˆ–æ£€ç´¢æ•°é‡")
        
        # é’ˆå¯¹å…·ä½“ä¼˜åŒ–åŠŸèƒ½çš„å»ºè®®
        for opt_name, opt_result in optimization_validation.items():
            if opt_name.startswith('optimization_') and opt_result.get('validation_score', 0) < 0.6:
                recommendations.append(f"å»ºè®®æ”¹è¿› {opt_result.get('optimization_name', opt_name)} çš„å®ç°æ•ˆæœ")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ç›‘æ§æ€§èƒ½æŒ‡æ ‡")
        
        return recommendations
    
    def _save_test_report(self, report: Dict):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        try:
            # ä¿å­˜åˆ°outputsç›®å½•
            os.makedirs("outputs", exist_ok=True)
            
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            report_file = f"outputs/optimized_rag_test_report_{timestamp}.json"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
            # åŒæ—¶ä¿å­˜ä¸€ä¸ªæœ€æ–°ç‰ˆæœ¬
            latest_file = "outputs/optimized_rag_test_report_latest.json"
            with open(latest_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"ä¿å­˜æµ‹è¯•æŠ¥å‘Šå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹ä¼˜åŒ–RAGç³»ç»Ÿå…¨é¢æµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = OptimizedRAGTester()
    
    # è¿è¡Œå®Œæ•´æµ‹è¯•
    start_time = time.time()
    test_report = tester.run_full_optimization_test()
    total_time = time.time() - start_time
    
    if "error" in test_report:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {test_report['error']}")
        return False
    
    # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦
    logger.info("ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
    overall_assessment = test_report.get('overall_assessment', {})
    logger.info(f"  æ€»ä½“è¯„åˆ†: {overall_assessment.get('grade', 'N/A')}")
    logger.info(f"  æ€»ä½“åˆ†æ•°: {overall_assessment.get('overall_score', 0):.2f}/1.00")
    logger.info(f"  å¹³å‡å“åº”æ—¶é—´: {overall_assessment.get('average_response_time', 0):.2f}ç§’")
    logger.info(f"  ç³»ç»Ÿç¨³å®šæ€§: {overall_assessment.get('system_stability', 'N/A')}")
    
    # æ˜¾ç¤ºä¼˜åŒ–åŠŸèƒ½éªŒè¯ç»“æœ
    optimization_validation = test_report.get('optimization_validation', {})
    logger.info("ğŸ”§ ä¼˜åŒ–åŠŸèƒ½éªŒè¯ç»“æœ:")
    for i, (opt_key, opt_result) in enumerate(optimization_validation.items(), 1):
        if opt_key.startswith('optimization_'):
            opt_name = opt_result.get('optimization_name', f'ä¼˜åŒ–{i}')
            status = opt_result.get('status', 'æœªçŸ¥')
            score = opt_result.get('validation_score', 0)
            logger.info(f"  {i}. {opt_name}: {status} (åˆ†æ•°: {score:.2f})")
    
    # æ˜¾ç¤ºå»ºè®®
    recommendations = overall_assessment.get('recommendations', [])
    if recommendations:
        logger.info("ğŸ’¡ æ”¹è¿›å»ºè®®:")
        for i, rec in enumerate(recommendations, 1):
            logger.info(f"  {i}. {rec}")
    
    logger.info(f"âœ… æµ‹è¯•å®Œæˆï¼Œæ€»ç”¨æ—¶: {total_time:.2f}ç§’")
    logger.info("ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ° outputs/optimized_rag_test_report_latest.json")
    
    return True

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æµ‹è¯•ç¨‹åºå¼‚å¸¸: {e}")
        sys.exit(1) 